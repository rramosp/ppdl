{"cells": [{"cell_type": "code", "execution_count": null, "id": "2e046c39", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "2b224313", "metadata": {}, "source": ["# Model-based reasoning and Bayesian inference\n", "\n", "In model-based reasoning we assume a model of the process that generated our data. In Bayesian inference this model has a probabilistic nature, i.e., it encodes our knowledge (or assumptions) of the world in terms of random variables, probabilistic distributions of these random variables and dependence/independence relationships.\n", "\n", "The following is a summary of the Bayesian approach to machine learning (https://www.cs.toronto.edu/~radford/ftp/bayes-tut.pdf):\n", "\n", "1. **Model formulation**: We formulate our knowledge about the world (or the mechanism that generated our data) probabilistically:\n", "  - We define a *model* that represents qualitative aspects of our knowledge. This model is expressed in terms of random variables and their relationships. The model has unknown parameters. \n", "  - We specify our believes (before seeing any data) about how we expect the values of the unknown parameters to behave using a *prior* probability.\n", "2. **Data collection**: We obtain data.\n", "3. **Posterior calculation**: We use the data to compute the *posterior* probability distribution of the unknown parameters, given the observed data and the priors.\n", "4. **Model application**: The posterior probability distribution can be used to:\n", "  - Derive scientific conclusions, taking into account uncertainty.\n", "  - Make predictions.\n", "  - Make decisions.\n", "\n", "To illustrate this process, we will a coin flipping experiment as an example.\n", "\n", "First, let us import the required libraries and modules."]}, {"cell_type": "code", "execution_count": null, "id": "187d7725", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["from scipy.special import comb\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use(\"ggplot\")"]}, {"cell_type": "markdown", "id": "8184ad96", "metadata": {}, "source": ["## Coin flipping example\n", "\n", "In this example, we want to model the toss of a coin using a Bayesian approach. "]}, {"cell_type": "markdown", "id": "8bde63ac", "metadata": {}, "source": ["### 1. Model formulation\n", "\n", "Our model will have two random variables:\n", "\n", "$X$: random variable representing the outcome of a coin toss\n", "\n", "$\\Theta$: probability of getting a head ($X=1$)\n", "\n", "The distribution of $Y$ is Bernoulli:\n", "\n", "$$\n", "P(X = x | \\theta) = \\theta^{y}(1 - \\theta)^{1 - y}\n", "$$\n", "\n", "An experiment consists of performing several coin tosses. The resulting data, $\\mathcal{D} = \\{y_1, y_2, \\dots, y_N\\}$, from the experiment can be summarized by two values: $N_{heads}$ (number of heads) and $N_{tails}$ (number of tails). The likelihood of a particular experiment outcome is given by the binomial distribution:\n", "\n", "$$\n", "P(\\mathcal{D} | \\theta) = \\binom{N_{heads} + N_{tails}}{N_{heads}} \\theta^{N_{heads}} (1 - \\theta)^{N_{tails}}\n", "$$\n", "\n", "This term is called the *likelihood*, the conditional probability of the data $\\mathcal{D}$ given the model's parameters $\\theta$. The following function implements the likelihood:"]}, {"cell_type": "code", "execution_count": null, "id": "c0473759", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def likelihood(num_heads, num_tails, theta):\n", "    return (\n", "        comb(num_heads + num_tails, num_heads) *\n", "        theta ** num_heads *\n", "        (1 - theta) ** num_tails\n", "        )"]}, {"cell_type": "markdown", "id": "ccb4d08e", "metadata": {}, "source": ["> **Note**: the `comb` function is an implementation of the binomial coefficient:\n", "\n", "$$\n", "\\binom(n}{k} = \\frac{n!}{k! (n - k)!}\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "b3830dc7", "metadata": {}, "outputs": [], "source": ["n = 10\n", "n_heads = 6"]}, {"cell_type": "code", "execution_count": null, "id": "6682b394", "metadata": {}, "outputs": [], "source": ["# Numpy implementation of the binomial coefficient\n", "print(np.math.factorial(n) / (np.math.factorial(n_heads) * np.math.factorial(n - n_heads)))"]}, {"cell_type": "code", "execution_count": null, "id": "ea6a1655", "metadata": {}, "outputs": [], "source": ["# comb function\n", "comb(n, n_heads)"]}, {"cell_type": "markdown", "id": "458464b6", "metadata": {}, "source": ["For instance if $\\theta = 0.3$, the probability of getting 6 heads and 4 tails in 10 coin tosses would be:"]}, {"cell_type": "code", "execution_count": null, "id": "eb36bf36", "metadata": {}, "outputs": [], "source": ["likelihood(6, 4, 0.3)"]}, {"cell_type": "markdown", "id": "79f3069e", "metadata": {}, "source": ["But if $\\theta=0.8$ the likelihood would be:"]}, {"cell_type": "code", "execution_count": null, "id": "7dd3fd90", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["likelihood(6, 4, 0.8)"]}, {"cell_type": "markdown", "id": "3b8dbda8", "metadata": {}, "source": ["As part of our modeling we have to also represent our prior knowledge of the possible values of $\\theta$. This is encoded by a distribution of $\\theta$ values which is called the prior distribution:\n", "\n", "$$\n", "P(\\theta)\n", "$$\n", "\n", "If we don't have any prior knowledge, this can be encoded as a uniform prior distribution. Since we are dealing with a coin, we can assume that coins that are closer to be fair are more likely, this means that we give higher probabilities to values of $\\theta$ closer to $0.5$. To this end, We'll use a discrete triangular distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "483049ba", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def triangular_prior(n_points):\n", "    theta_vals = np.linspace(0, 1, n_points)\n", "    theta_prior_p = np.concatenate([\n", "        np.arange(0, n_points // 2),\n", "        np.arange(n_points // 2, -1, -1)]\n", "        )\n", "    theta_prior_p = theta_prior_p / np.sum(theta_prior_p)\n", "    theta_prior = {\n", "            theta_vals[i]: theta_prior_p[i]\n", "            for i in range(n_points)\n", "            }\n", "    return theta_prior"]}, {"cell_type": "markdown", "id": "4371c8e4", "metadata": {}, "source": ["Let's visualize this distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "4f9c3aca", "metadata": {}, "outputs": [], "source": ["n_points = 11\n", "prior = triangular_prior(n_points)\n", "fig, ax = plt.subplots(figsize=(10, 7))\n", "ax.bar(\n", "        list(map(lambda x: f\"{x:.2f}\", prior.keys())),\n", "        prior.values(), width=0.05\n", "        )\n", "ax.set_xlabel(r\"$\\theta$\")\n", "ax.set_ylabel(r\"$P(\\theta)$\")"]}, {"cell_type": "markdown", "id": "20863ff8", "metadata": {}, "source": ["### 2. Data collection\n", "\n", "To collect data, we can toss a real coin a number of times and record how many heads ($N_{heads}$) and tails ($N_{tails}$) we get. This is our data $\\mathcal{D}$. For this exercise let's assume we got:"]}, {"cell_type": "code", "execution_count": null, "id": "933d7009", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["n_heads = 15\n", "n_tails = 5"]}, {"cell_type": "markdown", "id": "3f7e52f8", "metadata": {}, "source": ["### 3. Posterior calculation\n", "\n", "To calculate the posterior we will use the the Bayes theorem:\n", "\n", "$$\n", "P(\\theta|\\mathcal{D}) = \\frac{P(\\mathcal{D}|\\theta)P(\\theta)}{P(\\mathcal{D})}\n", "$$\n", "\n", "We already have the prior distribution and the likelihood function. We need to calculate the probability of the evidence (our data $D$). For this we will use the law to total probability, taking advantage of the fact that the prior distribution is discrete and only takes values greater than 0 for a finite set of theta values $\\{\\theta_0\\dots\\theta_{n-1}\\}$ :\n", "\n", "$$\n", "\\begin{align}\n", "P(D) &= \\sum_{\\theta_i}{P(\\mathcal{D},\\theta=\\theta_i)} \\\\\n", "     &= \\sum_{\\theta_i}{P(\\mathcal{D}|\\theta=\\theta_i)P(\\theta = \\theta_i)} \\\\\n", "\\end{align}\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "61314021", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def evidence(n_heads, n_tails, n_points):\n", "    priors = triangular_prior(n_points)\n", "    joints = [\n", "            likelihood(n_heads, n_tails, theta_i) *\n", "            prior_value\n", "            for theta_i, prior_value in priors.items()\n", "            ]\n", "    return sum(joints)"]}, {"cell_type": "markdown", "id": "ea5eacaa", "metadata": {}, "source": ["We can compute the evidence for different values:"]}, {"cell_type": "code", "execution_count": null, "id": "7bd1318c", "metadata": {}, "outputs": [], "source": ["evidence(n_heads=5, n_tails=5, n_points=3)"]}, {"cell_type": "code", "execution_count": null, "id": "5d8f382f", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["evidence(n_heads=10, n_tails=1, n_points=11)"]}, {"cell_type": "markdown", "id": "3e4675de", "metadata": {}, "source": ["Now, we are prepared to compute the posterior distribution function ($P(\\theta|\\mathcal{D})$):"]}, {"cell_type": "code", "execution_count": null, "id": "e5bd98b5", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def posterior(n_heads, n_tails, n_points):\n", "    p_theta = triangular_prior(n_points)\n", "    p_d = evidence(n_heads, n_tails, n_points)\n", "    p_theta_d = {\n", "            theta_i: \n", "            likelihood(n_heads, n_tails, theta_i) *\n", "            prior_i / p_d\n", "            for theta_i, prior_i in p_theta.items()\n", "            }\n", "\n", "    return p_theta_d"]}, {"cell_type": "markdown", "id": "133380ff", "metadata": {}, "source": ["Now we call the function with our data and plot the resulting posterior distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "f8a13a2e", "metadata": {}, "outputs": [], "source": ["theta_posterior = posterior(\n", "        n_heads=n_heads,\n", "        n_tails=n_tails,\n", "        n_points=n_points\n", "        )\n", "print(theta_posterior)"]}, {"cell_type": "markdown", "id": "381208da", "metadata": {}, "source": ["We can see a comparison between the prior distribution and the posterior:"]}, {"cell_type": "code", "execution_count": null, "id": "4dd08536", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(15, 7))\n", "\n", "theta_prior = triangular_prior(n_points)\n", "theta_posterior = posterior(\n", "        n_heads=n_heads,\n", "        n_tails=n_tails,\n", "        n_points=n_points\n", "        )\n", "\n", "ax.bar(\n", "        list(map(lambda x: f\"{x:.2f}\", prior.keys())),\n", "        prior.values(), width=0.1, label=r\"$P(\\theta)$\",\n", "        alpha=0.5\n", "        )\n", "\n", "ax.bar(\n", "        list(map(lambda x: f\"{x:.2f}\", theta_posterior.keys())),\n", "        theta_posterior.values(), width=0.1,\n", "        label=r\"$P(\\theta|\\mathcal{D})$\", alpha=0.5\n", "        )\n", "\n", "ax.set_xlabel(r\"$\\theta$\")\n", "ax.set_ylabel(r\"Probability\")\n", "ax.legend()"]}, {"cell_type": "markdown", "id": "bd64d494", "metadata": {}, "source": ["### 4. Model application\n", "\n", "We can see that the resulting posterior distribution gives higher probability to values of the parameter around $0.7$, this is consistent with the fact that the data from the experiment has more heads than tails. \n", "\n", "If we want to make a prediction of the outcome of a new coin toss, we can do it in different ways. Let's see some common methods:"]}, {"cell_type": "markdown", "id": "d6b46f32", "metadata": {}, "source": ["#### Maximum Aposteriori Probability\n", "\n", "We can use the mode of the posterior distribution, i.e.:\n", "\n", "$$\n", "\\theta_{\\text{MAP}} = \\underset{\\theta}{\\text{argmax}} P(\\theta|\\mathcal{D})\n", "$$\n", "\n", "This is called maximum a posteriori estimation (MAP). The following Python function calculates the MAP estimator for a given posterior distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "a1d1fc8f", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def map_estimator(theta_posterior):\n", "    return max(theta_posterior.items(), key=lambda x: x[1])"]}, {"cell_type": "markdown", "id": "11338c80", "metadata": {}, "source": ["In our example the MAP estimator will be:"]}, {"cell_type": "code", "execution_count": null, "id": "d651bfc9", "metadata": {}, "outputs": [], "source": ["theta_map, map_p = map_estimator(theta_posterior)"]}, {"cell_type": "code", "execution_count": null, "id": "f4e29ded", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["print(theta_map)\n", "print(map_p)"]}, {"cell_type": "markdown", "id": "d9bff9ae", "metadata": {}, "source": ["Which means that the probability of getting heads in the new experiment will be:\n", "\n", "$$\n", "P(Y=1|\\theta = \\theta_{\\text{MAP}}) = \\theta_{\\text{MAP}} = 0.7\n", "$$"]}, {"cell_type": "markdown", "id": "9bdc20d2", "metadata": {}, "source": ["#### Posterior Expectation\n", "\n", "Another alternative is to calculate the expected value of $\\theta$:\n", "\n", "$$\n", "\\theta_{\\text{Bayes}} = E_{P(\\theta|\\mathcal{D}}[\\theta]\n", "$$\n", "\n", "This is called the Bayes estimator. The following function calculates the Bayes estimator:"]}, {"cell_type": "code", "execution_count": null, "id": "167ec843", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def bayes_estimator(theta_posterior):\n", "    return sum(theta_i * p_i for theta_i, p_i in theta_posterior.items())"]}, {"cell_type": "markdown", "id": "f18cda71", "metadata": {}, "source": ["In our example the Bayes estimator will be:"]}, {"cell_type": "code", "execution_count": null, "id": "c6656d3a", "metadata": {}, "outputs": [], "source": ["print(bayes_estimator(theta_posterior))"]}, {"cell_type": "markdown", "id": "930c75ba", "metadata": {}, "source": ["The probability of getting heads in the new experiment according to this estimator will be:\n", "\n", "$$\n", "P(Y=1|\\theta = \\theta_{\\text{Bayes}}) = \\theta_{\\text{Bayes}} = 0.6961\n", "$$"]}, {"cell_type": "markdown", "id": "f5aa555d", "metadata": {}, "source": ["#### Bayesian Model Averaging\n", "\n", "Bayesian Model Averaging (BMA) leverages from random number generators and samplers from the posterior distribution to generate a prediction. In fact, We can generate any number of models from the Bayesian process, and then, average any quantity of interest. If we want to have an estimation of $\\theta_{BMA}$:\n", "\n", "$$\n", "\\theta_{BMA} = \\frac{\\sum_{i=1} ^ N_{samples} \\theta_i}{N_{samples}}\n", "$$\n", "\n", "This strategy is important, specially for some optimization techniques that do not directly provide a posterior distribution, but its samples (like Markov Chain Monte Carlo).\n", "\n", "Let's see an example for the coins:"]}, {"cell_type": "code", "execution_count": null, "id": "c356d060", "metadata": {}, "outputs": [], "source": ["n_samples = 100\n", "theta_values, theta_probs = theta_posterior.keys(), theta_posterior.values()\n", "theta_samples = np.random.choice(\n", "    list(theta_values),\n", "    p=list(theta_probs),\n", "    size=n_samples\n", "    )\n", "print(theta_samples)"]}, {"cell_type": "code", "execution_count": null, "id": "85202c4d", "metadata": {}, "outputs": [], "source": ["print(theta_samples.mean())"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}