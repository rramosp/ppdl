
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Probabilistic Programming and Bayesian Methods for Hackers Chapter 1 &#8212; Probabilistic Programming for Machine Learning</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/spectre-exp.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/spectre-icons.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/xglobal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/spectre.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-MBFEZ3PF64"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-MBFEZ3PF64');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/logo-tf-udea-unal.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Probabilistic Programming for Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../intro.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../outline.html">
   Course outline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../M1-videolist.html">
   1 Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../M2-videolist.html">
   2 TF for Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../M3-videolist.html">
   3 Intuitions on Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../M4-videolist.html">
   4 Tensorflow Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../M5-videolist.html">
   5 Bayesian Modelling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../M6-videolist.html">
   6 Variational Inference
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/04.02 - Variational inference/research/Ch1_Introduction_TFP.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/rramosp/ppdl/blob/main/content/04.02 - Variational inference/research/Ch1_Introduction_TFP.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#table-of-contents">
   Table of Contents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dependencies-prerequisites">
   Dependencies &amp; Prerequisites
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-philosophy-of-bayesian-inference">
   The Philosophy of Bayesian Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-bayesian-state-of-mind">
   The Bayesian state of mind
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-inference-in-practice">
   Bayesian Inference in Practice
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#incorporating-evidence">
     Incorporating evidence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#are-frequentist-methods-incorrect-then">
   Are frequentist methods incorrect then?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-note-on-big-data">
     A note on
     <em>
      Big Data
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#our-bayesian-framework">
   Our Bayesian framework
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-mandatory-coin-flip-example">
   Example: Mandatory coin-flip example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-bug-or-just-sweet-unintended-feature">
   Example: Bug, or just sweet, unintended feature?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-distributions">
   Probability Distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-case">
     Discrete Case
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-case">
     Continuous Case
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#but-what-is-lambda">
   But what is
   <span class="math notranslate nohighlight">
    \(\lambda \;\)
   </span>
   ?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-inferring-behaviour-from-text-message-data">
     Example: Inferring behaviour from text-message data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introducing-our-first-hammer-tensorflow-probability">
   Introducing our first hammer: TensorFlow Probability
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#specify-the-joint-log-density">
   Specify the joint log-density
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#specify-the-posterior-sampler">
   Specify the posterior sampler
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-the-results">
   Plot the Results
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-would-i-want-samples-from-the-posterior-anyways">
     Why would I want samples from the posterior, anyways?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Probabilistic Programming and Bayesian Methods for Hackers Chapter 1</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#table-of-contents">
   Table of Contents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dependencies-prerequisites">
   Dependencies &amp; Prerequisites
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-philosophy-of-bayesian-inference">
   The Philosophy of Bayesian Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-bayesian-state-of-mind">
   The Bayesian state of mind
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-inference-in-practice">
   Bayesian Inference in Practice
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#incorporating-evidence">
     Incorporating evidence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#are-frequentist-methods-incorrect-then">
   Are frequentist methods incorrect then?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-note-on-big-data">
     A note on
     <em>
      Big Data
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#our-bayesian-framework">
   Our Bayesian framework
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-mandatory-coin-flip-example">
   Example: Mandatory coin-flip example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-bug-or-just-sweet-unintended-feature">
   Example: Bug, or just sweet, unintended feature?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-distributions">
   Probability Distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-case">
     Discrete Case
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-case">
     Continuous Case
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#but-what-is-lambda">
   But what is
   <span class="math notranslate nohighlight">
    \(\lambda \;\)
   </span>
   ?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-inferring-behaviour-from-text-message-data">
     Example: Inferring behaviour from text-message data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introducing-our-first-hammer-tensorflow-probability">
   Introducing our first hammer: TensorFlow Probability
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#specify-the-joint-log-density">
   Specify the joint log-density
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#specify-the-posterior-sampler">
   Specify the posterior sampler
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-the-results">
   Plot the Results
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation">
   Interpretation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-would-i-want-samples-from-the-posterior-anyways">
     Why would I want samples from the posterior, anyways?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="probabilistic-programming-and-bayesian-methods-for-hackers-chapter-1">
<h1>Probabilistic Programming and Bayesian Methods for Hackers Chapter 1<a class="headerlink" href="#probabilistic-programming-and-bayesian-methods-for-hackers-chapter-1" title="Permalink to this headline">¶</a></h1>
<table class="tfo-notebook-buttons" align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_TFP.ipynb"><img height="32px" src="https://colab.research.google.com/img/colab_favicon.ico" />Run in Google Colab</a>
  </td>
  <td>
    <a target="_blank" href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_TFP.ipynb"><img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" />View source on GitHub</a>
  </td>
</table>
<br>
<br>
<br>
<p>Original content (<a class="reference external" href="https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC2.ipynb">this Jupyter notebook</a>) created by Cam Davidson-Pilon (<a class="reference external" href="https://twitter.com/Cmrn_DP"><code class="docutils literal notranslate"><span class="pre">&#64;Cmrn_DP</span></code></a>)</p>
<p>Ported to <a class="reference external" href="https://www.tensorflow.org/probability/">Tensorflow Probability</a> by Matthew McAteer (<a class="reference external" href="https://twitter.com/MatthewMcAteer0"><code class="docutils literal notranslate"><span class="pre">&#64;MatthewMcAteer0</span></code></a>) and Bryan Seybold, with help from the TFP team at  Google (<a class="reference external" href="mailto:tfprobability&#37;&#52;&#48;tensorflow&#46;org"><code class="docutils literal notranslate"><span class="pre">tfprobability&#64;tensorflow.org</span></code></a>).</p>
<p>Welcome to Bayesian Methods for Hackers. The full Github repository is available at <a class="reference external" href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers">github/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers</a>. The other chapters can be found on the project’s <a class="reference external" href="https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">homepage</a>. We hope you enjoy the book, and we encourage any contributions!</p>
<hr class="docutils" />
<div class="section" id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Dependencies &amp; Prerequisites</p></li>
<li><p>The Philosophy of Bayesian Inference</p></li>
<li><p>The Bayesian state of mind</p></li>
<li><p>Bayesian Inference in Practice</p></li>
<li><p>Are frequentist methods incorrect then?</p></li>
<li><p>Our Bayesian framework</p></li>
<li><p>Example: Mandatory coin-flip example</p></li>
<li><p>Example: Bug, or just sweet, unintended feature?</p></li>
<li><p>Probability Distributions</p>
<ul>
<li><p>Discrete Case</p></li>
</ul>
</li>
<li><p>Continuous Case</p></li>
<li><p>But what is <span class="math notranslate nohighlight">\(\lambda \;\)</span>?</p>
<ul>
<li><p>Example: Inferring behaviour from text-message data</p></li>
</ul>
</li>
<li><p>Introducing our first hammer: Tensorflow Probability</p></li>
<li><p>specify the joint log-density</p></li>
<li><p>Specify the posterior sampler</p></li>
<li><p>Execute the TF graph to sample from the posterior</p></li>
<li><p>Plot the Results</p></li>
<li><p>Interpretation</p></li>
<li><p>Exercises</p></li>
<li><p>References</p></li>
</ul>
</div>
<div class="section" id="dependencies-prerequisites">
<h2>Dependencies &amp; Prerequisites<a class="headerlink" href="#dependencies-prerequisites" title="Permalink to this headline">¶</a></h2>
<div class="alert alert-success">
    Tensorflow Probability is part of the colab default runtime, <b>so you don't need to install Tensorflow or Tensorflow Probability if you're running this in the colab</b>. 
    <br>
    If you're running this notebook in Jupyter on your own machine (and you have already installed Tensorflow), you can use the following
    <br>
      <ul>
    <li> For the most recent nightly installation: <code>pip3 install -q tfp-nightly</code></li>
    <li> For the most recent stable TFP release: <code>pip3 install -q --upgrade tensorflow-probability</code></li>
    <li> For the most recent stable GPU-connected version of TFP: <code>pip3 install -q --upgrade tensorflow-probability-gpu</code></li>
    <li> For the most recent nightly GPU-connected version of TFP: <code>pip3 install -q tfp-nightly-gpu</code></li>
    </ul>
Again, if you are running this in a Colab, Tensorflow and TFP are already installed
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Imports and Global Variables (make sure to run this cell)  { display-mode: &quot;form&quot; }</span>

<span class="k">try</span><span class="p">:</span>
  <span class="c1"># %tensorflow_version only exists in Colab.</span>
  <span class="o">%</span><span class="k">tensorflow_version</span> 2.x
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
  <span class="k">pass</span>


<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>


<span class="c1">#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)</span>
<span class="n">warning_status</span> <span class="o">=</span> <span class="s2">&quot;ignore&quot;</span> <span class="c1">#@param [&quot;ignore&quot;, &quot;always&quot;, &quot;module&quot;, &quot;once&quot;, &quot;default&quot;, &quot;error&quot;]</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">warning_status</span><span class="p">)</span>
<span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">warning_status</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">warning_status</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/)</span>
<span class="n">matplotlib_style</span> <span class="o">=</span> <span class="s1">&#39;fivethirtyeight&#39;</span> <span class="c1">#@param [&#39;fivethirtyeight&#39;, &#39;bmh&#39;, &#39;ggplot&#39;, &#39;seaborn&#39;, &#39;default&#39;, &#39;Solarize_Light2&#39;, &#39;classic&#39;, &#39;dark_background&#39;, &#39;seaborn-colorblind&#39;, &#39;seaborn-notebook&#39;]</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span><span class="p">;</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">matplotlib_style</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.axes</span> <span class="k">as</span> <span class="nn">axes</span><span class="p">;</span>
<span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>
<span class="c1">#%matplotlib inline</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;notebook&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">IPython.core.pylabtools</span> <span class="kn">import</span> <span class="n">figsize</span>
<span class="c1">#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)</span>
<span class="n">notebook_screen_res</span> <span class="o">=</span> <span class="s1">&#39;retina&#39;</span> <span class="c1">#@param [&#39;retina&#39;, &#39;png&#39;, &#39;jpeg&#39;, &#39;svg&#39;, &#39;pdf&#39;]</span>
<span class="c1">#%config InlineBackend.figure_format = notebook_screen_res</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>
<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>
<span class="n">tfb</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">bijectors</span>

<span class="k">class</span> <span class="nc">_TFColor</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enum of colors used in TF docs.&quot;&quot;&quot;</span>
    <span class="n">red</span> <span class="o">=</span> <span class="s1">&#39;#F15854&#39;</span>
    <span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#5DA5DA&#39;</span>
    <span class="n">orange</span> <span class="o">=</span> <span class="s1">&#39;#FAA43A&#39;</span>
    <span class="n">green</span> <span class="o">=</span> <span class="s1">&#39;#60BD68&#39;</span>
    <span class="n">pink</span> <span class="o">=</span> <span class="s1">&#39;#F17CB0&#39;</span>
    <span class="n">brown</span> <span class="o">=</span> <span class="s1">&#39;#B2912F&#39;</span>
    <span class="n">purple</span> <span class="o">=</span> <span class="s1">&#39;#B276B2&#39;</span>
    <span class="n">yellow</span> <span class="o">=</span> <span class="s1">&#39;#DECF3F&#39;</span>
    <span class="n">gray</span> <span class="o">=</span> <span class="s1">&#39;#4D4D4D&#39;</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">red</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">orange</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">green</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blue</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pink</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">brown</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">purple</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">yellow</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gray</span><span class="p">,</span>
        <span class="p">][</span><span class="n">i</span> <span class="o">%</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">TFColor</span> <span class="o">=</span> <span class="n">_TFColor</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">session_options</span><span class="p">(</span><span class="n">enable_gpu_ram_resizing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">enable_xla</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Allowing the notebook to make use of GPUs if they&#39;re available.</span>

<span class="sd">    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear</span>
<span class="sd">    algebra that optimizes TensorFlow computations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span>
    <span class="n">gpu_devices</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">enable_gpu_ram_resizing</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">gpu_devices</span><span class="p">:</span>
           <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">enable_xla</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">set_jit</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">config</span>

<span class="n">session_options</span><span class="p">(</span><span class="n">enable_gpu_ram_resizing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">enable_xla</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TensorFlow 2.x selected.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;module &#39;tensorflow_core._api.v2.config&#39; from &#39;/usr/local/lib/python3.6/dist-packages/tensorflow_core/_api/v2/config/__init__.py&#39;&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="the-philosophy-of-bayesian-inference">
<h2>The Philosophy of Bayesian Inference<a class="headerlink" href="#the-philosophy-of-bayesian-inference" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>You are a skilled programmer, but bugs still slip into your code. After a particularly difficult implementation of an algorithm, you decide to test your code on a trivial example. It passes. You test the code on a harder problem. It passes once again. And it passes the next, even more difficult, test too! You are starting to believe that there may be no bugs in this code…</p>
</div></blockquote>
<p>If you think this way, then congratulations, you already are thinking Bayesian! Bayesian inference is simply updating your beliefs after considering new evidence. A Bayesian can rarely be certain about a result, but he or she can be very confident. Just like in the example above, we can never be 100% sure that our code is bug-free unless we test it on every possible problem; something rarely possible in practice. Instead, we can test it on a large number of problems, and if it succeeds we can feel more confident about our code, but still not certain. Bayesian inference works identically: we update our beliefs about an outcome; rarely can we be absolutely sure unless we rule out all other alternatives.</p>
</div>
<div class="section" id="the-bayesian-state-of-mind">
<h2>The Bayesian state of mind<a class="headerlink" href="#the-bayesian-state-of-mind" title="Permalink to this headline">¶</a></h2>
<p>Bayesian inference differs from more traditional statistical inference by preserving uncertainty. At first, this sounds like a bad statistical technique. Isn’t statistics all about deriving certainty from randomness? To reconcile this, we need to start thinking like Bayesians.</p>
<p>The Bayesian world-view interprets probability as measure of believability in an event, that is, how confident we are in an event occurring. In fact, we will see in a moment that this is the natural interpretation of probability.</p>
<p>For this to be clearer, we consider an alternative interpretation of probability: Frequentist, known as the more classical version of statistics, assume that probability is the long-run frequency of events (hence the bestowed title). For example, the probability of plane accidents under a frequentist philosophy is interpreted as the long-term frequency of plane accidents. This makes logical sense for many probabilities of events, but becomes more difficult to understand when events have no long-term frequency of occurrences. Consider: we often assign probabilities to outcomes of presidential elections, but the election itself only happens once! Frequentists get around this by invoking alternative realities and saying across all these realities, the frequency of occurrences defines the probability.</p>
<p>Bayesians, on the other hand, have a more intuitive approach. Bayesians interpret a probability as measure of belief, or confidence, of an event occurring. Simply, a probability is a summary of an opinion. An individual who assigns a belief of 0 to an event has no confidence that the event will occur; conversely, assigning a belief of 1 implies that the individual is absolutely certain of an event occurring. Beliefs between 0 and 1 allow for weightings of other outcomes. This definition agrees with the probability of a plane accident example, for having observed the frequency of plane accidents, an individual’s belief should be equal to that frequency, excluding any outside information. Similarly, under this definition of probability being equal to beliefs, it is meaningful to speak about probabilities (beliefs) of presidential election outcomes: how confident are you candidate A will win?</p>
<p>Notice in the paragraph above, I assigned the belief (probability) measure to an individual, not to Nature. This is very interesting, as this definition leaves room for conflicting beliefs between individuals. Again, this is appropriate for what naturally occurs: different individuals have different beliefs of events occurring, because they possess different information about the world. The existence of different beliefs does not imply that anyone is wrong. Consider the following examples demonstrating the relationship between individual beliefs and probabilities:</p>
<ul class="simple">
<li><p>I flip a coin, and we both guess the result. We would both agree, assuming the coin is fair, that the probability of Heads is 1/2. Assume, then, that I peek at the coin. Now I know for certain what the result is: I assign probability 1.0 to either Heads or Tails (whichever it is). Now what is your belief that the coin is Heads? My knowledge of the outcome has not changed the coin’s results. Thus we assign different probabilities to the result.</p></li>
<li><p>Your code either has a bug in it or not, but we do not know for certain which is true, though we have a belief about the presence or absence of a bug.</p></li>
<li><p>A medical patient is exhibiting symptoms <em>x</em>, <em>y</em> and <em>z</em>. There are a number of diseases that could be causing all of them, but only a single disease is present. A doctor has beliefs about which disease, but a second doctor may have slightly different beliefs.</p></li>
</ul>
<p>This philosophy of treating beliefs as probability is natural to humans. We employ it constantly as we interact with the world and only see partial truths, but gather evidence to form beliefs. Alternatively, you have to be trained to think like a frequentist.</p>
<p>To align ourselves with traditional probability notation, we denote our belief about event <span class="math notranslate nohighlight">\(A\)</span> as <span class="math notranslate nohighlight">\(P(A)\)</span>. We call this quantity the prior probability.</p>
<p>John Maynard Keynes, a great economist and thinker, said “When the facts change, I change my mind. What do you do, sir?” This quote reflects the way a Bayesian updates his or her beliefs after seeing evidence. Even — especially — if the evidence is counter to what was initially believed, the evidence cannot be ignored. We denote our updated belief as <span class="math notranslate nohighlight">\(P(A|X)\)</span>, interpreted as the probability of <span class="math notranslate nohighlight">\(A\)</span> given the evidence <span class="math notranslate nohighlight">\(X\)</span>. We call the updated belief the posterior probability so as to contrast it with the prior probability. For example, consider the posterior probabilities (read: posterior beliefs) of the above examples, after observing some evidence <span class="math notranslate nohighlight">\(X\)</span>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A)\)</span>: the coin has a 50 percent chance of being Heads. <span class="math notranslate nohighlight">\(P(A|X)\)</span>: You look at the coin, observe a Heads has landed, denote this information  <span class="math notranslate nohighlight">\(X\)</span>, and trivially assign probability 1.0 to Heads and 0.0 to Tails.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A)\)</span>: This big, complex code likely has a bug in it. <span class="math notranslate nohighlight">\(P(A|X)\)</span>: The code passed all <span class="math notranslate nohighlight">\(X\)</span> tests; there still might be a bug, but its presence is less likely now.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A)\)</span>: The patient could have any number of diseases. <span class="math notranslate nohighlight">\(P(A|X)\)</span>: Performing a blood test generated evidence <span class="math notranslate nohighlight">\(X\)</span>, ruling out some of the possible diseases from consideration.</p></li>
</ol>
<p>It’s clear that in each example we did not completely discard the prior belief after seeing new evidence <span class="math notranslate nohighlight">\(X\)</span>, but we re-weighted the prior to incorporate the new evidence (i.e. we put more weight, or confidence, on some beliefs versus others).</p>
<p>By introducing prior uncertainty about events, we are already admitting that any guess we make is potentially very wrong. After observing data, evidence, or other information, we update our beliefs, and our guess becomes less wrong. This is the alternative side of the prediction coin, where typically we try to be more right.</p>
</div>
<div class="section" id="bayesian-inference-in-practice">
<h2>Bayesian Inference in Practice<a class="headerlink" href="#bayesian-inference-in-practice" title="Permalink to this headline">¶</a></h2>
<p>If frequentist and Bayesian inference were programming functions, with inputs being statistical problems, then the two would be different in what they return to the user. The frequentist inference function would return a number, representing an estimate (typically a summary statistic like the sample average etc.), whereas the Bayesian function would return probabilities.</p>
<p>For example, in our debugging problem above, calling the frequentist function with the argument “My code passed all <span class="math notranslate nohighlight">\(X\)</span> tests; is my code bug-free?” would return a YES. On the other hand, asking our Bayesian function “Often my code has bugs. My code passed all <span class="math notranslate nohighlight">\(X\)</span> tests; is my code bug-free?” would return something very different: probabilities of YES and NO. The function might return:</p>
<blockquote>
<div><p>YES, with probability 0.8; NO, with probability 0.2</p>
</div></blockquote>
<p>This is very different from the answer the frequentist function returned. Notice that the Bayesian function accepted an additional argument: “Often my code has bugs”. This parameter is the prior. By including the prior parameter, we are telling the Bayesian function to include our belief about the situation. Technically this parameter in the Bayesian function is optional, but we will see excluding it has its own consequences.</p>
<div class="section" id="incorporating-evidence">
<h3>Incorporating evidence<a class="headerlink" href="#incorporating-evidence" title="Permalink to this headline">¶</a></h3>
<p>As we acquire more and more instances of evidence, our prior belief is washed out by the new evidence. This is to be expected. For example, if your prior belief is something ridiculous, like “I expect the sun to explode today”, and each day you are proved wrong, you would hope that any inference would correct you, or at least align your beliefs better. Bayesian inference will correct this belief.</p>
<p>Denote <span class="math notranslate nohighlight">\(N\)</span> as the number of instances of evidence we possess. As we gather an infinite amount of evidence, say as <span class="math notranslate nohighlight">\(N→∞,\)</span> our Bayesian results (often) align with frequentist results. Hence for large N, statistical inference is more or less objective. On the other hand, for small <span class="math notranslate nohighlight">\(N\)</span>, inference is much more unstable: frequentist estimates have more variance and larger confidence intervals. This is where Bayesian analysis excels. By introducing a prior, and returning probabilities (instead of a scalar estimate), we preserve the uncertainty that reflects the instability of statistical inference of a small N dataset.</p>
<p>One may think that for large <span class="math notranslate nohighlight">\(N\)</span>, one can be indifferent between the two techniques since they offer similar inference, and might lean towards the computationally-simpler, frequentist methods. An individual in this position should consider the following quote by Andrew Gelman (2005)<a class="reference external" href="#scrollTo=nDdph0r1ABCn">[1]</a>, before making such a decision:</p>
<p>Sample sizes are never large. If <span class="math notranslate nohighlight">\(N\)</span>, is too small to get a sufficiently-precise estimate, you need to get more data (or make more assumptions). But once <span class="math notranslate nohighlight">\(N\)</span>, is “large enough,” you can start subdividing the data to learn more (for example, in a public opinion poll, once you have a good estimate for the entire country, you can estimate among men and women, northerners and southerners, different age groups, etc.). <span class="math notranslate nohighlight">\(N\)</span>, is never enough because if it were “enough” you’d already be on to the next problem for which you need more data.</p>
</div>
</div>
<div class="section" id="are-frequentist-methods-incorrect-then">
<h2>Are frequentist methods incorrect then?<a class="headerlink" href="#are-frequentist-methods-incorrect-then" title="Permalink to this headline">¶</a></h2>
<p>No.</p>
<p>Frequentist methods are still useful or state-of-the-art in many areas. Tools such as least squares linear regression, LASSO regression, and expectation-maximization algorithms are all powerful and fast. Bayesian methods complement these techniques by solving problems that these approaches cannot, or by illuminating the underlying system with more flexible modeling.</p>
<div class="section" id="a-note-on-big-data">
<h3>A note on <em>Big Data</em><a class="headerlink" href="#a-note-on-big-data" title="Permalink to this headline">¶</a></h3>
<p>Paradoxically, big data’s predictive analytic problems are actually solved by relatively simple algorithms <a class="reference external" href="#scrollTo=nDdph0r1ABCn">[2]</a><a class="reference external" href="#scrollTo=nDdph0r1ABCn">[3]</a>. Thus we can argue that big data’s prediction difficulty does not lie in the algorithm used, but instead on the computational difficulties of storage and execution on big data. (One should also consider Gelman’s quote from above and ask “Do I really have big data?”)</p>
<p>The much more difficult analytic problems involve medium data and, especially troublesome, really small data. Using a similar argument as Gelman’s above, if big data problems are big enough to be readily solved, then we should be more interested in the not-quite-big enough datasets.</p>
</div>
</div>
<div class="section" id="our-bayesian-framework">
<h2>Our Bayesian framework<a class="headerlink" href="#our-bayesian-framework" title="Permalink to this headline">¶</a></h2>
<p>We are interested in beliefs, which can be interpreted as probabilities by thinking Bayesian. We have a prior belief in event A, beliefs formed by previous information, e.g., our prior belief about bugs being in our code before performing tests.</p>
<p>Secondly, we observe our evidence. To continue our buggy-code example: if our code passes X tests, we want to update our belief to incorporate this. We call this new belief the posterior probability. Updating our belief is done via the following equation, known as Bayes’ Theorem, after its discoverer Thomas Bayes:</p>
<div class="math notranslate nohighlight">
\[ P(A|X) = \frac{P(X | A) P(A) }{P(X) } \]</div>
<div class="math notranslate nohighlight">
\[ P(A|X) \propto{P(X | A) P(A) } \]</div>
<p>NOTE: (<span class="math notranslate nohighlight">\(\propto\)</span> is “proportional to”)</p>
<p>The above formula is not unique to Bayesian inference: it is a mathematical fact with uses outside Bayesian inference. Bayesian inference merely uses it to connect prior probabilities <span class="math notranslate nohighlight">\(P(A)\)</span> with an updated posterior probabilities <span class="math notranslate nohighlight">\(P(A|X)\)</span>.</p>
</div>
<div class="section" id="example-mandatory-coin-flip-example">
<h2>Example: Mandatory coin-flip example<a class="headerlink" href="#example-mandatory-coin-flip-example" title="Permalink to this headline">¶</a></h2>
<p>Every statistics text must contain a coin-flipping example, I’ll use it here to get it out of the way. Suppose, naively, that you are unsure about the probability of heads in a coin flip (spoiler alert: it’s 50%). You believe there is some true underlying ratio, call it p, but have no prior opinion on what p might be.</p>
<p>We begin to flip a coin, and record the observations: either H or T. This is our observed data. An interesting question to ask is how our inference changes as we observe more and more data? More specifically, what do our posterior probabilities look like when we have little data, versus when we have lots of data.</p>
<p>Below we plot a sequence of updating posterior probabilities as we observe increasing amounts of data (coin flips), while also demonstrating some of the best practices when it comes to evaluating tensors and plotting the data. First, the easy part: We define the values in our Tensorflow graph</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build Graph</span>
<span class="n">rv_coin_flip_prior</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="n">num_trials</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">])</span>

<span class="n">coin_flip_data</span> <span class="o">=</span> <span class="n">rv_coin_flip_prior</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">num_trials</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># prepend a 0 onto tally of heads and tails, for zeroth flip</span>
<span class="n">coin_flip_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">coin_flip_data</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,]]),</span><span class="s2">&quot;CONSTANT&quot;</span><span class="p">)</span>

<span class="c1"># compute cumulative headcounts from 0 to 2000 flips, and then grab them at each of num_trials intervals</span>
<span class="n">cumulative_headcounts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">coin_flip_data</span><span class="p">),</span> <span class="n">num_trials</span><span class="p">)</span>

<span class="n">rv_observed_heads</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span>
    <span class="n">concentration1</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">cumulative_headcounts</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">concentration0</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">num_trials</span> <span class="o">-</span> <span class="n">cumulative_headcounts</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">probs_of_heads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;linspace&quot;</span><span class="p">)</span>
<span class="n">observed_probs_heads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">rv_observed_heads</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">probs_of_heads</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we move onto plotting our tensors in matplotlib.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For the already prepared, I&#39;m using Binomial&#39;s conj. prior.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_trials</span><span class="p">)):</span>
    <span class="n">sx</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_trials</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$p$, probability of heads&quot;</span><span class="p">)</span> \
    <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_trials</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">sx</span><span class="o">.</span><span class="n">get_yticklabels</span><span class="p">(),</span> <span class="n">visible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">probs_of_heads</span><span class="p">,</span> <span class="n">observed_probs_heads</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> 
             <span class="n">label</span><span class="o">=</span><span class="s2">&quot;observe </span><span class="si">%d</span><span class="s2"> tosses,</span><span class="se">\n</span><span class="s2"> </span><span class="si">%d</span><span class="s2"> heads&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_trials</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cumulative_headcounts</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">probs_of_heads</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">observed_probs_heads</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> 
                     <span class="n">color</span><span class="o">=</span><span class="n">TFColor</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">leg</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">leg</span><span class="o">.</span><span class="n">get_frame</span><span class="p">()</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">autoscale</span><span class="p">(</span><span class="n">tight</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Bayesian updating of posterior probabilities&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">,</span>
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Ch1_Introduction_TFP_14_0.png" src="../../../_images/Ch1_Introduction_TFP_14_0.png" />
</div>
</div>
<p>The posterior probabilities are represented by the curves, and our uncertainty is proportional to the width of the curve. As the plot above shows, as we start to observe data our posterior probabilities start to shift and move around. Eventually, as we observe more and more data (coin-flips), our probabilities will tighten closer and closer around the true value of <span class="math notranslate nohighlight">\(p=0.5\)</span> (marked by a dashed line).</p>
<p>Notice that the plots are not always peaked at 0.5. There is no reason it should be: recall we assumed we did not have a prior opinion of what p is. In fact, if we observe quite extreme data, say 8 flips and only 1 observed heads, our distribution would look very biased away from lumping around 0.5 (with no prior opinion, how confident would you feel betting on a fair coin after observing 8 tails and 1 head?). As more data accumulates, we would see more and more probability being assigned at <span class="math notranslate nohighlight">\(p=0.5\)</span>, though never all of it.</p>
<p>The next example is a simple demonstration of the mathematics of Bayesian inference.</p>
</div>
<div class="section" id="example-bug-or-just-sweet-unintended-feature">
<h2>Example: Bug, or just sweet, unintended feature?<a class="headerlink" href="#example-bug-or-just-sweet-unintended-feature" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> denote the event that our code has no bugs in it. Let <span class="math notranslate nohighlight">\(X\)</span> denote the event that the code passes all debugging tests. For now, we will leave the prior probability of no bugs as a variable, i.e. <span class="math notranslate nohighlight">\(P(A)=p\)</span>.</p>
<p>We are interested in <span class="math notranslate nohighlight">\(P(A|X)\)</span>, i.e. the probability of no bugs, given our debugging tests <span class="math notranslate nohighlight">\(X\)</span>. To use the formula above, we need to compute some quantities.</p>
<p>What is <span class="math notranslate nohighlight">\(P(X|A)\)</span>, i.e., the probability that the code passes <span class="math notranslate nohighlight">\(X\)</span> tests given there are no bugs? Well, it is equal to 1, for a code with no bugs will pass all tests.</p>
<p><span class="math notranslate nohighlight">\(P(X)\)</span> is a little bit trickier: The event <span class="math notranslate nohighlight">\(X\)</span> can be divided into two possibilities, event X occurring even though our code indeed has bugs (denoted <span class="math notranslate nohighlight">\(∼A\)</span>, spoken not <span class="math notranslate nohighlight">\(A\)</span>), or event <span class="math notranslate nohighlight">\(X\)</span> without bugs <span class="math notranslate nohighlight">\((A)\)</span>. <span class="math notranslate nohighlight">\( P(X)\)</span> can be represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
P(A|X) &amp;= \frac{P(X | A) P(A) }{P(X) } \\
 P(X) &amp;= P(X \text{ and } A) + P(X \text{ and } \sim A) \\
  &amp;= P(X|A)P(A) + P(X | \sim A)P(\sim A) \\
  &amp;= P(X|A)p + P(X | \sim A)(1-p) \end{align*} \end{split}\]</div>
<p>We have already computed <span class="math notranslate nohighlight">\(P(X|A)\)</span> above. On the other hand, <span class="math notranslate nohighlight">\(P(X|\sim A)\)</span> is subjective: our code can pass tests but still have a bug in it, though the probability there is a bug present is reduced. Note this is dependent on the number of tests performed, the degree of complication in the tests, etc. Let’s be conservative and assign <span class="math notranslate nohighlight">\(P(X|\sim A)=0.5\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
P(A | X) &amp;= \frac{1\cdot p}{ 1\cdot p +0.5 (1-p) } \\
&amp;= \frac{ 2 p}{1+p} \end{align*} \end{split}\]</div>
<p>This is the posterior probability. What does it look like as a function of our prior, <span class="math notranslate nohighlight">\(p\in[0,1]\)</span>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining our range of probabilities</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># Visualization.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">p</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">p</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">TFColor</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1">#plt.fill_between(p, 2*p/(1+p), alpha=.5, facecolor=[&quot;#A60628&quot;])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span><span class="o">/</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">TFColor</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Prior, $P(A) = p$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Posterior, $P(A|X)$, with $P(A) = p$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Are there bugs in my code?&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Ch1_Introduction_TFP_19_0.png" src="../../../_images/Ch1_Introduction_TFP_19_0.png" />
</div>
</div>
<p>We can see the biggest gains if we observe the <span class="math notranslate nohighlight">\(X\)</span> tests passed when the prior probability, <span class="math notranslate nohighlight">\(p\)</span>, is low. Let’s settle on a specific value for the prior. I’m a strong programmer (I think), so I’m going to give myself a realistic prior of 0.20, that is, there is a 20% chance that I write code bug-free. To be more realistic, this prior should be a function of how complicated and large the code is, but let’s pin it at 0.20. Then my updated belief that my code is bug-free is 0.33.</p>
<p>Recall that the prior is a probability: <span class="math notranslate nohighlight">\(p\)</span> is the prior probability that there are no bugs, so <span class="math notranslate nohighlight">\(1 \text{-} p\)</span> is the prior probability that there are bugs.</p>
<p>Similarly, our posterior is also a probability, with <span class="math notranslate nohighlight">\(P(A|X)\)</span> the probability there is no bug given we saw all tests pass, hence <span class="math notranslate nohighlight">\(1 \text{-} P(A|X)\)</span> is the probability there is a bug given all tests passed. What does our posterior probability look like? Below is a chart of both the prior and the posterior probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining our priors and posteriors</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">])</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mf">2.</span><span class="o">/</span><span class="mi">3</span><span class="p">])</span>

<span class="c1"># Our Simple Visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">colours</span> <span class="o">=</span> <span class="p">[</span><span class="n">TFColor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">TFColor</span><span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.7</span><span class="p">],</span> <span class="n">prior</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.70</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior distribution&quot;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="s2">&quot;3&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="mi">0</span><span class="o">+</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">.7</span><span class="o">+</span><span class="mf">0.25</span><span class="p">],</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;posterior distribution&quot;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="s2">&quot;3&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mf">0.20</span><span class="p">,</span> <span class="mf">.95</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Bugs Absent&quot;</span><span class="p">,</span> <span class="s2">&quot;Bugs Present&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Prior and Posterior probability of bugs present&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Ch1_Introduction_TFP_21_0.png" src="../../../_images/Ch1_Introduction_TFP_21_0.png" />
</div>
</div>
<p>Notice that after we observed <span class="math notranslate nohighlight">\(X\)</span> occur, the probability of bugs being absent increased. By increasing the number of tests, we can approach confidence (probability 1) that there are no bugs present.</p>
<p>This was a very simple example of Bayesian inference and Bayes rule. Unfortunately, the mathematics necessary to perform more complicated Bayesian inference only becomes more difficult, except for artificially constructed cases. We will later see that this type of mathematical analysis is actually unnecessary. First we must broaden our modeling tools. The next section deals with probability distributions. If you are already familiar, feel free to skip (or at least skim), but for the less familiar the next section is essential.</p>
</div>
<div class="section" id="probability-distributions">
<h2>Probability Distributions<a class="headerlink" href="#probability-distributions" title="Permalink to this headline">¶</a></h2>
<p>Let’s quickly recall what a probability distribution is: Let <span class="math notranslate nohighlight">\(Z\)</span> be some random variable. Then associated with <span class="math notranslate nohighlight">\(Z\)</span> is a probability distribution function that assigns probabilities to the different outcomes <span class="math notranslate nohighlight">\(Z\)</span> can take. Graphically, a probability distribution is a curve where the probability of an outcome is proportional to the height of the curve. You can see examples in the first figure of this chapter.</p>
<p>We can divide random variables into three classifications:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Z\)</span> is discrete: Discrete random variables may only assume values on a specified list. Things like populations, movie ratings, and number of votes are all discrete random variables. Discrete random variables become more clear when we contrast them with…</p></li>
<li><p><span class="math notranslate nohighlight">\(Z\)</span> is continuous: Continuous random variable can take on arbitrarily exact values. For example, temperature, speed, time, color are all modeled as continuous variables because you can progressively make the values more and more precise.</p></li>
<li><p><span class="math notranslate nohighlight">\(Z\)</span> is mixed: Mixed random variables assign probabilities to both discrete and continuous random variables, i.e. it is a combination of the above two categories.</p></li>
</ul>
<div class="section" id="discrete-case">
<h3>Discrete Case<a class="headerlink" href="#discrete-case" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(Z\)</span> is discrete, then its distribution is called a <em>probability mass function</em>, which measures the probability <span class="math notranslate nohighlight">\(Z\)</span> takes on the value <span class="math notranslate nohighlight">\(k\)</span>, denoted <span class="math notranslate nohighlight">\(P(Z=k)\)</span>. Note that the probability mass function completely describes the random variable <span class="math notranslate nohighlight">\(Z\)</span>, that is, if we know the mass function, we know how <span class="math notranslate nohighlight">\(Z\)</span> should behave. There are popular probability mass functions that consistently appear: we will introduce them as needed, but let’s introduce the first very useful probability mass function. We say <span class="math notranslate nohighlight">\(Z\)</span> is <em>Poisson</em>-distributed if:</p>
<div class="math notranslate nohighlight">
\[P(Z = k) =\frac{ \lambda^k e^{-\lambda} }{k!}, \; \; k=0,1,2, \dots \]</div>
<p><span class="math notranslate nohighlight">\(\lambda\)</span> is called a parameter of the distribution, and it controls the distribution’s shape. For the Poisson distribution, <span class="math notranslate nohighlight">\(\lambda\)</span> can be any positive number. By increasing <span class="math notranslate nohighlight">\(\lambda\)</span>, we add more probability to larger values, and conversely by decreasing <span class="math notranslate nohighlight">\(\lambda\)</span> we add more probability to smaller values. One can describe <span class="math notranslate nohighlight">\(\lambda\)</span> as the <em>intensity</em> of the Poisson distribution.</p>
<p>Unlike <span class="math notranslate nohighlight">\(\lambda\)</span>, which can be any positive number, the value <span class="math notranslate nohighlight">\(k\)</span> in the above formula must be a non-negative integer, i.e., <span class="math notranslate nohighlight">\(k\)</span> must take on values 0,1,2, and so on. This is very important, because if you wanted to model a population you could not make sense of populations with 4.25 or 5.612 members.</p>
<p>If a random variable <span class="math notranslate nohighlight">\(Z\)</span> has a Poisson mass distribution, we denote this by writing</p>
<div class="math notranslate nohighlight">
\[Z \sim \text{Poi}(\lambda) \]</div>
<p>One useful property of the Poisson distribution is that its expected value is equal to its parameter, i.e.:</p>
<div class="math notranslate nohighlight">
\[E\large[ \;Z\; | \; \lambda \;\large] = \lambda \]</div>
<p>We will use this property often, so it’s useful to remember. Below, we plot the probability mass distribution for different <span class="math notranslate nohighlight">\(\lambda\)</span> values. The first thing to notice is that by increasing <span class="math notranslate nohighlight">\(\lambda\)</span>, we add more probability of larger values occurring. Second, notice that although the graph ends at 15, the distributions do not. They assign positive probability to every non-negative integer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build graph.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span> <span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mf">16.</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">lambdas</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">4.25</span><span class="p">])</span>

<span class="n">poi_pmf</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span>
  <span class="n">rate</span><span class="o">=</span><span class="n">lambdas</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Display results in two different histograms, for easier comparison</span>
<span class="n">colours</span> <span class="o">=</span> <span class="p">[</span><span class="n">TFColor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">TFColor</span><span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_autoscaley_on</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Probability mass function of a Poisson random variable&quot;</span><span class="p">);</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
          <span class="n">poi_pmf</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
          <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
          <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\lambda = </span><span class="si">%.1f</span><span class="s2">$&quot;</span> <span class="o">%</span> <span class="n">lambdas</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.60</span><span class="p">,</span>
          <span class="n">edgecolor</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="s2">&quot;3&quot;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.5</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;probability of $k$&quot;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$k$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Ch1_Introduction_TFP_25_0.png" src="../../../_images/Ch1_Introduction_TFP_25_0.png" />
</div>
</div>
</div>
<div class="section" id="continuous-case">
<h3>Continuous Case<a class="headerlink" href="#continuous-case" title="Permalink to this headline">¶</a></h3>
<p>Instead of a probability mass function, a continuous random variable has a <em>probability density function</em>. This might seem like unnecessary nomenclature, but the density function and the mass function are very different creatures. An example of continuous random variable is a random variable with <em>exponential density</em>. The density function for an exponential random variable looks like this:</p>
<div class="math notranslate nohighlight">
\[f_Z(z | \lambda) = \lambda e^{-\lambda z }, \;\; z\ge 0\]</div>
<p>Like a Poisson random variable, an exponential random variable can take on only non-negative values. But unlike a Poisson variable, the exponential can take on <em>any</em> non-negative values, including non-integral values such as 4.25 or 5.612401. This property makes it a poor choice for count data, which must be an integer, but a great choice for time data, temperature data (measured in Kelvins, of course), or any other precise <em>and positive</em> variable. The graph below shows two probability density functions with different <span class="math notranslate nohighlight">\(\lambda\)</span> values.</p>
<p>When a random variable <span class="math notranslate nohighlight">\(Z\)</span> has an exponential distribution with parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, we say <em><span class="math notranslate nohighlight">\(Z\)</span> is exponential</em> and write</p>
<div class="math notranslate nohighlight">
\[Z \sim \text{Exp}(\lambda)\]</div>
<p>Given a specific <span class="math notranslate nohighlight">\(\lambda\)</span>, the expected value of an exponential random variable is equal to the inverse of <span class="math notranslate nohighlight">\(\lambda\)</span>, that is:</p>
<div class="math notranslate nohighlight">
\[E[\; Z \;|\; \lambda \;] = \frac{1}{\lambda}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining our Data and assumptions (use tf.linspace for continuous)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">0.04</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">lambdas</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>

<span class="c1"># Now we use TFP to compute probabilities in a vectorized manner.</span>
<span class="n">expo_pdf</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">lambdas</span><span class="p">)</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1"># Visualizing our results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lambdas</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">expo_pdf</span><span class="p">)[</span><span class="n">i</span><span class="p">],</span>
             <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">TFColor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\lambda = </span><span class="si">%.1f</span><span class="s2">$&quot;</span> <span class="o">%</span> <span class="n">lambdas</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">expo_pdf</span><span class="p">)[</span><span class="n">i</span><span class="p">],</span>
                         <span class="n">color</span><span class="o">=</span><span class="n">TFColor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.33</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PDF at $z$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$z$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Probability density function of an Exponential random variable; differing $\lambda$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Ch1_Introduction_TFP_27_0.png" src="../../../_images/Ch1_Introduction_TFP_27_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="but-what-is-lambda">
<h2>But what is <span class="math notranslate nohighlight">\(\lambda \;\)</span>?<a class="headerlink" href="#but-what-is-lambda" title="Permalink to this headline">¶</a></h2>
<p><strong>This question is what motivates statistics</strong>. In the real world, <span class="math notranslate nohighlight">\(\lambda\)</span> is hidden from us. We see only <span class="math notranslate nohighlight">\(Z\)</span>, and must go backwards to try and determine <span class="math notranslate nohighlight">\(\lambda\)</span>. The problem is difficult because there is no one-to-one mapping from <span class="math notranslate nohighlight">\(Z\)</span> to <span class="math notranslate nohighlight">\(\lambda\)</span>. Many different methods have been created to solve the problem of estimating <span class="math notranslate nohighlight">\(\lambda\)</span>, but since <span class="math notranslate nohighlight">\(\lambda\)</span> is never actually observed, no one can say for certain which method is best!</p>
<p>Bayesian inference is concerned with <em>beliefs</em> about what <span class="math notranslate nohighlight">\(\lambda\)</span> might be. Rather than try to guess <span class="math notranslate nohighlight">\(\lambda\)</span> exactly, we can only talk about what <span class="math notranslate nohighlight">\(\lambda\)</span> is likely to be by assigning a probability distribution to <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>This might seem odd at first. After all, <span class="math notranslate nohighlight">\(\lambda\)</span> is fixed; it is not (necessarily) random! How can we assign probabilities to values of a non-random variable? Ah, we have fallen for our old, frequentist way of thinking. Recall that under Bayesian philosophy, we <em>can</em> assign probabilities if we interpret them as beliefs. And it is entirely acceptable to have <em>beliefs</em> about the parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="section" id="example-inferring-behaviour-from-text-message-data">
<h3>Example: Inferring behaviour from text-message data<a class="headerlink" href="#example-inferring-behaviour-from-text-message-data" title="Permalink to this headline">¶</a></h3>
<p>Let’s try to model a more interesting example, one that concerns the rate at which a user sends and receives text messages:</p>
<blockquote>
<div><p>You are given a series of daily text-message counts from a user of your system. The data, plotted over time, appears in the chart below. You are curious to know if the user’s text-messaging habits have changed over time, either gradually or suddenly. How can you model this? (This is in fact my own text-message data. Judge my popularity as you wish.)</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining our Data and assumptions</span>
<span class="n">count_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="mi">13</span><span class="p">,</span>  <span class="mi">24</span><span class="p">,</span>   <span class="mi">8</span><span class="p">,</span>  <span class="mi">24</span><span class="p">,</span>   <span class="mi">7</span><span class="p">,</span>  <span class="mi">35</span><span class="p">,</span>  <span class="mi">14</span><span class="p">,</span>  <span class="mi">11</span><span class="p">,</span>  <span class="mi">15</span><span class="p">,</span>  <span class="mi">11</span><span class="p">,</span>  <span class="mi">22</span><span class="p">,</span>  <span class="mi">22</span><span class="p">,</span>  <span class="mi">11</span><span class="p">,</span>  <span class="mi">57</span><span class="p">,</span>  
    <span class="mi">11</span><span class="p">,</span>  <span class="mi">19</span><span class="p">,</span>  <span class="mi">29</span><span class="p">,</span>   <span class="mi">6</span><span class="p">,</span>  <span class="mi">19</span><span class="p">,</span>  <span class="mi">12</span><span class="p">,</span>  <span class="mi">22</span><span class="p">,</span>  <span class="mi">12</span><span class="p">,</span>  <span class="mi">18</span><span class="p">,</span>  <span class="mi">72</span><span class="p">,</span>  <span class="mi">32</span><span class="p">,</span>   <span class="mi">9</span><span class="p">,</span>   <span class="mi">7</span><span class="p">,</span>  <span class="mi">13</span><span class="p">,</span>  
    <span class="mi">19</span><span class="p">,</span>  <span class="mi">23</span><span class="p">,</span>  <span class="mi">27</span><span class="p">,</span>  <span class="mi">20</span><span class="p">,</span>   <span class="mi">6</span><span class="p">,</span>  <span class="mi">17</span><span class="p">,</span>  <span class="mi">13</span><span class="p">,</span>  <span class="mi">10</span><span class="p">,</span>  <span class="mi">14</span><span class="p">,</span>   <span class="mi">6</span><span class="p">,</span>  <span class="mi">16</span><span class="p">,</span>  <span class="mi">15</span><span class="p">,</span>   <span class="mi">7</span><span class="p">,</span>   <span class="mi">2</span><span class="p">,</span>  
    <span class="mi">15</span><span class="p">,</span>  <span class="mi">15</span><span class="p">,</span>  <span class="mi">19</span><span class="p">,</span>  <span class="mi">70</span><span class="p">,</span>  <span class="mi">49</span><span class="p">,</span>   <span class="mi">7</span><span class="p">,</span>  <span class="mi">53</span><span class="p">,</span>  <span class="mi">22</span><span class="p">,</span>  <span class="mi">21</span><span class="p">,</span>  <span class="mi">31</span><span class="p">,</span>  <span class="mi">19</span><span class="p">,</span>  <span class="mi">11</span><span class="p">,</span>  <span class="mi">18</span><span class="p">,</span>  <span class="mi">20</span><span class="p">,</span>  
    <span class="mi">12</span><span class="p">,</span>  <span class="mi">35</span><span class="p">,</span>  <span class="mi">17</span><span class="p">,</span>  <span class="mi">23</span><span class="p">,</span>  <span class="mi">17</span><span class="p">,</span>   <span class="mi">4</span><span class="p">,</span>   <span class="mi">2</span><span class="p">,</span>  <span class="mi">31</span><span class="p">,</span>  <span class="mi">30</span><span class="p">,</span>  <span class="mi">13</span><span class="p">,</span>  <span class="mi">27</span><span class="p">,</span>   <span class="mi">0</span><span class="p">,</span>  <span class="mi">39</span><span class="p">,</span>  <span class="mi">37</span><span class="p">,</span>   
    <span class="mi">5</span><span class="p">,</span>  <span class="mi">14</span><span class="p">,</span>  <span class="mi">13</span><span class="p">,</span>  <span class="mi">22</span><span class="p">,</span>
<span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">n_count_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">count_data</span><span class="p">)</span>
<span class="n">days</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">n_count_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="c1"># Visualizing the Results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">days</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">count_data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#5DA5DA&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (days)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;count of text-msgs received&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Did the user&#39;s texting habits change over time?&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_count_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">());</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Ch1_Introduction_TFP_30_0.png" src="../../../_images/Ch1_Introduction_TFP_30_0.png" />
</div>
</div>
<p>Before we start modeling, see what you can figure out just by looking at the chart above. Would you say there was a change in behaviour during this time period?</p>
<p>How can we start to model this? Well, as we have conveniently already seen, a Poisson random variable is a very appropriate model for this type of <em>count</em> data. Denoting day <span class="math notranslate nohighlight">\(i\)</span>’s text-message count by <span class="math notranslate nohighlight">\(C_i\)</span>,</p>
<div class="math notranslate nohighlight">
\[ C_i \sim \text{Poisson}(\lambda)  \]</div>
<p>We are not sure what the value of the <span class="math notranslate nohighlight">\(\lambda\)</span> parameter really is, however. Looking at the chart above, it appears that the rate might become higher late in the observation period, which is equivalent to saying that <span class="math notranslate nohighlight">\(\lambda\)</span> increases at some point during the observations. (Recall that a higher value of <span class="math notranslate nohighlight">\(\lambda\)</span> assigns more probability to larger outcomes. That is, there is a higher probability of many text messages having been sent on a given day.)</p>
<p>How can we represent this observation mathematically? Let’s assume that on some day during the observation period (call it <span class="math notranslate nohighlight">\(\tau\)</span>), the parameter <span class="math notranslate nohighlight">\(\lambda\)</span> suddenly jumps to a higher value. So we really have two <span class="math notranslate nohighlight">\(\lambda\)</span> parameters: one for the period before <span class="math notranslate nohighlight">\(\tau\)</span>, and one for the rest of the observation period. In the literature, a sudden transition like this would be called a <em>switchpoint</em>:</p>
<div class="math notranslate nohighlight">
\[\lambda = 
\begin{cases} \lambda_1  &amp; \text{if } t \lt \tau \cr
\lambda_2 &amp; \text{if } t \ge \tau
\end{cases}
\]</div>
<p>If, in reality, no sudden change occurred and indeed <span class="math notranslate nohighlight">\(\lambda_1 = \lambda_2\)</span>, then the <span class="math notranslate nohighlight">\(\lambda\)</span>s posterior distributions should look about equal.</p>
<p>We are interested in inferring the unknown <span class="math notranslate nohighlight">\(\lambda\)</span>s. To use Bayesian inference, we need to assign prior probabilities to the different possible values of <span class="math notranslate nohighlight">\(\lambda\)</span>. What would be good prior probability distributions for <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span>? Recall that <span class="math notranslate nohighlight">\(\lambda\)</span> can be any positive number. As we saw earlier, the <em>exponential</em> distribution provides a continuous density function for positive numbers, so it might be a good choice for modeling <span class="math notranslate nohighlight">\(\lambda_i\)</span>. But recall that the exponential distribution takes a parameter of its own, so we’ll need to include that parameter in our model. Let’s call that parameter <span class="math notranslate nohighlight">\(\alpha\)</span>.
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-02156904-ee43-4598-ad18-a03b6eb2a2bd">
<span class="eqno">()<a class="headerlink" href="#equation-02156904-ee43-4598-ad18-a03b6eb2a2bd" title="Permalink to this equation">¶</a></span>\[\begin{align}
&amp;\lambda_1 \sim \text{Exp}( \alpha ) \\
&amp;\lambda_2 \sim \text{Exp}( \alpha )
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
$\alpha$ is called a *hyper-parameter* or *parent variable*. In literal terms, it is a parameter that influences other parameters. Our initial guess at $\alpha$ does not influence the model too strongly, so we have some flexibility in our choice.  A good rule of thumb is to set the exponential parameter equal to the inverse of the average of the count data. Since we're modeling $\lambda$ using an exponential distribution, we can use the expected value identity shown earlier to get:\\$$\frac{1}{N}\sum_{i=0}^N \;C_i \approx E[\; \lambda \; |\; \alpha ] = \frac{1}{\alpha}\end{aligned}\end{align} \]</div>
<p>An alternative, and something I encourage the reader to try, would be to have two priors: one for each <span class="math notranslate nohighlight">\(\lambda_i\)</span>. Creating two exponential distributions with different <span class="math notranslate nohighlight">\(\alpha\)</span> values reflects our prior belief that the rate changed at some point during the observations.</p>
<p>What about <span class="math notranslate nohighlight">\(\tau\)</span>? Because of the noisiness of the data, it’s difficult to pick out a priori when <span class="math notranslate nohighlight">\(\tau\)</span> might have occurred. Instead, we can assign a <em>uniform prior belief</em> to every possible day. This is equivalent to saying
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-ee96301d-9adc-47fb-bef9-396e9c5a9510">
<span class="eqno">()<a class="headerlink" href="#equation-ee96301d-9adc-47fb-bef9-396e9c5a9510" title="Permalink to this equation">¶</a></span>\[\begin{align}
&amp; \tau \sim \text{DiscreteUniform(1,70) }\\
&amp; \Rightarrow P( \tau = k ) = \frac{1}{70}
\end{align}\]</div>
<p>$$
So after all this, what does our overall prior distribution for the unknown variables look like? Frankly, <em>it doesn’t matter</em>. What we should understand is that it’s an ugly, complicated mess involving symbols only a mathematician could love. And things will only get uglier the more complicated our models become. Regardless, all we really care about is the posterior distribution.</p>
<p>We next turn to <a class="reference external" href="https://tensorflow.org/probability">TensorFlow Probability</a>, a Python library for performing Bayesian analysis that is undaunted by the mathematical monster we have created.</p>
</div>
</div>
<div class="section" id="introducing-our-first-hammer-tensorflow-probability">
<h2>Introducing our first hammer: TensorFlow Probability<a class="headerlink" href="#introducing-our-first-hammer-tensorflow-probability" title="Permalink to this headline">¶</a></h2>
<p>TensorFlow Probability (TFP) is a Python library for programming Bayesian analysis. It is intended for data scientists, statisticians, machine learning practitioners, and scientists. Since it is built on the TensorFlow (TF) stack, it brings the runtime benefits of TF to Bayesian analysis. These include write-once run-many (ability to run your development model in production) and speedups via state-of-the-art hardware (GPUs and TPUs).</p>
<p>Since TFP is relatively new, the TFP community is actively developing documentation,
especially docs and examples that bridge the gap between beginner and hacker. One of this book’s main goals is to solve that problem, and also to demonstrate why TFP is so cool.</p>
<p>We will model the problem above using TFP. This type of programming is called <em>probabilistic programming</em>, an unfortunate misnomer that invokes ideas of randomly-generated code and has likely confused and frightened users away from this field. The code is not random; it is probabilistic in the sense that we create probability models using programming variables as the model’s components.</p>
<p>B. Cronin <a class="reference external" href="#scrollTo=nDdph0r1ABCn">[4]</a> has a very motivating description of probabilistic programming:</p>
<blockquote>
<div><p>Another way of thinking about this: unlike a traditional program, which only runs in the forward directions, a probabilistic program is run in both the forward and backward direction. It runs forward to compute the consequences of the assumptions it contains about the world (i.e., the model space it represents), but it also runs backward from the data to constrain the possible explanations. In practice, many probabilistic programming systems will cleverly interleave these forward and backward operations to efficiently home in on the best explanations.</p>
</div></blockquote>
<p>Because of the confusion engendered by the term <em>probabilistic programming</em>, I’ll refrain from using it. Instead, I’ll simply say <em>programming</em>, since that’s what it really is.</p>
<p>TFP code is easy to read. The only novel thing should be the syntax. Simply remember that we are representing the model’s components (<span class="math notranslate nohighlight">\(\tau, \lambda_1, \lambda_2\)</span> ) as variables.</p>
</div>
<div class="section" id="specify-the-joint-log-density">
<h2>Specify the joint log-density<a class="headerlink" href="#specify-the-joint-log-density" title="Permalink to this headline">¶</a></h2>
<p>We’ll assume the data is a consequence of the following generative model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\lambda_{1}^{(0)} &amp;\sim \text{Exponential}(\text{rate}=\alpha) \\
\lambda_{2}^{(0)} &amp;\sim \text{Exponential}(\text{rate}=\alpha) \\
\tau &amp;\sim \text{Uniform}[\text{low}=0,\text{high}=1) \\
\text{for }  i &amp;= 1\ldots N: \\
\lambda_i &amp;= \begin{cases} \lambda_{1}^{(0)}, &amp; \tau &gt; i/N \\ \lambda_{2}^{(0)}, &amp; \text{otherwise}\end{cases}\\
 X_i &amp;\sim \text{Poisson}(\text{rate}=\lambda_i)
\end{align*}\end{split}\]</div>
<p>Happily, this model can be easily implemented using TF and TFP’s distributions:</p>
<p>This code creates a new function <code class="docutils literal notranslate"><span class="pre">lambda_</span></code>, but really we can think of it as a random variable: the random variable <span class="math notranslate nohighlight">\(\lambda\)</span> from above. The <a class="reference external" href="https://https://www.tensorflow.org/api_docs/python/tf/gather">gather</a> function assigns <code class="docutils literal notranslate"><span class="pre">lambda_1</span></code> or <code class="docutils literal notranslate"><span class="pre">lambda_2</span></code> as the value of <code class="docutils literal notranslate"><span class="pre">lambda_</span></code>, depending on what side of <code class="docutils literal notranslate"><span class="pre">tau</span></code> we are on. The values of <code class="docutils literal notranslate"><span class="pre">lambda_</span></code> up until <code class="docutils literal notranslate"><span class="pre">tau</span></code> are <code class="docutils literal notranslate"><span class="pre">lambda_1</span></code> and the values afterwards are <code class="docutils literal notranslate"><span class="pre">lambda_2</span></code>.</p>
<p>Note that because <code class="docutils literal notranslate"><span class="pre">lambda_1</span></code>, <code class="docutils literal notranslate"><span class="pre">lambda_2</span></code> and <code class="docutils literal notranslate"><span class="pre">tau</span></code> are random, <code class="docutils literal notranslate"><span class="pre">lambda_</span></code> will be random. We are <strong>not</strong> fixing any variables yet.</p>
<p>TFP performs probabilistic inference by evaluating the model parameters using a joint_log_prob function, which we’ll describe more in Chapter 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">joint_log_prob</span><span class="p">(</span><span class="n">count_data</span><span class="p">,</span> <span class="n">lambda_1</span><span class="p">,</span> <span class="n">lambda_2</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>
 
    <span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">count_data</span><span class="p">))</span>
    <span class="n">rv_lambda_1</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">rv_lambda_2</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
 
    <span class="n">rv_tau</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Uniform</span><span class="p">()</span>
 
    <span class="n">lambda_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
         <span class="p">[</span><span class="n">lambda_1</span><span class="p">,</span> <span class="n">lambda_2</span><span class="p">],</span>
         <span class="n">indices</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">count_data</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">count_data</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="n">rv_observation</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">lambda_</span><span class="p">)</span>
 
    <span class="k">return</span> <span class="p">(</span>
         <span class="n">rv_lambda_1</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">lambda_1</span><span class="p">)</span>
         <span class="o">+</span> <span class="n">rv_lambda_2</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">lambda_2</span><span class="p">)</span>
         <span class="o">+</span> <span class="n">rv_tau</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
         <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">rv_observation</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">count_data</span><span class="p">))</span>
    <span class="p">)</span>


<span class="c1"># Define a closure over our joint_log_prob.</span>
<span class="k">def</span> <span class="nf">unnormalized_log_posterior</span><span class="p">(</span><span class="n">lambda1</span><span class="p">,</span> <span class="n">lambda2</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">joint_log_prob</span><span class="p">(</span><span class="n">count_data</span><span class="p">,</span> <span class="n">lambda1</span><span class="p">,</span> <span class="n">lambda2</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that the implementation is arguably very close to being a 1:1 translation of the mathematical model. The main difference is merely that once we’ve specified the probabilistic model, we return the sum of the log_probs.</p>
</div>
<div class="section" id="specify-the-posterior-sampler">
<h2>Specify the posterior sampler<a class="headerlink" href="#specify-the-posterior-sampler" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># wrap the mcmc sampling call in a @tf.function to speed it up</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">autograph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">graph_sample_chain</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tfp</span><span class="o">.</span><span class="n">mcmc</span><span class="o">.</span><span class="n">sample_chain</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">num_burnin_steps</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">num_results</span> <span class="o">=</span> <span class="mi">20000</span>


<span class="c1"># Set the chain&#39;s start state.</span>
<span class="n">initial_chain_state</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">count_data</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;init_lambda1&quot;</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">count_data</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;init_lambda2&quot;</span><span class="p">),</span>
    <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;init_tau&quot;</span><span class="p">),</span>
<span class="p">]</span>


<span class="c1"># Since HMC operates over unconstrained space, we need to transform the</span>
<span class="c1"># samples so they live in real-space.</span>
<span class="n">unconstraining_bijectors</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">bijectors</span><span class="o">.</span><span class="n">Exp</span><span class="p">(),</span>       <span class="c1"># Maps a positive real to R.</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">bijectors</span><span class="o">.</span><span class="n">Exp</span><span class="p">(),</span>       <span class="c1"># Maps a positive real to R.</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">bijectors</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>   <span class="c1"># Maps [0,1] to R.  </span>
<span class="p">]</span>

<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">kernel</span><span class="o">=</span><span class="n">tfp</span><span class="o">.</span><span class="n">mcmc</span><span class="o">.</span><span class="n">TransformedTransitionKernel</span><span class="p">(</span>
        <span class="n">inner_kernel</span><span class="o">=</span><span class="n">tfp</span><span class="o">.</span><span class="n">mcmc</span><span class="o">.</span><span class="n">HamiltonianMonteCarlo</span><span class="p">(</span>
            <span class="n">target_log_prob_fn</span><span class="o">=</span><span class="n">unnormalized_log_posterior</span><span class="p">,</span>
            <span class="n">num_leapfrog_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span>
            <span class="n">state_gradients_are_stopped</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">bijector</span><span class="o">=</span><span class="n">unconstraining_bijectors</span><span class="p">)</span>

<span class="n">kernel</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">mcmc</span><span class="o">.</span><span class="n">SimpleStepSizeAdaptation</span><span class="p">(</span>
    <span class="n">inner_kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">num_adaptation_steps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_burnin_steps</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">))</span>


<span class="c1"># Sample from the chain.</span>
<span class="p">[</span>
    <span class="n">lambda_1_samples</span><span class="p">,</span>
    <span class="n">lambda_2_samples</span><span class="p">,</span>
    <span class="n">posterior_tau</span><span class="p">,</span>
<span class="p">],</span> <span class="n">kernel_results</span> <span class="o">=</span> <span class="n">graph_sample_chain</span><span class="p">(</span>
    <span class="n">num_results</span><span class="o">=</span><span class="n">num_results</span><span class="p">,</span>
    <span class="n">num_burnin_steps</span><span class="o">=</span><span class="n">num_burnin_steps</span><span class="p">,</span>
    <span class="n">current_state</span><span class="o">=</span><span class="n">initial_chain_state</span><span class="p">,</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">)</span>
    
<span class="n">tau_samples</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">posterior_tau</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">count_data</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;acceptance rate: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">kernel_results</span><span class="o">.</span><span class="n">inner_results</span><span class="o">.</span><span class="n">inner_results</span><span class="o">.</span><span class="n">is_accepted</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;final step size: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">kernel_results</span><span class="o">.</span><span class="n">inner_results</span><span class="o">.</span><span class="n">inner_results</span><span class="o">.</span><span class="n">accepted_results</span><span class="o">.</span><span class="n">step_size</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>acceptance rate: 0.5755000114440918
final step size: 0.030197443440556526
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plot-the-results">
<h2>Plot the Results<a class="headerlink" href="#plot-the-results" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="c1">#histogram of the samples:</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">311</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_autoscaley_on</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">lambda_1_samples</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;posterior of $\lambda_1$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">TFColor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;&quot;&quot;Posterior distributions of the variables $\lambda_1,\;\lambda_2,\;\tau$&quot;&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\lambda_1$ value&quot;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">312</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_autoscaley_on</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">lambda_2_samples</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;posterior of $\lambda_2$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">TFColor</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\lambda_2$ value&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">313</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">tau_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">tau_samples</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">tau_samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">n_count_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;posterior of $\tau$&quot;</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="n">TFColor</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">rwidth</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_count_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.75</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">35</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">count_data</span><span class="p">)</span><span class="o">-</span><span class="mi">20</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tau$ (in days)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;probability&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Ch1_Introduction_TFP_41_0.png" src="../../../_images/Ch1_Introduction_TFP_41_0.png" />
</div>
</div>
</div>
<div class="section" id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">¶</a></h2>
<p>Recall that Bayesian methodology returns a <em>distribution</em>. Hence we now have distributions to describe the unknown <span class="math notranslate nohighlight">\(\lambda\)</span>s and <span class="math notranslate nohighlight">\(\tau\)</span>. What have we gained? Immediately, we can see the uncertainty in our estimates: the wider the distribution, the less certain our posterior belief should be. We can also see what the plausible values for the parameters are: <span class="math notranslate nohighlight">\(\lambda_1\)</span> is around 18 and <span class="math notranslate nohighlight">\(\lambda_2\)</span> is around 23. The posterior distributions of the two <span class="math notranslate nohighlight">\(\lambda\)</span>s are clearly distinct, indicating that it is indeed likely that there was a change in the user’s text-message behaviour.</p>
<p>What other observations can you make? If you look at the original data again, do these results seem reasonable?</p>
<p>Notice also that the posterior distributions for the <span class="math notranslate nohighlight">\(\lambda\)</span>s do not look like exponential distributions, even though our priors for these variables were exponential. In fact, the posterior distributions are not really of any form that we recognize from the original model. But that’s OK! This is one of the benefits of taking a computational point of view. If we had instead done this analysis using mathematical approaches, we would have been stuck with an analytically intractable (and messy) distribution. Our use of a computational approach makes us indifferent to mathematical tractability.</p>
<p>Our analysis also returned a distribution for <span class="math notranslate nohighlight">\(\tau\)</span>. Its posterior distribution looks a little different from the other two because it is a discrete random variable, so it doesn’t assign probabilities to intervals. We can see that near day 45, there was a 50% chance that the user’s behaviour changed. Had no change occurred, or had the change been gradual over time, the posterior distribution of <span class="math notranslate nohighlight">\(\tau\)</span> would have been more spread out, reflecting that many days were plausible candidates for <span class="math notranslate nohighlight">\(\tau\)</span>. By contrast, in the actual results we see that only three or four days make any sense as potential transition points.</p>
<div class="section" id="why-would-i-want-samples-from-the-posterior-anyways">
<h3>Why would I want samples from the posterior, anyways?<a class="headerlink" href="#why-would-i-want-samples-from-the-posterior-anyways" title="Permalink to this headline">¶</a></h3>
<p>We will deal with this question for the remainder of the book, and it is an understatement to say that it will lead us to some amazing results. For now, let’s end this chapter with one more example.</p>
<p>We’ll use the posterior samples to answer the following question: what is the expected number of texts at day <span class="math notranslate nohighlight">\(t, \; 0 \le t \le 70\)</span> ? Recall that the expected value of a Poisson variable is equal to its parameter <span class="math notranslate nohighlight">\(\lambda\)</span>. Therefore, the question is equivalent to <em>what is the expected value of <span class="math notranslate nohighlight">\(\lambda\)</span> at time <span class="math notranslate nohighlight">\(t\)</span></em>?</p>
<p>In the code below, let <span class="math notranslate nohighlight">\(i\)</span> index samples from the posterior distributions. Given a day <span class="math notranslate nohighlight">\(t\)</span>, we average over all possible <span class="math notranslate nohighlight">\(\lambda_i\)</span> for that day <span class="math notranslate nohighlight">\(t\)</span>, using <span class="math notranslate nohighlight">\(\lambda_i = \lambda_{1,i}\)</span> if <span class="math notranslate nohighlight">\(t \lt \tau_i\)</span> (that is, if the behaviour change has not yet occurred), else we use <span class="math notranslate nohighlight">\(\lambda_i = \lambda_{2,i}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># tau_samples, lambda_1_samples, lambda_2_samples contain</span>
<span class="c1"># N samples from the corresponding posterior distribution</span>

<span class="n">N_</span> <span class="o">=</span> <span class="n">tau_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">expected_texts_per_day</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N_</span><span class="p">,</span><span class="n">n_count_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1">#(10000,74)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">12.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="n">day_range</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n_count_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">delta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="c1"># expand from shape of 74 to (10000,74)</span>
<span class="n">day_range</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">day_range</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">day_range</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">day_range</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">N_</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># expand from shape of 10000 to 10000,74</span>
<span class="n">tau_samples_per_day</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tau_samples</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tau_samples_per_day</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tau_samples_per_day</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">day_range</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">])))</span>

<span class="n">tau_samples_per_day</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tau_samples_per_day</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="c1">#ix_day is (10000,74) tensor where axis=0 is number of samples, axis=1 is day. each value is true iff sampleXday value is &lt; tau_sample value</span>
<span class="n">ix_day</span> <span class="o">=</span> <span class="n">day_range</span> <span class="o">&lt;</span> <span class="n">tau_samples_per_day</span>

<span class="n">lambda_1_samples_per_day</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">lambda_1_samples</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lambda_1_samples_per_day</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">lambda_1_samples_per_day</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">day_range</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">])))</span>
<span class="n">lambda_2_samples_per_day</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">lambda_2_samples</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lambda_2_samples_per_day</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">lambda_2_samples_per_day</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">day_range</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">])))</span>

<span class="n">expected_texts_per_day</span> <span class="o">=</span> <span class="p">((</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">lambda_1_samples_per_day</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">ix_day</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">lambda_2_samples_per_day</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="o">~</span><span class="n">ix_day</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">N_</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_count_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">expected_texts_per_day</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#E24A33&quot;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;expected number of text-messages received&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_count_data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Day&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Expected # text-messages&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Expected number of text-messages received&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">count_data</span><span class="p">)),</span> <span class="n">count_data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#5DA5DA&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.65</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;observed texts per day&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Ch1_Introduction_TFP_43_0.png" src="../../../_images/Ch1_Introduction_TFP_43_0.png" />
</div>
</div>
<p>Our analysis shows strong support for believing the user’s behavior did change (<span class="math notranslate nohighlight">\(\lambda_1\)</span> would have been close in value to <span class="math notranslate nohighlight">\(\lambda_2\)</span> had this not been true), and that the change was sudden rather than gradual (as demonstrated by <span class="math notranslate nohighlight">\(\tau\)</span>’s strongly peaked posterior distribution). We can speculate what might have caused this: a cheaper text-message rate, a recent weather-to-text subscription, or perhaps a new relationship. (In fact, the 45th day corresponds to Christmas, and I moved away to Toronto the next month, leaving a girlfriend behind.)</p>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">lambda_1_samples</span></code> and <code class="docutils literal notranslate"><span class="pre">lambda_2_samples</span></code>, what is the mean of the posterior distributions of <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span>?</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#type your code here.</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p>What is the expected percentage increase in text-message rates? <code class="docutils literal notranslate"><span class="pre">hint:</span></code> compute the mean of <code class="docutils literal notranslate"><span class="pre">lambda_1_samples/lambda_2_samples</span></code>. Note that this quantity is very different from <code class="docutils literal notranslate"><span class="pre">lambda_1_samples.mean()/lambda_2_samples.mean()</span></code></p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#type your code here.</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p>What is the mean of <span class="math notranslate nohighlight">\(\lambda_1\)</span> <strong>given</strong> that we know <span class="math notranslate nohighlight">\(\tau\)</span> is less than 45? That is, suppose we have been given new information that the change in behaviour occurred prior to day 45. What is the expected value of <span class="math notranslate nohighlight">\(\lambda_1\)</span> now? (You do not need to redo the TFP part. Just consider all instances where <code class="docutils literal notranslate"><span class="pre">tau_samples</span> <span class="pre">&lt;</span> <span class="pre">45</span></code>.)</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#type your code here.</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>[1] Gelman, Andrew. N.p.. Web. 22 Jan 2013. <a class="reference external" href="http://andrewgelman.com/2005/07/31/n_is_never_larg">N is never large enough</a></p>
<p>[2] Norvig, Peter. 2009. <a class="reference external" href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf">The Unreasonable Effectiveness of Data</a>.</p>
<p>[3] Jimmy Lin and Alek Kolcz. Large-Scale Machine Learning at Twitter. Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data (SIGMOD 2012), pages 793-804, May 2012, Scottsdale, Arizona.</p>
<p>[4] Cronin, Beau. “Why Probabilistic Programming Matters.” 24 Mar 2013. Google, Online Posting to Google . Web. 24 Mar. 2013. <a class="reference external" href="https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1">https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1</a>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/04.02 - Variational inference/research"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Raúl Ramos / Universidad de Antioquia, Fabio González / Universidad Nacional de Colombia<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>