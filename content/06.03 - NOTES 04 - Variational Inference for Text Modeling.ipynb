{"cells": [{"cell_type": "code", "execution_count": null, "id": "2bdc1c0d", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "f64d691b", "metadata": {}, "source": ["# Variational Inference for Text Modeling\n", "\n", "In this notebook, we will implement the variational inference algorithm for topic modeling using a neural network and Latent Dirichlet Allocation (LDA). We'll review the bayesian concepts behind LDA, and its implementation using tensorflow probability.\n", "\n", "First, let us import the necessary libraries."]}, {"cell_type": "code", "execution_count": null, "id": "80ffe509", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "import tensorflow_probability as tfp\n", "from collections import Counter\n", "\n", "tfd = tfp.distributions\n", "tfb = tfp.bijectors\n", "plt.style.use(\"ggplot\")"]}, {"cell_type": "markdown", "id": "3d47370b", "metadata": {}, "source": ["## Distributions in a Corpus of Documents\n", "\n", "Let us consider the following simple corpus of documents:"]}, {"cell_type": "code", "execution_count": null, "id": "f3e6fd61", "metadata": {}, "outputs": [], "source": ["corpus = [\n", "    \"the cat sat on the mat the mat was on the cat the cat was on the mat\",\n", "    \"the dog sat on the mat the mat was on the dog the dog was on the mat\",\n", "    \"when the cat was on the roof the roof was on the cat the cat was on the roof\",\n", "    \"the bird sang a song the song sang the bird the bird sang a song\"\n", "    ]"]}, {"cell_type": "markdown", "id": "2025c9f1", "metadata": {}, "source": ["\n", "First, we'll compute the words distribution for a the first document in the corpus.\n", "\n", "Let us tokenize the document into words by splitting on the space characters:"]}, {"cell_type": "code", "execution_count": null, "id": "c93183a9", "metadata": {}, "outputs": [], "source": ["words = corpus[0].split()\n", "print(words)"]}, {"cell_type": "markdown", "id": "3117ceb9", "metadata": {}, "source": ["Next, we'll compute words occurences in the document and the vocabulary:"]}, {"cell_type": "code", "execution_count": null, "id": "fb158207", "metadata": {}, "outputs": [], "source": ["counts = Counter(words)\n", "vocab = np.array(list(counts.keys()))\n", "print(vocab)\n", "print(counts)"]}, {"cell_type": "markdown", "id": "f0d0e892", "metadata": {}, "source": ["Finally, we'll estimate the parameters of the multinomial distribution for this document:"]}, {"cell_type": "code", "execution_count": null, "id": "885040d6", "metadata": {}, "outputs": [], "source": ["dist_params = tf.Variable(list(counts.values()))\n", "dist_params = dist_params / tf.reduce_sum(dist_params)\n", "\n", "document_dist = tfd.Multinomial(\n", "        total_count=float(len(vocab)),\n", "        probs=dist_params,\n", "        )\n", "print(document_dist)"]}, {"cell_type": "markdown", "id": "42304b47", "metadata": {}, "source": ["Using this distribution, we can sample documents counts with the counts distribution of the first document:"]}, {"cell_type": "code", "execution_count": null, "id": "519c5e0d", "metadata": {}, "outputs": [], "source": ["sample = document_dist.sample(10)\n", "print(sample)"]}, {"cell_type": "markdown", "id": "d23f403b", "metadata": {}, "source": ["We can convert the sample into a list of words, here, We repeat each word in the vocabulary the number of times it appears in the `sample` matrix:"]}, {"cell_type": "code", "execution_count": null, "id": "04bb6983", "metadata": {}, "outputs": [], "source": ["sample_np = (\n", "        sample\n", "        .numpy()\n", "        .astype(\"int\")\n", "        )\n", "\n", "for sample_id, doc_counts in enumerate(sample_np):\n", "    print(f\"Sample {sample_id}: \", end=\"\")\n", "    for i, count in enumerate(doc_counts):\n", "        if count > 0:\n", "            print(\" \".join([vocab[i]] * count), end=\" \")\n", "    print()"]}, {"cell_type": "markdown", "id": "5e69f122", "metadata": {}, "source": ["Now, We'll compute the parameters of the multinomial distribution for each document in the corpus. \n", "\n", "First, we'll compute the vocabulary for the corpus:"]}, {"cell_type": "code", "execution_count": null, "id": "f009bd72", "metadata": {}, "outputs": [], "source": ["vocab = np.unique(\n", "        np.concatenate([\n", "            np.array(doc.split())\n", "            for doc in corpus\n", "            ])\n", "        )\n", "print(vocab)"]}, {"cell_type": "markdown", "id": "6b19a12c", "metadata": {}, "source": ["Also, we need a mapping from the vocabulary to the indices of the parameters."]}, {"cell_type": "code", "execution_count": null, "id": "279c7314", "metadata": {}, "outputs": [], "source": ["word2idx = {w: i for i, w in enumerate(vocab)}"]}, {"cell_type": "markdown", "id": "0ef923f9", "metadata": {}, "source": ["We can compute the counts for each document:"]}, {"cell_type": "code", "execution_count": null, "id": "0ec0c2b2", "metadata": {}, "outputs": [], "source": ["words = list(map(lambda doc: doc.split(), corpus))\n", "print(words)"]}, {"cell_type": "code", "execution_count": null, "id": "743644b7", "metadata": {}, "outputs": [], "source": ["tokens = [[word2idx[w] for w in doc] for doc in words]\n", "print(tokens)"]}, {"cell_type": "code", "execution_count": null, "id": "0edcc1d2", "metadata": {}, "outputs": [], "source": ["counts = list(map(Counter, tokens))\n", "print(counts)"]}, {"cell_type": "markdown", "id": "1b6f6271", "metadata": {}, "source": ["We can now compute the parameters of the multinomial distribution for each document:"]}, {"cell_type": "code", "execution_count": null, "id": "c729bf55", "metadata": {}, "outputs": [], "source": ["params = np.zeros((len(corpus), len(vocab)))\n", "for i, doc in enumerate(counts):\n", "    params[i, list(doc.keys())] = list(doc.values())\n", "params = params / params.sum(axis=1, keepdims=True)\n", "print(params)"]}, {"cell_type": "markdown", "id": "891517ab", "metadata": {}, "source": ["The distribution for each document in the corpus:"]}, {"cell_type": "code", "execution_count": null, "id": "b70ff57b", "metadata": {}, "outputs": [], "source": ["document_dist = tfd.Multinomial(\n", "        total_count=float(len(vocab)),\n", "        probs=params\n", "        )\n", "print(document_dist)"]}, {"cell_type": "markdown", "id": "82cae4b8", "metadata": {}, "source": ["Let us generate two sample corpuses from the distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "aeb6318d", "metadata": {}, "outputs": [], "source": ["sample = document_dist.sample(2)\n", "sample_np = (\n", "        sample\n", "        .numpy()\n", "        .astype(\"int\")\n", "        )\n", "print(sample_np)"]}, {"cell_type": "code", "execution_count": null, "id": "65ae02b4", "metadata": {}, "outputs": [], "source": ["for sample_id, sample in enumerate(sample_np):\n", "    print(f\"Sample {sample_id}: \")\n", "    for document_id, doc_counts in enumerate(sample):\n", "        print(f\"\\tDocument {document_id}: \", end=\"\")\n", "        for i, count in enumerate(doc_counts):\n", "            if count > 0:\n", "                print(\" \".join([vocab[i]] * count), end=\" \")\n", "        print()"]}, {"cell_type": "markdown", "id": "be5ed513", "metadata": {}, "source": ["This approach always generates a corpus with 4 documents that have the same distribution of the original documents. However, there are some questions that enmark some limitations of this approach:\n", "\n", "* What would happen if we had a large corpus?\n", "\n", "> Under this approach, we would have to sample the distribution for each document in the corpus. This would be a very expensive operation.\n", "\n", "* What happens to documents with similar distributions, is it necessary to save equivalent vectors multiple times?\n", "\n", "> We would have to save the same vector multiple times. Which is memory inefficient.\n", "\n", "* Is there any way to generate a corpus with a different number of documents?\n", "\n", "> There is no way to generate a corpus with a different number of documents, since we have positional distributions for a fixed-length corpus.\n", "\n", "To address these problems, we can use a distribution for the parameters of the multinomial distribution (Bayesian modeling), which allows us to: save the parameters for the parameters' distribution only; summarize multiple distributions in one; and to generate corpuses with different number of documents.\n", "\n", "However, what kind of distribution is the parameters' distribution?\n", "\n", "> Let us recap about Bayesian conjugates. First, consider the Bernoulli distribution (special case of the Multinomial distribution), its conjugate is the Beta distribution. We need a multivariate distribution that generalizes the beta distribution and that can be used for the Multinomial's parameters distribution (Dirichlet)."]}, {"cell_type": "markdown", "id": "fb909c89", "metadata": {}, "source": ["## Bayesian Modeling for the Corpus Distribution\n", "\n", "Let us implement the following two-level hierarchical model for the corpus distribution:\n", "\n", "$$\n", "p \\sim Dirichlet(\\alpha)\\\\\n", "x \\sim Multinomial(p)\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "2b6e8cdc", "metadata": {}, "outputs": [], "source": ["corpus_dist = tfd.JointDistributionNamed(\n", "        {\n", "            \"p\": tfd.Dirichlet(\n", "                concentration=tf.Variable(tf.ones(len(vocab)))\n", "                ),\n", "            \"x\": lambda p: tfd.Multinomial(\n", "                total_count=float(len(vocab)),\n", "                probs=p\n", "                )\n", "            }\n", "        )"]}, {"cell_type": "markdown", "id": "0188981b", "metadata": {}, "source": ["We can generate some samples from this distribution, notice that the `JointDistributionNamed` object generates a sample for each of the levels:"]}, {"cell_type": "code", "execution_count": null, "id": "f9e8f8ec", "metadata": {}, "outputs": [], "source": ["sample = corpus_dist.sample(10)\n", "print(sample)"]}, {"cell_type": "markdown", "id": "6e9e7faa", "metadata": {}, "source": ["Let's view the corpus for this case:"]}, {"cell_type": "code", "execution_count": null, "id": "2e267e32", "metadata": {}, "outputs": [], "source": ["sample_np = corpus_dist.sample(10)[\"x\"].numpy().astype(\"int\")\n", "for document_id, doc_counts in enumerate(sample_np):\n", "    print(f\"\\tDocument {document_id}: \", end=\"\")\n", "    for i, count in enumerate(doc_counts):\n", "        if count > 0:\n", "            print(\" \".join([vocab[i]] * count), end=\" \")\n", "    print()"]}, {"cell_type": "markdown", "id": "d2ec7bf3", "metadata": {}, "source": ["We can solve this problem using variational inference, in this matter, We would need a surrogate posterior distribution, whose parameters can be easily estimated with tensorflow's automatic differentiation."]}, {"cell_type": "code", "execution_count": null, "id": "f15947d2", "metadata": {}, "outputs": [], "source": ["surrogate_posterior = tfd.JointDistributionNamedAutoBatched({\n", "    \"p\": tfb.Sigmoid()(\n", "        tfd.Normal(\n", "            loc=tf.Variable(tf.ones(len(vocab))),\n", "            scale=tf.ones(len(vocab))\n", "            )\n", "        )\n", "    })"]}, {"cell_type": "markdown", "id": "cb4a98d8", "metadata": {}, "source": ["We need the bag-of-words representation of the corpus:"]}, {"cell_type": "code", "execution_count": null, "id": "5ded6659", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["counts_mat = np.zeros((len(corpus), len(vocab)))\n", "for i, doc in enumerate(counts):\n", "    counts_mat[i, list(doc.keys())] = list(doc.values())\n", "counts_mat = tf.constant(counts_mat, dtype=tf.float32)"]}, {"cell_type": "markdown", "id": "cf49f066", "metadata": {}, "source": ["We can now compute the log-likelihood of the corpus distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "e1d55758", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def log_prob(p):\n", "    return corpus_dist.log_prob({\"p\": p, \"x\": counts_mat})"]}, {"cell_type": "markdown", "id": "9f3bb726", "metadata": {}, "source": ["Finally, we solve the problem using variational inference:"]}, {"cell_type": "code", "execution_count": null, "id": "d71cb37e", "metadata": {}, "outputs": [], "source": ["optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n", "loss = tfp.vi.fit_surrogate_posterior(\n", "        target_log_prob_fn=log_prob,\n", "        surrogate_posterior=surrogate_posterior,\n", "        optimizer=optimizer,\n", "        num_steps=int(1e3),\n", "        )"]}, {"cell_type": "markdown", "id": "62e5f507", "metadata": {}, "source": ["Let's view a sample of documents:"]}, {"cell_type": "code", "execution_count": null, "id": "b28b6e33", "metadata": {}, "outputs": [], "source": ["params = surrogate_posterior.sample(10)[\"p\"].numpy()\n", "params = params / params.sum(axis=1, keepdims=True)\n", "dist = tfd.Multinomial(\n", "        total_count=float(len(vocab)),\n", "        probs=params\n", "        )"]}, {"cell_type": "code", "execution_count": null, "id": "332cf4a8", "metadata": {}, "outputs": [], "source": ["sample_np = dist.sample(4).numpy().astype(\"int\")\n", "for sample_id, sample in enumerate(sample_np):\n", "    print(f\"Sample {sample_id}: \")\n", "    for document_id, doc_counts in enumerate(sample):\n", "        print(f\"\\tDocument {document_id}: \", end=\"\")\n", "        for i, count in enumerate(doc_counts):\n", "            if count > 0:\n", "                print(\" \".join([vocab[i]] * count), end=\" \")\n", "        print()"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}