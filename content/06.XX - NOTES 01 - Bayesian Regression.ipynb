{"cells": [{"cell_type": "code", "execution_count": null, "id": "2c15464c", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "f2cc1fcd", "metadata": {}, "source": ["# Bayesian Linear Regression\n", "---\n", "\n", "In this notebook we'll implement a Bayesian linear regression model and review its mathematical details, first, let us import some base libraries:"]}, {"cell_type": "code", "execution_count": null, "id": "517718c5", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import tensorflow as tf\n", "import tensorflow_probability as tfp\n", "from tensorflow.keras.optimizers import Adam\n", "from IPython.display import display, Math\n", "plt.style.use(\"ggplot\")"]}, {"cell_type": "code", "execution_count": null, "id": "32e71a1c", "metadata": {}, "outputs": [], "source": ["tfd = tfp.distributions"]}, {"cell_type": "markdown", "id": "6be4b3f6", "metadata": {}, "source": ["## Probabilistic Linear Regression\n", "---\n", "\n", "The linear regression is one of the simplest models in machine learning, the main goal is to compute an estimation $\\tilde{\\mathbf{y}} \\in \\mathbb{R} ^ N$ (where $N$ is the number of samples) of a target variable $\\mathbf{y} \\in \\mathbb{R} ^ N$ using some input variables $\\mathbf{X} \\in \\mathbb{R} ^ {N \\times m}$ (where $m$ is the number of variables) through a linear model:\n", "\n", "$$\n", "\\tilde{\\mathbf{y}} =  \\mathbf{x} \\cdot \\mathbf{w}\n", "$$\n", "\n", "Conventionally, the parameters $\\mathbf{w} \\in \\mathbb{R} ^ m$ are estimated through the minimization of the Mean Squared Error (MSE). However, this is equivalent to the Maximum Likelihood Estimation (MLE) of a probabilistic model. More precisely, we assume that the distribution of the label's noise is given by a normal distribution, so it is possible to define the following model:\n", "\n", "$$\n", "\\begin{split}\n", "\\mathbf{y} &= \\mathbf{\\tilde{y}} + \\epsilon\\\\\n", "\\mathbf{y} &= \\mathbf{X} \\cdot \\mathbf{w} + \\epsilon\n", "\\end{split}\n", "$$\n", "\n", "Where $\\epsilon \\sim \\mathcal{N}(\\mu = 0, \\sigma=E)$ and $E$ is the noise's magnitude or standard error. This can be summarized as the distribution of the target variables as follows:\n", "\n", "$$\n", "P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, E) = \\mathcal{N}(\\mathbf{y} = \\mathbf{X} \\cdot \\mathbf{w}, \\sigma=E)\n", "$$\n", "\n", "The MLE would then consist on determining $\\mathbf{w}$ and $E$ such that the likelihood probability is maximized, or equivalently, minimizing the negative log-likelihood.\n", "\n", "Let's see an example over some synthetic data:"]}, {"cell_type": "code", "execution_count": null, "id": "4588bfd2", "metadata": {}, "outputs": [], "source": ["x = tf.random.uniform(\n", "        shape = (1000, 1),\n", "        minval = -1,\n", "        maxval = 1\n", "        )\n", "X = tf.concat([x, tf.ones_like(x)], axis=1)\n", "display(Math(r\"\\mathbf{X}:\"))\n", "display(X[:5])\n", "display(X.shape)"]}, {"cell_type": "markdown", "id": "ef05b908", "metadata": {}, "source": ["Let's assume the following values for $\\mathbf{w}$ and $E$:"]}, {"cell_type": "code", "execution_count": null, "id": "011af516", "metadata": {}, "outputs": [], "source": ["w_real = tf.constant([[1.0], [-1.0]])\n", "e_real = tf.constant(0.1)\n", "display(Math(r\"\\mathbf{w}:\"))\n", "display(w_real)\n", "display(Math(r\"E:\"))\n", "display(e_real)"]}, {"cell_type": "markdown", "id": "27a67971", "metadata": {}, "source": ["Using these theoretical values, We can generate the $\\mathbf{y}$ values:"]}, {"cell_type": "code", "execution_count": null, "id": "65de9b5d", "metadata": {}, "outputs": [], "source": ["y = X @ w_real + tf.random.normal(shape=(1000, 1), mean=0, stddev=e_real)\n", "display(Math(r\"\\mathbf{y}:\"))\n", "display(y[:5])\n", "display(y.shape)"]}, {"cell_type": "markdown", "id": "af4e3852", "metadata": {}, "source": ["We can visualize the generated dataset:"]}, {"cell_type": "code", "execution_count": null, "id": "e095cb7b", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.scatter(tf.squeeze(x), tf.squeeze(y), alpha=0.2)\n", "ax.set_xlabel(r\"$\\mathbf{x}$\")\n", "ax.set_ylabel(r\"$\\mathbf{y}$\")\n", "fig.show()"]}, {"cell_type": "markdown", "id": "df0d9d3b", "metadata": {}, "source": ["For the estimation of these values We'll use the automatic differentiation of `tensorflow`, to this end, we must define an initial set of parameters:"]}, {"cell_type": "code", "execution_count": null, "id": "cf4f7974", "metadata": {}, "outputs": [], "source": ["w = tf.Variable([[0.0], [0.0]])\n", "e = tf.Variable(1.0)"]}, {"cell_type": "markdown", "id": "b17091b7", "metadata": {}, "source": ["Now, We can define a `tensorflow_probability` distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "eeb637cd", "metadata": {}, "outputs": [], "source": ["dist = tfd.Normal(loc=X @ w, scale=e)\n", "display(dist)"]}, {"cell_type": "markdown", "id": "0c2f98d9", "metadata": {}, "source": ["Let's define the training loop:"]}, {"cell_type": "code", "execution_count": null, "id": "862c3f7b", "metadata": {}, "outputs": [], "source": ["n_iters = 100\n", "optimizer = Adam(learning_rate=1e-2)\n", "training_variables = [w, e]\n", "for i in range(n_iters):\n", "    with tf.GradientTape() as t:\n", "        dist = tfd.Normal(loc=X @ w, scale=e)\n", "        nll = -tf.reduce_sum(dist.log_prob(y))\n", "        grads = t.gradient(nll, training_variables)\n", "        optimizer.apply_gradients(zip(grads, training_variables))"]}, {"cell_type": "markdown", "id": "252c4397", "metadata": {}, "source": ["We can compare the real parameters $\\mathbf{w}$ and the estimated ones $\\tilde{\\mathbf{w}}$"]}, {"cell_type": "code", "execution_count": null, "id": "cb8a8901", "metadata": {}, "outputs": [], "source": ["display(Math(r\"\\mathbf{w}\"))\n", "display(w_real)\n", "display(Math(r\"\\tilde{\\mathbf{w}}\"))\n", "display(w)"]}, {"cell_type": "markdown", "id": "f140f86e", "metadata": {}, "source": ["Also the noise's magnitude $E$ and the estimated one $\\tilde{E}$"]}, {"cell_type": "code", "execution_count": null, "id": "d10374a3", "metadata": {}, "outputs": [], "source": ["display(Math(r\"E\"))\n", "display(e_real)\n", "display(Math(r\"\\tilde{E}\"))\n", "display(e)"]}, {"cell_type": "markdown", "id": "662eb616", "metadata": {}, "source": ["As you can see, it's a valid probabilistic model. We can generate predictions from it:"]}, {"cell_type": "code", "execution_count": null, "id": "57362bb1", "metadata": {}, "outputs": [], "source": ["x_test = tf.cast(\n", "        tf.reshape(tf.linspace(start=-1, stop=1, num=100), (-1, 1)),\n", "        tf.float32\n", "        )\n", "X_test = tf.concat([x_test, tf.ones_like(x_test)], axis=1)\n", "y_pred = X_test @ w\n", "y_pred_high = y_pred + 3 * tf.ones_like(y_pred) * e\n", "y_pred_low = y_pred - 3 * tf.ones_like(y_pred) * e"]}, {"cell_type": "markdown", "id": "efab3e08", "metadata": {}, "source": ["Let's visualize the predictions:"]}, {"cell_type": "code", "execution_count": null, "id": "70f8fbeb", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.scatter(tf.squeeze(x), tf.squeeze(y), alpha=0.2, label=\"data\")\n", "ax.plot(tf.squeeze(x_test), tf.squeeze(y_pred), label=r\"$\\tilde{y}$\", color=\"k\")\n", "ax.fill_between(\n", "    tf.squeeze(x_test),\n", "    tf.squeeze(y_pred_low),\n", "    tf.squeeze(y_pred_high),\n", "    alpha=0.5, color=\"k\"\n", ")\n", "ax.set_xlabel(r\"$\\mathbf{x}$\")\n", "ax.set_ylabel(r\"$\\mathbf{y}$\")\n", "ax.legend()\n", "fig.show()"]}, {"cell_type": "markdown", "id": "c435b3cc", "metadata": {}, "source": ["## Bayesian Approach\n", "---\n", "\n", "One of the most important aspects of the probabilistic approach for linear regression is the ability to incorporate prior information into the model through a Bayesian approach.\n", "\n", "For instance, we can assume that the model's $\\mathbf{w}$ are very likely to be within a circle of radius 3, as shown in the following figure:"]}, {"cell_type": "code", "execution_count": null, "id": "e64eca7b", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(7, 7))\n", "circle = plt.Circle((0, 0), 3, color=\"#33333355\", label=\"Prior\")\n", "ax.add_patch(circle)\n", "ax.scatter([w_real[0]], [w_real[1]], label=r\"$\\mathbf{w}$\")\n", "ax.set_xlabel(r\"$w_0$\")\n", "ax.set_ylabel(r\"$w_1$\")\n", "ax.legend()\n", "fig.show()"]}, {"cell_type": "markdown", "id": "ae4b5389", "metadata": {}, "source": ["In fact, this is equivalent to the $L_2$ regularization that is typically used in models like Ridge Regression or neural networks. Similarly, this behavior can also be represented through a circular distribution, more precisely:\n", "\n", "$$\n", "P(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w} = [0, 0], \\Sigma=\\mathbf{I})\n", "$$\n", "\n", "It is well known that around 96% of the density for this distribution is within a circle with radius 3 and centered at $[0, 0]$. This can be seen in the following figure:"]}, {"cell_type": "code", "execution_count": null, "id": "42503c42", "metadata": {}, "outputs": [], "source": ["x_range = np.linspace(-5, 5, 100)\n", "X1, X2 = np.meshgrid(x_range, x_range)\n", "X_grid = np.concatenate([X1.reshape(-1, 1), X2.reshape(-1, 1)], axis=1)\n", "dist = tfd.MultivariateNormalDiag(loc=[0., 0.], scale_diag=[1., 1.])\n", "probs = dist.prob(X_grid).numpy().reshape(X1.shape)"]}, {"cell_type": "code", "execution_count": null, "id": "0f253a10", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(7, 7))\n", "ax.contourf(X1, X2, probs, cmap=\"gray\", label=r\"$P(\\mathbf{w})$\")\n", "ax.scatter([w_real[0]], [w_real[1]], label=r\"$\\mathbf{w}$\")\n", "ax.set_xlabel(r\"$w_0$\")\n", "ax.set_ylabel(r\"$w_1$\")\n", "ax.legend()\n", "fig.show()"]}, {"cell_type": "markdown", "id": "36223ed6", "metadata": {}, "source": ["We can use this information as a prior distribution in a Bayesian approach, let's assume that the standard error $E$ is constant, therefore:\n", "\n", "$$\n", "\\begin{split}\n", "P(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}=[0, 0], \\Sigma=\\mathbf{I}) \\\\\n", "P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, E) = \\mathcal{N}(\\mathbf{y} = \\mathbf{X} \\cdot \\mathbf{w}, \\sigma=E)\n", "\\end{split}\n", "$$\n", "\n", "Using the Bayes rule, we obtain:\n", "\n", "$$\n", "P(\\mathbf{w} | \\mathbf{X}, \\mathbf{y}, E) = \\frac{P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, E) P(\\mathbf{w})}{P(\\mathbf{X}, \\mathbf{y}, E)}\n", "$$"]}, {"cell_type": "markdown", "id": "64020397", "metadata": {}, "source": ["## Maximum Aposteriori Estimation\n", "---\n", "\n", "The maximum aposteriori estimation (MAP) is similar to maximum likelihood estimation (MLE), however, in this case we perform the following optimization:\n", "\n", "$$\n", "\\mathbf{w_{map}} = \\underset{\\mathbf{w}}{\\text{argmax}} \\left( P(\\mathbf{w} | \\mathbf{X}, \\mathbf{y}, E) \\right)\n", "$$\n", "\n", "Which is equivalent of the optimiziation of the log-posterior considering the convexity of the log function:\n", "\n", "$$\n", "\\mathbf{w_{map}} = \\underset{\\mathbf{w}}{\\text{argmax}} \\left(\\log{P(\\mathbf{w} | \\mathbf{X}, \\mathbf{y}, E)}\\right)\n", "$$\n", "\n", "We can use the Bayes rule:\n", "\n", "$$\n", "\\mathbf{w_{map}} = \\underset{\\mathbf{w}}{\\text{argmax}} \\left( \\log{P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, E) } + \\log{P(\\mathbf{w})} - \\log{P(\\mathbf{X}, \\mathbf{y}, E)} \\right)\n", "$$\n", "\n", "However, the term $\\log{P(\\mathbf{X}, \\mathbf{y}, E)}$ does not depend on $\\mathbf{w}$ and the optimization can be simplified to:\n", "\n", "$$\n", "\\mathbf{w_{map}} = \\underset{\\mathbf{w}}{\\text{argmax}} \\left( \\log{P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, E)} + \\log{P(\\mathbf{w})} \\right)\n", "$$"]}, {"cell_type": "markdown", "id": "772c8f4c", "metadata": {}, "source": ["### Closed-form Solution\n", "---\n", "\n", "We can obtain an analytical solution in some cases, for example, when both the prior and the posterior are normal.\n", "\n", "Let's see that case:\n", "\n", "$$\n", "\\begin{split}\n", "\\mathbf{w_{map}} = \\underset{\\mathbf{w}}{\\text{argmax}} \\left( \\log{P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, E)} + \\log{P(\\mathbf{w})} \\right)\\\\\n", "\\mathbf{w_{map}} = \\underset{\\mathbf{w}}{\\text{argmax}} \\left( \\log{\\mathcal{N} (y = \\mathbf{X} \\cdot \\mathbf{w}, \\sigma=E)} + \\log{\\mathcal{N}(\\mathbf{w} = [0, 0], \\sigma=\\tau)}\\right)\\\\\n", "\\mathbf{w_{map}} = \\underset{\\mathbf{w}}{\\text{argmax}} - \\frac{1}{E} (y - \\mathbf{X} \\cdot \\mathbf{w}) ^ 2 - \\frac{1}{2\\tau ^ 2}||\\mathbf{w}|| ^ 2\n", "\\end{split}\n", "$$\n", "\n", "As you can see, this is equivalent to the optimization of the mean squared error:\n", "\n", "$$\n", "\\mathcal{L} = (y - \\mathbf{X} \\cdot \\mathbf{w}) ^ 2\n", "$$\n", "\n", "Using a regularization term:\n", "\n", "$$\n", "\\mathcal{R} = ||\\mathbf{w}|| ^ 2\n", "$$\n", "\n", "And a regularization constant:\n", "\n", "$$\n", "\\lambda = \\frac{1}{2 \\tau ^ 2}\n", "$$\n", "\n", "We can find a closed-form solution using the derivative:\n", "\n", "$$\n", "\\frac{\\partial}{\\partial \\mathbf{w}} (\\mathcal{L} + \\lambda \\mathcal{R}) = 0\n", "$$\n", "\n", "Which leads to the solution of a Ridge regression model:\n", "\n", "$$\n", "\\mathbf{w_{map}} = (\\mathbf{X} ^ T \\cdot \\mathbf{X} + \\lambda \\mathbf{I}) ^ {-1} \\mathbf{X} ^ T \\mathbf{y}\n", "$$\n", "\n", "Let's see this in `tensorflow`:"]}, {"cell_type": "code", "execution_count": null, "id": "72ed4c05", "metadata": {}, "outputs": [], "source": ["l = 0.5\n", "w_map = (\n", "        tf.linalg.pinv(tf.transpose(X) @ X + l * tf.eye(X.shape[1])) @\n", "        tf.transpose(X) @ y\n", "        )\n", "display(w_map)"]}, {"cell_type": "markdown", "id": "27a6ed76", "metadata": {}, "source": ["### Optimization\n", "---\n", "\n", "In a more general scenario the distributions may not be normal. However, We can find a MAP estimation using automatic differentiation and `tensorflow_probability`.\n", "\n", "Let's see an example with the following likelihood and prior distributions:\n", "\n", "$$\n", "\\begin{split}\n", "P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, E) = N(\\mathbf{y} = \\mathbf{X} \\cdot \\mathbf{w}, \\sigma=E)\\\\\n", "P(\\mathbf{w}) = \\text{Laplace}(\\mathbf{w} = [0, 0], \\Sigma=\\mathbf{I})\n", "\\end{split}\n", "$$\n", "\n", "We can define this model as a joint distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "ef2f38dc", "metadata": {}, "outputs": [], "source": ["model = tfd.JointDistributionNamedAutoBatched({\n", "    \"w\": tfd.Independent(tfd.Laplace(loc=[0, 0], scale=1), reinterpreted_batch_ndims=1),\n", "    \"y\": lambda w: tfd.Normal(loc=X @ tf.reshape(w, (-1, 1)), scale=e_real)\n", "    })"]}, {"cell_type": "markdown", "id": "52d1e2e2", "metadata": {}, "source": ["And We can find the MAP estimation through a training loop:"]}, {"cell_type": "code", "execution_count": null, "id": "e2a089a7", "metadata": {}, "outputs": [], "source": ["n_iters = 100\n", "w = tf.Variable([0.0, 0.0])\n", "l = 0.5\n", "optimizer = Adam(learning_rate=1e-2)\n", "training_variables = [w]\n", "for i in range(n_iters):\n", "    with tf.GradientTape() as t:\n", "        model = tfd.JointDistributionNamedAutoBatched({\n", "            \"w\": tfd.Independent(\n", "                tfd.Laplace(loc=[0, 0], scale=l),\n", "                reinterpreted_batch_ndims=1,\n", "            ),\n", "            \"y\": lambda w: tfd.Normal(loc=X @ tf.reshape(w, (-1, 1)), scale=e_real)\n", "            })\n", "        nll = -tf.reduce_sum(model.log_prob(w = w, y = y))\n", "        grads = t.gradient(nll, training_variables)\n", "        optimizer.apply_gradients(zip(grads, training_variables))"]}, {"cell_type": "markdown", "id": "66acf3db", "metadata": {}, "source": ["Let's see $\\mathbf{w_{map}}$ estimation:"]}, {"cell_type": "code", "execution_count": null, "id": "869e592a", "metadata": {}, "outputs": [], "source": ["display(Math(r\"\\mathbf{w}\"))\n", "display(w_real)\n", "display(Math(r\"\\mathbf{w_{map}}\"))\n", "display(w)"]}, {"cell_type": "markdown", "id": "fb9938a6", "metadata": {}, "source": ["As you can see, the weights are lower in comparison to the real ones, which is a result of the regularization.\n", "\n", "With this approach, it's possible to optimize the standard error too:"]}, {"cell_type": "code", "execution_count": null, "id": "2c891804", "metadata": {}, "outputs": [], "source": ["n_iters = 100\n", "w = tf.Variable([0.0, 0.0])\n", "e = tf.Variable(1.0)\n", "l = 0.1\n", "optimizer = Adam(learning_rate=1e-2)\n", "training_variables = [w, e]\n", "for i in range(n_iters):\n", "    with tf.GradientTape() as t:\n", "        model = tfd.JointDistributionNamedAutoBatched({\n", "            \"w\": tfd.Independent(\n", "                tfd.Laplace(loc=[0, 0], scale=l),\n", "                reinterpreted_batch_ndims=1,\n", "            ),\n", "            \"y\": lambda w: tfd.Normal(loc=X @ tf.reshape(w, (-1, 1)), scale=e)\n", "            })\n", "        nll = -tf.reduce_sum(model.log_prob(w = w, y = y))\n", "        grads = t.gradient(nll, training_variables)\n", "        optimizer.apply_gradients(zip(grads, training_variables))"]}, {"cell_type": "markdown", "id": "b2180b75", "metadata": {}, "source": ["We find the following results:"]}, {"cell_type": "code", "execution_count": null, "id": "d8d481a2", "metadata": {}, "outputs": [], "source": ["display(Math(r\"\\mathbf{w}\"))\n", "display(w_real)\n", "display(Math(r\"\\mathbf{w_{map}}\"))\n", "display(w)"]}, {"cell_type": "code", "execution_count": null, "id": "9a910697", "metadata": {}, "outputs": [], "source": ["display(Math(r\"E\"))\n", "display(e_real)\n", "display(Math(r\"\\tilde{E}\"))\n", "display(e)"]}, {"cell_type": "markdown", "id": "9ae61156", "metadata": {}, "source": ["Let's visualize the results:"]}, {"cell_type": "code", "execution_count": null, "id": "709f30e6", "metadata": {}, "outputs": [], "source": ["x_test = tf.cast(\n", "        tf.reshape(tf.linspace(start=-1, stop=1, num=100), (-1, 1)),\n", "        tf.float32\n", "        )\n", "X_test = tf.concat([x_test, tf.ones_like(x_test)], axis=1)\n", "y_pred = X_test @ tf.reshape(w, (-1, 1))\n", "y_pred_high = y_pred + 3 * tf.ones_like(y_pred) * e\n", "y_pred_low = y_pred - 3 * tf.ones_like(y_pred) * e"]}, {"cell_type": "code", "execution_count": null, "id": "860f7f0f", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.scatter(tf.squeeze(x), tf.squeeze(y), alpha=0.2, label=\"data\")\n", "ax.plot(tf.squeeze(x_test), tf.squeeze(y_pred), label=r\"$\\tilde{y}$\", color=\"k\")\n", "ax.fill_between(\n", "    tf.squeeze(x_test),\n", "    tf.squeeze(y_pred_low),\n", "    tf.squeeze(y_pred_high),\n", "    alpha=0.5, color=\"k\"\n", ")\n", "ax.set_xlabel(r\"$\\mathbf{x}$\")\n", "ax.set_ylabel(r\"$\\mathbf{y}$\")\n", "ax.legend()\n", "fig.show()"]}, {"cell_type": "markdown", "id": "e3fbf94b", "metadata": {}, "source": ["This result is equivalent to the Lasso regression model, which uses $L_1$ regularization (equivalent to the Laplace distribution). Nevertheless, it's possible to optimize any model by changing the distributions."]}, {"cell_type": "markdown", "id": "1fd19538", "metadata": {}, "source": ["## Sampling From the Posterior\n", "---\n", "\n", "Up to this point, We've seen how to obtain the most likely parameters according to the posterior distribution (MAP estimation), however, it would be dessirable to have the posterior distribution or at least some samples from it.\n", "\n", "This can be achieved through Markov Chain Monte Carlo (MCMC), to this end, let us define the following model:\n", "\n", "$$\n", "\\begin{split}\n", "P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, E) = N(\\mathbf{y} = \\mathbf{X} \\cdot \\mathbf{w}, \\sigma = E)\\\\\n", "P(\\mathbf{w}) = N(\\mathbf{w} = [0, 0], \\Sigma = \\mathbf{I})\n", "\\end{split}\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "dcd0cbb3", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["model = tfd.JointDistributionNamedAutoBatched({\n", "    \"w\": tfd.Normal(loc=tf.zeros(shape=(2, 1)), scale=4.0),\n", "    \"y\": lambda w: tfd.Normal(loc=X @ w, scale=e_real)\n", "    })"]}, {"cell_type": "markdown", "id": "68c0eba0", "metadata": {}, "source": ["We can define the log function to optimize from this model:"]}, {"cell_type": "code", "execution_count": null, "id": "8ddc0ce5", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def log_prob(w):\n", "    return model.log_prob(w=w, y=y)"]}, {"cell_type": "markdown", "id": "2555aaef", "metadata": {}, "source": ["Also, let us define the MCMC procedure as a `tensorflow` function:"]}, {"cell_type": "code", "execution_count": null, "id": "e2cc8d0a", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["@tf.function\n", "def mcmc():\n", "    kernel = tfp.mcmc.NoUTurnSampler(\n", "            target_log_prob_fn = log_prob,\n", "            step_size=1e-3\n", "            )\n", "    return tfp.mcmc.sample_chain(\n", "            num_results = 1000,\n", "            num_burnin_steps = 500,\n", "            current_state = [tf.zeros(shape=(2, 1))],\n", "            kernel = kernel,\n", "            trace_fn = lambda _, results: results.target_log_prob\n", "            )"]}, {"cell_type": "markdown", "id": "7e720b09", "metadata": {}, "source": ["Now, we can compute samples from the posterior distribution $P(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}, E)$:"]}, {"cell_type": "code", "execution_count": null, "id": "1cf6de6d", "metadata": {}, "outputs": [], "source": ["samples, log_probs = mcmc()\n", "w_posterior = tf.squeeze(samples[0])"]}, {"cell_type": "markdown", "id": "1f2f00b8", "metadata": {}, "source": ["We can visualize these distributions:"]}, {"cell_type": "code", "execution_count": null, "id": "c88c0000", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(7, 7))\n", "sns.scatterplot(x = w_posterior[:, 0], y = w_posterior[:, 1], ax = ax)\n", "sns.histplot(x = w_posterior[:, 0], y = w_posterior[:, 1], ax = ax)\n", "sns.kdeplot(\n", "    x = w_posterior[:, 0],\n", "    y = w_posterior[:, 1],\n", "    levels=5,\n", "    color=\"w\", linewidths=1,\n", "    ax = ax\n", ")\n", "ax.set_xlabel(r\"$\\tilde{w}_1$\")\n", "ax.set_ylabel(r\"$\\tilde{w}_2$\")\n", "ax.set_title(r\"$\\mathbf{w} = \" + f\"[{w_real[0, 0]:.2f}, {w_real[1, 0]:.2f}]$\")\n", "fig.show()"]}, {"cell_type": "markdown", "id": "ae5d29f8", "metadata": {}, "source": ["## Approximating the Posterior\n", "---\n", "\n", "Another approach is to approximate the posterior distribution through variational inference.\n", "\n", "In this case, we can use a surrogate posterior distribution $Q(\\mathbf{w})$ to approximate $P(\\mathbf{w} | \\mathbf{y}, \\mathbf{X}, E)$ through the minimization of the kullback-leibler divergence:\n", "\n", "$$\n", "KL(Q || P) = \\int_{\\mathbf{w}} Q(\\mathbf{w}) \\log \\frac{Q(\\mathbf{w})}{P(\\mathbf{w} | \\mathbf{y}, \\mathbf{X}, E)} d\\mathbf{w}\n", "$$\n", "\n", "From the Bayes rule, we obtain:\n", "\n", "$$\n", "KL(Q || P) = \\int_{\\mathbf{w}} Q(\\mathbf{w}) \\log{Q(\\mathbf{w})} - \\log{P(\\mathbf{y}| \\mathbf{w}, \\mathbf{X}, E) P(\\mathbf{w})} + \\log(P(\\mathbf{y}, \\mathbf{X}, E)) d\\mathbf{w}\n", "$$\n", "\n", "The term $P(\\mathbf{y}, \\mathbf{X}, E)$ does not depend on $\\mathbf{w}$, therefore, the problem would be equivalent to minimizing the evidence lower bound function:\n", "\n", "$$\n", "ELBO(Q || P) = \\underset{Q(\\mathbf{w})}{\\mathbb{E}}[\\log Q(\\mathbf{w}) - \\log P(\\mathbf{y}, \\mathbf{w} | \\mathbf{X}, E)]\n", "$$\n", "\n", "We can train a Bayesian linear regression with this approach, for instance, we can use the following surrogate posterior distribution:\n", "\n", "$$\n", "Q(\\mathbf{w}) = N(\\mathbf{w_{vi}}, \\sigma_{vi})\n", "$$\n", "\n", "Therefore, we must learn its parameters:"]}, {"cell_type": "code", "execution_count": null, "id": "7cf55ce1", "metadata": {}, "outputs": [], "source": ["w_vi = tf.Variable([0., 0.])\n", "sigma_vi = tf.Variable(1.0)"]}, {"cell_type": "markdown", "id": "c6564a77", "metadata": {}, "source": ["Let's define a the surrogate posterior distribution $Q(\\mathbf{w})$ that we'll fit:"]}, {"cell_type": "code", "execution_count": null, "id": "5b449717", "metadata": {}, "outputs": [], "source": ["surrogate_posterior = tfd.JointDistributionNamedAutoBatched({\n", "    \"w\": tfd.Independent(tfd.Normal(loc=w_vi, scale=sigma_vi), reinterpreted_batch_ndims=1)\n", "    })"]}, {"cell_type": "markdown", "id": "d59f18ee", "metadata": {}, "source": ["Likewise, we can define the joint distribution $P(\\mathbf{y}, \\mathbf{w} | \\mathbf{X}, E)$ according to the linear model that we want to learn:"]}, {"cell_type": "code", "execution_count": null, "id": "14fa0ca8", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["model = tfd.JointDistributionNamedAutoBatched({\n", "    \"w\": tfd.Normal(loc=tf.zeros(shape=(2, )), scale=2.0),\n", "    \"y\": lambda w: tfd.Normal(loc=X @ tf.reshape(w, (-1, 1)), scale=e_real)\n", "    })"]}, {"cell_type": "markdown", "id": "1303f9fc", "metadata": {}, "source": ["From this model, we can obtain the distribution for different $\\mathbf{w}$ values, since $\\mathbf{y}$ is constant:"]}, {"cell_type": "code", "execution_count": null, "id": "805ed04b", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def log_prob(w):\n", "    return model.log_prob(w=w, y=y)"]}, {"cell_type": "markdown", "id": "36102628", "metadata": {}, "source": ["Using these distributions, we can optimize the surrogate parameters using variational inference and gradient-based optimization:"]}, {"cell_type": "code", "execution_count": null, "id": "614b87b2", "metadata": {}, "outputs": [], "source": ["optimizer = Adam(learning_rate=1e-3)\n", "loss = tfp.vi.fit_surrogate_posterior(\n", "        target_log_prob_fn=log_prob,\n", "        surrogate_posterior=surrogate_posterior,\n", "        optimizer=optimizer,\n", "        num_steps=5000,\n", "        )"]}, {"cell_type": "markdown", "id": "6893cb94", "metadata": {}, "source": ["Let's see the learned parameters:"]}, {"cell_type": "code", "execution_count": null, "id": "7711a10d", "metadata": {}, "outputs": [], "source": ["display(Math(r\"\\mathbf{w}\"))\n", "display(w_real)\n", "display(Math(r\"\\mathbf{w_{vi}}\"))\n", "display(w_vi)"]}, {"cell_type": "markdown", "id": "209d358f", "metadata": {}, "source": ["Finally, we can visualize the surrogate posterior distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "4e40238e", "metadata": {}, "outputs": [], "source": ["w1_range = np.linspace(0.9, 1.1, 100)\n", "w2_range = np.linspace(-1.1, -0.9, 100)\n", "W1, W2 = np.meshgrid(w1_range, w2_range)\n", "W_grid = np.concatenate([W1.reshape(-1, 1), W2.reshape(-1, 1)], axis=1)\n", "probs = surrogate_posterior.prob(w=W_grid).numpy().reshape(W1.shape)"]}, {"cell_type": "code", "execution_count": null, "id": "e113d567", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(7, 7))\n", "ax.contourf(W1, W2, probs, cmap=\"gray\")\n", "ax.scatter([w_real[0]], [w_real[1]], label=r\"$\\mathbf{w}$\")\n", "ax.set_xlabel(r\"$w_0$\")\n", "ax.set_ylabel(r\"$w_1$\")\n", "ax.set_title(r\"$Q(\\mathbf{w})$\")\n", "ax.legend()\n", "fig.show()"]}], "metadata": {"jupytext": {"cell_metadata_filter": "magic_args,-all"}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}