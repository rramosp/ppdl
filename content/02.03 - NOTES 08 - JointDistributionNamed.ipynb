{"cells": [{"cell_type": "code", "execution_count": null, "id": "29e06adb", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.dev.git > /dev/null 2> /dev/null\n", "!mv -n ppdl.dev/content/init.py ppdl.dev/content/local . 2> /dev/null\n", "!pip install -r ppdl.dev/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "f13f717d", "metadata": {}, "source": ["Joint Distributions\n", "\n", "`tensorflow-probability` contains a `JointDistribution` class that can be used for multilevel and hierarchical bayesian modeling. Generally, We'll be using two classes:\n", "\n", "* `JointDistributionSequential`: this distribution is similar to `tf.keras.Sequential` class, since in allows to build a model with a sequence of elements. In this case distributions or callables.\n", "* `JointDistributionNamed`: this distribution builds a model from a dictionary of distributions or callables."]}, {"cell_type": "code", "execution_count": null, "id": "64bfc497", "metadata": {}, "outputs": [], "source": ["from ppdl.samplers import LinearRegressionSampler\n", "import tensorflow as tf\n", "import numpy as np\n", "import tensorflow_probability as tfp\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "plt.style.use(\"ggplot\")\n", "tfd = tfp.distributions"]}, {"cell_type": "markdown", "id": "bdc64dfa", "metadata": {}, "source": ["## OLS with Joint Distributions\n", "\n", "In this example we'll use `JointDistribution` to build a linear regression model, with the following structure:\n", "\n", "$$\n", "w_0 \\sim \\mathcal{N}(0, 1)\\\\\n", "w_1 \\sim \\mathcal{N}(0, 1)\\\\\n", "y \\sim \\mathcal{N}(w_1 * x + w_0, 1)\n", "$$"]}, {"cell_type": "markdown", "id": "b355e7e0", "metadata": {}, "source": ["First, let us load the data."]}, {"cell_type": "code", "execution_count": null, "id": "e0984030", "metadata": {}, "outputs": [], "source": ["sampler = LinearRegressionSampler(noise_std=1.0)\n", "data = sampler(1000, seed=42)"]}, {"cell_type": "markdown", "id": "97f4e6ad", "metadata": {}, "source": ["Let us visualize the data:"]}, {"cell_type": "code", "execution_count": null, "id": "47f3dbe7", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(8, 8))\n", "data.plot(x=\"x_0\", y=\"y\", kind=\"scatter\", ax=ax)"]}, {"cell_type": "markdown", "id": "0b29d334", "metadata": {}, "source": ["The JointDistributionNamed class allows building a model using a syntax similar to it's mathematical definition."]}, {"cell_type": "code", "execution_count": null, "id": "686801ee", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def ols_model(x):\n", "    model = tfd.JointDistributionNamedAutoBatched({\n", "        \"w_0\": tfd.Normal(loc=tf.zeros(1), scale=1),\n", "        \"w_1\": tfd.Normal(loc=tf.zeros(1), scale=1),\n", "        \"y\": lambda w_1, w_0: tfd.Normal(\n", "            loc=w_1 * x + w_0,\n", "            scale=0.1\n", "            )\n", "        })\n", "    return model"]}, {"cell_type": "markdown", "id": "ba866e41", "metadata": {}, "source": ["In this case, `AutoBatched` is used to automatically batch the data, i.e., we didn't consider the batch axis in the function that computes `y`. Also, the `JointDistributionNamed` class allows to define a multilevel model from a dictionary.\n", "\n", "We can verify that the model is a linear regression model:"]}, {"cell_type": "code", "execution_count": null, "id": "7130889f", "metadata": {}, "outputs": [], "source": ["model = ols_model(data.x_0)"]}, {"cell_type": "code", "execution_count": null, "id": "7b648d7f", "metadata": {}, "outputs": [], "source": ["sample = model.sample(1)\n", "y_prior = sample[\"y\"]\n", "w_0 = sample[\"w_0\"][0, 0]\n", "w_1 = sample[\"w_1\"][0, 0]"]}, {"cell_type": "code", "execution_count": null, "id": "44dcf993", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(8, 8))\n", "ax.scatter(data.x_0, y_prior, label=\"data\")\n", "ax.set_title(f\"$y = {w_1:.3f} x + {w_0:.3f}$\")"]}, {"cell_type": "markdown", "id": "571b3630", "metadata": {}, "source": ["We can solve the linear regression model using Markov Chain Monte Carlo:"]}, {"cell_type": "code", "execution_count": null, "id": "22068502", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def log_prob(w_0, w_1):\n", "    return model.log_prob(w_0=w_0, w_1=w_1, y=data.y.values)"]}, {"cell_type": "code", "execution_count": null, "id": "de2e1ee5", "metadata": {}, "outputs": [], "source": ["num_results = int(10e3)\n", "burning_steps = int(1e3)\n", "\n", "mcmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n", "    target_log_prob_fn=log_prob,\n", "    num_leapfrog_steps=3,\n", "    step_size=1.,\n", "    )\n", "adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n", "        mcmc_kernel, num_adaptation_steps=int(burning_steps * 0.8),\n", "        )"]}, {"cell_type": "code", "execution_count": null, "id": "c83022f9", "metadata": {}, "outputs": [], "source": ["samples, is_accepted = tfp.mcmc.sample_chain(\n", "    num_results=num_results,\n", "    num_burnin_steps=burning_steps,\n", "    current_state=[tf.ones(1), tf.ones(1)],\n", "    kernel=adaptive_hmc,\n", "    trace_fn=lambda _, pkr: pkr.inner_results.is_accepted\n", "    )"]}, {"cell_type": "markdown", "id": "30534458", "metadata": {}, "source": ["Let's visualize the posterior distributions for the weights:"]}, {"cell_type": "code", "execution_count": null, "id": "f181c33a", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n", "sns.kdeplot(samples[0].numpy().flatten(), ax=ax[0])\n", "sns.kdeplot(samples[1].numpy().flatten(), ax=ax[1])\n", "ax[0].set_title(\"$w_0$\")\n", "ax[1].set_title(\"$w_1$\")\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "85e93f63", "metadata": {}, "source": ["Now, let's visualize the mean posterior model over the data:"]}, {"cell_type": "code", "execution_count": null, "id": "ed9ef23d", "metadata": {}, "outputs": [], "source": ["w_0_mean = tf.reduce_mean(samples[0]).numpy()\n", "w_1_mean = tf.reduce_mean(samples[1]).numpy()\n", "\n", "print(f\"mean(w_0): {w_0_mean:.3f}\")\n", "print(f\"mean(w_1): {w_1_mean:.3f}\")"]}, {"cell_type": "code", "execution_count": null, "id": "18fb45f0", "metadata": {}, "outputs": [], "source": ["x = np.linspace(data.x_0.min(), data.x_0.max(), 100)\n", "y_pred = w_1_mean * x + w_0_mean"]}, {"cell_type": "code", "execution_count": null, "id": "4b8c457b", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(8, 8))\n", "ax.scatter(data.x_0, data.y, label=\"data\", alpha=0.5)\n", "ax.plot(x, y_pred, label=\"prediction\", color=\"k\")\n", "ax.legend()"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}