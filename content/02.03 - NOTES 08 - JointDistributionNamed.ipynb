{"cells": [{"cell_type": "code", "execution_count": null, "id": "2a1fde0a", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "0200116d", "metadata": {}, "source": ["Joint Distributions\n", "\n", "`tensorflow-probability` contains a `JointDistribution` class that can be used for multilevel and hierarchical bayesian modeling. Generally, We'll be using two classes:\n", "\n", "* `JointDistributionSequential`: this distribution is similar to `tf.keras.Sequential` class, since in allows to build a model with a sequence of elements. In this case distributions or callables.\n", "* `JointDistributionNamed`: this distribution builds a model from a dictionary of distributions or callables."]}, {"cell_type": "code", "execution_count": null, "id": "f03e067e", "metadata": {}, "outputs": [], "source": ["from ppdl.samplers import LinearRegressionSampler\n", "import tensorflow as tf\n", "import numpy as np\n", "import tensorflow_probability as tfp\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "plt.style.use(\"ggplot\")\n", "tfd = tfp.distributions"]}, {"cell_type": "markdown", "id": "df0d4fe2", "metadata": {}, "source": ["## OLS with Joint Distributions\n", "\n", "In this example we'll use `JointDistribution` to build a linear regression model, with the following structure:\n", "\n", "$$\n", "w_0 \\sim \\mathcal{N}(0, 1)\\\\\n", "w_1 \\sim \\mathcal{N}(0, 1)\\\\\n", "y \\sim \\mathcal{N}(w_1 * x + w_0, 1)\n", "$$"]}, {"cell_type": "markdown", "id": "fb189c4b", "metadata": {}, "source": ["First, let us load the data."]}, {"cell_type": "code", "execution_count": null, "id": "77f4c131", "metadata": {}, "outputs": [], "source": ["sampler = LinearRegressionSampler(noise_std=1.0)\n", "data = sampler(1000, seed=42)"]}, {"cell_type": "markdown", "id": "3ad828e6", "metadata": {}, "source": ["Let us visualize the data:"]}, {"cell_type": "code", "execution_count": null, "id": "bff0ca67", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(8, 8))\n", "data.plot(x=\"x_0\", y=\"y\", kind=\"scatter\", ax=ax)"]}, {"cell_type": "markdown", "id": "a281c3e3", "metadata": {}, "source": ["The JointDistributionNamed class allows building a model using a syntax similar to it's mathematical definition."]}, {"cell_type": "code", "execution_count": null, "id": "d1797831", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def ols_model(x):\n", "    model = tfd.JointDistributionNamedAutoBatched({\n", "        \"w_0\": tfd.Normal(loc=tf.zeros(1), scale=1),\n", "        \"w_1\": tfd.Normal(loc=tf.zeros(1), scale=1),\n", "        \"y\": lambda w_1, w_0: tfd.Normal(\n", "            loc=w_1 * x + w_0,\n", "            scale=0.1\n", "            )\n", "        })\n", "    return model"]}, {"cell_type": "markdown", "id": "f2e150bd", "metadata": {}, "source": ["In this case, `AutoBatched` is used to automatically batch the data, i.e., we didn't consider the batch axis in the function that computes `y`. Also, the `JointDistributionNamed` class allows to define a multilevel model from a dictionary.\n", "\n", "We can verify that the model is a linear regression model:"]}, {"cell_type": "code", "execution_count": null, "id": "9edbdd67", "metadata": {}, "outputs": [], "source": ["model = ols_model(data.x_0)"]}, {"cell_type": "code", "execution_count": null, "id": "f22bbf88", "metadata": {}, "outputs": [], "source": ["sample = model.sample(1)\n", "y_prior = sample[\"y\"]\n", "w_0 = sample[\"w_0\"][0, 0]\n", "w_1 = sample[\"w_1\"][0, 0]"]}, {"cell_type": "code", "execution_count": null, "id": "11c58f01", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(8, 8))\n", "ax.scatter(data.x_0, y_prior, label=\"data\")\n", "ax.set_title(f\"$y = {w_1:.3f} x + {w_0:.3f}$\")"]}, {"cell_type": "markdown", "id": "91cb40a1", "metadata": {}, "source": ["We can solve the linear regression model using Markov Chain Monte Carlo:"]}, {"cell_type": "code", "execution_count": null, "id": "7e65758f", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def log_prob(w_0, w_1):\n", "    return model.log_prob(w_0=w_0, w_1=w_1, y=data.y.values)"]}, {"cell_type": "code", "execution_count": null, "id": "13473c9d", "metadata": {}, "outputs": [], "source": ["num_results = int(10e3)\n", "burning_steps = int(1e3)\n", "\n", "mcmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n", "    target_log_prob_fn=log_prob,\n", "    num_leapfrog_steps=3,\n", "    step_size=1.,\n", "    )\n", "adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n", "        mcmc_kernel, num_adaptation_steps=int(burning_steps * 0.8),\n", "        )"]}, {"cell_type": "code", "execution_count": null, "id": "50a97928", "metadata": {}, "outputs": [], "source": ["samples, is_accepted = tfp.mcmc.sample_chain(\n", "    num_results=num_results,\n", "    num_burnin_steps=burning_steps,\n", "    current_state=[tf.ones(1), tf.ones(1)],\n", "    kernel=adaptive_hmc,\n", "    trace_fn=lambda _, pkr: pkr.inner_results.is_accepted\n", "    )"]}, {"cell_type": "markdown", "id": "35e5f363", "metadata": {}, "source": ["Let's visualize the posterior distributions for the weights:"]}, {"cell_type": "code", "execution_count": null, "id": "65708904", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n", "sns.kdeplot(samples[0].numpy().flatten(), ax=ax[0])\n", "sns.kdeplot(samples[1].numpy().flatten(), ax=ax[1])\n", "ax[0].set_title(\"$w_0$\")\n", "ax[1].set_title(\"$w_1$\")\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "ce34004a", "metadata": {}, "source": ["Now, let's visualize the mean posterior model over the data:"]}, {"cell_type": "code", "execution_count": null, "id": "75ac0f0f", "metadata": {}, "outputs": [], "source": ["w_0_mean = tf.reduce_mean(samples[0]).numpy()\n", "w_1_mean = tf.reduce_mean(samples[1]).numpy()\n", "\n", "print(f\"mean(w_0): {w_0_mean:.3f}\")\n", "print(f\"mean(w_1): {w_1_mean:.3f}\")"]}, {"cell_type": "code", "execution_count": null, "id": "7af6ec41", "metadata": {}, "outputs": [], "source": ["x = np.linspace(data.x_0.min(), data.x_0.max(), 100)\n", "y_pred = w_1_mean * x + w_0_mean"]}, {"cell_type": "code", "execution_count": null, "id": "5ff36e15", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(8, 8))\n", "ax.scatter(data.x_0, data.y, label=\"data\", alpha=0.5)\n", "ax.plot(x, y_pred, label=\"prediction\", color=\"k\")\n", "ax.legend()"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}