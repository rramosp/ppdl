{"cells": [{"cell_type": "code", "execution_count": null, "id": "2bd572be", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "bae995bd", "metadata": {}, "source": ["# Lab 04.03.2: Variational Neural Topic Modeling "]}, {"cell_type": "code", "execution_count": null, "id": "1cac68cc", "metadata": {}, "outputs": [], "source": ["## Ignore this cell\n", "!pip install ppdl==0.1.5 rlxmoocapi==0.1.0 --quiet"]}, {"cell_type": "code", "execution_count": null, "id": "42fdf337", "metadata": {}, "outputs": [], "source": ["import inspect\n", "import nltk, re\n", "import tensorflow as tf\n", "import tensorflow_probability as tfp\n", "from rlxmoocapi import submit, session\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras.layers import Dense, Input, Layer\n", "from tensorflow.keras.initializers import GlorotNormal\n", "from sklearn.datasets import fetch_20newsgroups\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from nltk.corpus import stopwords\n", "from tqdm import tqdm\n", "\n", "tfd = tfp.distributions\n", "tfb = tfp.bijectors\n", "tfpl = tfp.layers\n", "nltk.download(\"popular\")"]}, {"cell_type": "markdown", "id": "b7126dfd", "metadata": {}, "source": ["## Topic Models\n", "In this lab, we will use a neural network for variational topic modeling. First, let us introduce a general topic model:\n", "\n", "![Topic model](local/imgs/topic_model.png)\n", "\n", "Where:\n", "\n", "* $N$ is the number of documents in the corpus.\n", "* $V$ is the vocabulary size.\n", "* $K$ is the number of topics.\n", "* $P(V=v_j|D=d_i)$ is the probability of word $v_j$ in document $d_i$ (a Bag-of-Words representation).\n", "* $P(K=k|D=d_i)$ is the probability of topic $k$ in document $d_i$.\n", "* $P(V=v_j|K=k)$ is the probability of document $d_i$ belonging to topic $k$.\n", "\n", "In this case, we will use a probabilistic encoder-decoder neural network to approximate $P(K=k|D=d_i)$ and $P(V=v_j|K=k)$.\n", "\n", "First, let us load the 20 newsgroups dataset:"]}, {"cell_type": "code", "execution_count": null, "id": "ed43f44f", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["newsgroups = fetch_20newsgroups(subset=\"test\")\n", "corpus, labels = newsgroups.data, newsgroups.target"]}, {"cell_type": "markdown", "id": "0f7fe4f7", "metadata": {}, "source": ["Let us preprocess the data:"]}, {"cell_type": "code", "execution_count": null, "id": "95f5b133", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def preprocess_doc(doc):\n", "    \"\"\"\n", "    preprocess a document.\n", "    \"\"\"\n", "    lower_doc = doc.lower()\n", "    clean_doc = re.sub(r\"[^a-z]\", \" \", lower_doc)\n", "    clean_doc = re.sub(r\"\\s+\", \" \", clean_doc)\n", "    tokens = clean_doc.split(\" \")\n", "    sw = stopwords.words(\"english\")\n", "    filtered_tokens = filter(\n", "            lambda token: token not in sw,\n", "            tokens\n", "            )\n", "\n", "    return \" \".join(filtered_tokens)"]}, {"cell_type": "code", "execution_count": null, "id": "31031f13", "metadata": {}, "outputs": [], "source": ["preprocessed_corpus = list(map(preprocess_doc, tqdm(corpus)))\n", "print(preprocessed_corpus[:5])"]}, {"cell_type": "markdown", "id": "2adb2666", "metadata": {}, "source": ["The BoW representation of the documents is a matrix of size $N \\times V$:"]}, {"cell_type": "code", "execution_count": null, "id": "7b7b3373", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["bow = (\n", "        CountVectorizer(min_df=50)\n", "        .fit(preprocessed_corpus)\n", "        )\n", "X = (\n", "        bow\n", "        .transform(preprocessed_corpus)\n", "        .toarray()\n", "        )\n", "vocab = bow.get_feature_names_out()\n", "print(X.shape)"]}, {"cell_type": "markdown", "id": "d54b844d", "metadata": {}, "source": ["## Task 1:\n", "\n", "Implement the `Encoder` class that takes the BoW of a document ($P(V=v_j|D=d_i)$) as input and outputs a probability distribution over topics ($P(K=k|D=d_i)$), you must:\n", "\n", "* Implement the constructor, adding the `Dense` layers that you will need.\n", "* Implement the `call` method to connect the input and the layers, and return the output of the last layer.\n", "* The last layer must be a `Dense` layer with a `clipped_softplus` activation with `n_topics` units."]}, {"cell_type": "code", "execution_count": null, "id": "eb17ff95", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def clipped_softplus(x):\n", "    return tf.clip_by_value(tf.nn.softplus(x), .1, 1e3)"]}, {"cell_type": "markdown", "id": "65072b91", "metadata": {}, "source": ["The following `DecodingLayer` class is used to decode the topics distributions into a BoW representation."]}, {"cell_type": "markdown", "id": "24a1c7b0", "metadata": {}, "source": ["## Task 2:\n", "\n", "Implement the `prior` function to describe the prior distribution of the topics.\n", "\n", "You must:\n", "* Create a constant tensor of shape `(1, n_topics)` with the `init_val`\n", "* Use a `Softplus` bijetor to transform the constant tensor.\n", "* Create a `Dirichlet` distribution with the bijector result as concentration."]}, {"cell_type": "markdown", "id": "e18e726b", "metadata": {}, "source": ["The following model uses a prior distribution, the `Encoder` and the `DecodingLayer` to build the variational neural topic model."]}, {"cell_type": "code", "execution_count": null, "id": "b37534c4", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["class NeuralTopicModel:\n", "    def __init__(\n", "            self,\n", "            prior_dist,\n", "            n_topics=20,\n", "            hidden_layers=(256, 256),\n", "            activation=\"relu\",\n", "            vocab_size=10000,\n", "            ):\n", "        self.prior_dist = prior_dist\n", "        self.n_topics = n_topics\n", "        self.hidden_layers = hidden_layers\n", "        self.activation = activation\n", "        self.vocab_size = vocab_size\n", "\n", "    def build(self, elbo, optimizer=\"adam\"):\n", "        input = Input(shape=(len(vocab),))\n", "        encoded = Encoder(\n", "                n_topics=self.n_topics,\n", "                hidden_layers=self.hidden_layers,\n", "                activation=self.activation\n", "                )(input)\n", "\n", "        topics_layer = tfpl.DistributionLambda(\n", "                lambda x: tfd.Dirichlet(x),\n", "                convert_to_tensor_fn=lambda x: x.sample()\n", "                )(encoded)\n", "\n", "        decoded = DecodingLayer(\n", "                n_topics=self.n_topics,\n", "                vocab_size=self.vocab_size,\n", "                )(topics_layer)\n", "        self.topics_dist = tfd.Dirichlet(\n", "                concentration=encoded\n", "                )\n", "        self.rec_dist = tfd.OneHotCategorical(\n", "                probs=decoded\n", "                )\n", "        self.rec_model = Model(inputs=input, outputs=decoded)\n", "        self.encoder = Model(inputs=input, outputs=encoded)\n", "\n", "        loss = -elbo(\n", "                input, self.topics_dist, self.rec_dist,\n", "                self.prior_dist\n", "                )\n", "        self.rec_model.add_loss(loss)\n", "        self.rec_model.compile(optimizer=optimizer)\n", "        return self\n", "\n", "    def train(self, counts, epochs=10, batch_size=32):\n", "        self.rec_model.fit(\n", "                counts,\n", "                epochs=epochs,\n", "                batch_size=batch_size\n", "                )\n", "        return self\n", "\n", "    def get_topics_dist(self, X):\n", "        params = self.encoder.predict(X)\n", "        dist = tfd.Dirichlet(concentration=params)\n", "        return dist\n", "    \n", "    def get_topics_words(self, X):\n", "        return self.rec_model.layers[3].get_topic_words()"]}, {"cell_type": "markdown", "id": "f4b83297", "metadata": {}, "source": ["## Task 3:\n", "\n", "The `build` method of the `NeuralTopicModel`, uses the `elbo` function, for our model it's given by:\n", "\n", "```\n", "mean(log_prob(X) - kl(prior || topics_posterior))\n", "```\n", "\n", "Where:\n", "\n", "* `X` is the BoW representation of the document.\n", "* `topics_posterior` is the posterior distribution of the topics (encoder distribution output).\n", "* `rec_dist` is the distribution of the reconstruction of the document (decoder distribution output).\n", "* `prior` is the prior distribution of the topics."]}, {"cell_type": "markdown", "id": "e0a890b6", "metadata": {}, "source": ["Let us train the model"]}, {"cell_type": "code", "execution_count": null, "id": "13b58855", "metadata": {}, "outputs": [], "source": ["# hyperparameters\n", "N_TOPICS = 20\n", "HIDDEN_LAYERS = (256, 256)\n", "ACTIVATION = \"relu\"\n", "prior_dist = prior(n_topics=N_TOPICS, init_val=2.0)"]}, {"cell_type": "code", "execution_count": null, "id": "56b0b1d0", "metadata": {}, "outputs": [], "source": ["model = (\n", "        NeuralTopicModel(\n", "            prior_dist=prior_dist,\n", "            n_topics=N_TOPICS,\n", "            hidden_layers=HIDDEN_LAYERS,\n", "            activation=ACTIVATION,\n", "            vocab_size=len(vocab)\n", "            )\n", "        .build(elbo)\n", "        .train(X, epochs=20, batch_size=32)\n", "        )"]}, {"cell_type": "markdown", "id": "a761f27b", "metadata": {}, "source": ["Finally, let us review the learned distributions.\n", "\n", "* The posterior distribution of the topics:"]}, {"cell_type": "code", "execution_count": null, "id": "18447e92", "metadata": {}, "outputs": [], "source": ["topics_posterior = model.get_topics_dist(X)\n", "print(topics_posterior.mean())\n", "print(topics_posterior.stddev())"]}, {"cell_type": "markdown", "id": "5edb0257", "metadata": {}, "source": ["* The probabilities of the words in the topics:"]}, {"cell_type": "code", "execution_count": null, "id": "ed43d51a", "metadata": {}, "outputs": [], "source": ["topics_words = model.get_topics_words(X)\n", "print(topics_words)"]}, {"cell_type": "markdown", "id": "d237881d", "metadata": {}, "source": ["* Finally, We can view the 15 most relevant terms for each topic:"]}, {"cell_type": "code", "execution_count": null, "id": "86e21b85", "metadata": {}, "outputs": [], "source": ["for i, comp in enumerate(topics_words.numpy()):\n", "    terms_comp = zip(vocab, comp)\n", "    sorted_terms = sorted(\n", "            terms_comp, key= lambda x:x[1],\n", "            reverse=True\n", "            )[:15]\n", "    print(\"Topic {}: {}\".format(\n", "        i, \" \".join(\n", "            map(\n", "                lambda x:x[0], sorted_terms\n", "                )\n", "             )\n", "         ))"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}