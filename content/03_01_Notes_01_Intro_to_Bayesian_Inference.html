
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Model-based reasoning and Bayesian inference &#8212; Probabilistic Programming for Deep Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-MBFEZ3PF64"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-MBFEZ3PF64');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo-tf-udea-unal.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Probabilistic Programming for Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="M1.html">
   Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="M1.1.html">
     1.1 Probabilistic programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="M1.2.html">
     1.2 Course mechanics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="M2.html">
   2. Fundamentals
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="M2.1.html">
     2.1 Tensorflow core
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="M2.2-videolist.html">
     2.2 Intuitions on probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="M2.3.html">
     2.3 Tensorflow probability basics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="M3.html">
   3. Bayesian Inference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="M3.1.html">
     3.1 Model based reasoning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="M3.2.html">
     3.2 Bayesian inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="M3.3.html">
     3.3 Bayesian hard classifier
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="M4.html">
   4. Variational Inference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="M4.1.html">
     4.1 Revisiting uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="M4.2.html">
     4.2 Variational inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="M4.3.html">
     4.3 VI on latent variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="M4.4.html">
     4.4 VI on latent parameters
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/03_01_Notes_01_Intro_to_Bayesian_Inference.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/rramosp/ppdl/blob/main/content/03_01_Notes_01_Intro_to_Bayesian_Inference.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Model-based reasoning and Bayesian inference
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coin-flipping-example">
   Coin flipping example
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-formulation">
     1. Model formulation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-collection">
     2. Data collection
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-calculation">
     3. Posterior calculation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-application">
     4. Model application
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Model-based reasoning and Bayesian inference</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Model-based reasoning and Bayesian inference
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coin-flipping-example">
   Coin flipping example
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-formulation">
     1. Model formulation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-collection">
     2. Data collection
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-calculation">
     3. Posterior calculation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-application">
     4. Model application
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="model-based-reasoning-and-bayesian-inference">
<h1>Model-based reasoning and Bayesian inference<a class="headerlink" href="#model-based-reasoning-and-bayesian-inference" title="Permalink to this headline">¶</a></h1>
<p>In model-based reasoning we assume a model of the process that generated our data. In Bayesian inference this model is probabilistic in nature, i.e. it encodes our knowledge (or asumptions) of the world in terms of ramdom variables, probabilistic distributions of these random variables and dependence/independence realtionships.</p>
<p>The following is a summary of the Bayesian approach to machine learning (<a class="reference external" href="https://www.cs.toronto.edu/~radford/ftp/bayes-tut.pdf">https://www.cs.toronto.edu/~radford/ftp/bayes-tut.pdf</a>):</p>
<ol class="simple">
<li><p><strong>Model formulation</strong>: We formulate our knowledge about the world (or the mechanism that generated our data) probabilistically:</p></li>
</ol>
<ul class="simple">
<li><p>We define a <em>model</em> that express qualitative aspects of our knowledge. This model will be expressed in terms of random variables and their relationships. The model will have unknown parameters.</p></li>
<li><p>We specify our believes (before seeing any data) about how we expect the values of the unknown parameters to behave using a <em>prior</em> probability.</p></li>
</ul>
<ol class="simple">
<li><p><strong>Data collection</strong>: We obtain data.</p></li>
<li><p><strong>Posterior calculation</strong>: We use the data to compute the <em>posterior</em> probability of the unknown parameters, given the observed data.</p></li>
<li><p><strong>Model application</strong>: The posterior probability distribution can be used to:</p></li>
</ol>
<ul class="simple">
<li><p>Derive scientific conclusions, taking into account uncertainty.</p></li>
<li><p>Make predictions.</p></li>
<li><p>Make decisions.</p></li>
</ul>
<p>To illustrate this process, we will a coin flipping experiment as an example.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="coin-flipping-example">
<h1>Coin flipping example<a class="headerlink" href="#coin-flipping-example" title="Permalink to this headline">¶</a></h1>
<p>In this example, we want to model the toss of a coin using a Bayesian approach.</p>
<div class="section" id="model-formulation">
<h2>1. Model formulation<a class="headerlink" href="#model-formulation" title="Permalink to this headline">¶</a></h2>
<p>Our model will have two random variables:</p>
<p><span class="math notranslate nohighlight">\(Y\)</span>: random variable representing the outcome of a coin toss</p>
<p><span class="math notranslate nohighlight">\(\Theta\)</span>: probability of getting a head (<span class="math notranslate nohighlight">\(Y=1\)</span>)</p>
<p>The distribution of <span class="math notranslate nohighlight">\(Y\)</span> is Bernoulli:
$<span class="math notranslate nohighlight">\(
P(Y = y\ |\ \theta) = \theta^{y}(1 - \theta)^{1 - y}
\)</span>$</p>
<p>An experiment consists of performing several coin tosses. The resulting data, <span class="math notranslate nohighlight">\(D\)</span>, from the experiment can be summarized by two values: <span class="math notranslate nohighlight">\(#heads\)</span> and <span class="math notranslate nohighlight">\(#tails\)</span>. The likelihood of a particular experiment outcome is given by the binomila distribution:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
P(D\ |\ \theta) &amp;= \binom{\#heads + \#tails}{\#heads} \theta^{\#heads}(1 - \theta)^{\#tails}
\end{align}
\]</div>
<p>This term is called the <em>likelihood</em>, the conditional probability of the data given the parameters. The following Python function calculates it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">comb</span>
<span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">num_tails</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">comb</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">+</span> <span class="n">num_tails</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> 
            <span class="o">*</span> <span class="n">theta</span> <span class="o">**</span> <span class="n">num_heads</span> 
            <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="n">num_tails</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For instance if <span class="math notranslate nohighlight">\(\theta = 0.3\)</span>, the probability of getting 6 heads and 4 tails in 10 coin tosses would be:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">likelihood</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.03675690899999999
</pre></div>
</div>
</div>
</div>
<p>But if <span class="math notranslate nohighlight">\(\theta=0.8\)</span> the likelihood would be:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">likelihood</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.08808038399999996
</pre></div>
</div>
</div>
</div>
<p>As part of our modeling we have to also represent our prior knowledge of the possible values of <span class="math notranslate nohighlight">\(\theta\)</span>. This is encoded by a distribution of <span class="math notranslate nohighlight">\(\theta\)</span> values which is called the prior distribution:</p>
<div class="math notranslate nohighlight">
\[
P(\theta)
\]</div>
<p>If we don’t have any prior knowledge, this can be encoded as a uniform prior distribution. Since we are dealing with a coin, we can assume that coins that are closer to be fair are more likely, this means that we give higher probabilities to values of <span class="math notranslate nohighlight">\(\theta\)</span> closer to <span class="math notranslate nohighlight">\(0.5\)</span>. Also we will assume a discrete probability since it will simplify our calculations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="k">def</span> <span class="nf">theta_prior_fun</span><span class="p">():</span>
    <span class="n">theta_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
    <span class="n">theta_prior_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
    <span class="n">theta_prior_p</span> <span class="o">=</span> <span class="n">theta_prior_p</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">theta_prior_p</span><span class="p">)</span>
    <span class="n">theta_prior</span> <span class="o">=</span> <span class="p">{</span><span class="n">theta_vals</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">theta_prior_p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">))}</span>
    <span class="k">return</span> <span class="n">theta_prior</span>

<span class="k">def</span> <span class="nf">plot_theta_dist</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">theta_p</span><span class="p">):</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">))</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">theta_p</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.1</span> <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">val</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">theta_vals</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$P(\theta$)&#39;</span><span class="p">)</span>
    
<span class="n">theta_prior</span> <span class="o">=</span> <span class="n">theta_prior_fun</span><span class="p">()</span>
<span class="n">theta_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">theta_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta_prior</span><span class="p">[</span><span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">theta_vals</span><span class="p">]</span>
<span class="n">plot_theta_dist</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">theta_p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/03_01_Notes_01_Intro_to_Bayesian_Inference_9_0.png" src="../_images/03_01_Notes_01_Intro_to_Bayesian_Inference_9_0.png" />
</div>
</div>
</div>
<div class="section" id="data-collection">
<h2>2. Data collection<a class="headerlink" href="#data-collection" title="Permalink to this headline">¶</a></h2>
<p>Two collect data we can toss a real coin a number of times and record how many
heads (<span class="math notranslate nohighlight">\(\# heads\)</span>) and tails (<span class="math notranslate nohighlight">\(\# tails\)</span>) we get. This is our data <span class="math notranslate nohighlight">\(D\)</span>. For this exercise let’s assume we got  <span class="math notranslate nohighlight">\(\# heads = 15\)</span> and <span class="math notranslate nohighlight">\(\# tails = 5\)</span>.</p>
</div>
<div class="section" id="posterior-calculation">
<h2>3. Posterior calculation<a class="headerlink" href="#posterior-calculation" title="Permalink to this headline">¶</a></h2>
<p>To calculate the posterior we will use the the Bayes theorem:</p>
<div class="math notranslate nohighlight">
\[
P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
\]</div>
<p>We already have the prior distribution and the likelihood function. We need to calculate the probability of the evidence (our data <span class="math notranslate nohighlight">\(D\)</span>). For this we will use the law to total probability, taking advantage of the fact that the prior distribution is discrete and only takes values greater than 0 for a finite set of theta values <span class="math notranslate nohighlight">\(\{\theta_0\dots\theta_{n-1}\}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
P(D) &amp;= \sum_{i}{P(D,\theta=\theta_i)} \\
 &amp; = \sum_{i}{P(D\ |\ \theta=\theta_i)P(\theta = \theta_i)} \\
\end{align}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evidence</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">num_tails</span><span class="p">,</span> <span class="n">theta_vals</span><span class="p">):</span>
    <span class="n">joints</span> <span class="o">=</span> <span class="p">[</span><span class="n">likelihood</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">num_tails</span><span class="p">,</span> <span class="n">theta_i</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta_prior</span><span class="p">[</span><span class="n">theta_i</span><span class="p">]</span>
              <span class="k">for</span> <span class="n">theta_i</span> <span class="ow">in</span> <span class="n">theta_vals</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">joints</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we are prepared for defining the posterior distribution function (<span class="math notranslate nohighlight">\(P(\theta|D)\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">posterior</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">num_tails</span><span class="p">,</span> <span class="n">theta_vals</span><span class="p">):</span>
    <span class="n">evidence_p</span> <span class="o">=</span> <span class="n">evidence</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">num_tails</span><span class="p">,</span> <span class="n">theta_vals</span><span class="p">)</span>
    <span class="n">posterior_p</span> <span class="o">=</span> <span class="p">{</span><span class="n">theta_i</span><span class="p">:</span><span class="n">likelihood</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">num_tails</span><span class="p">,</span> <span class="n">theta_i</span><span class="p">)</span> 
                            <span class="o">*</span> <span class="n">theta_prior</span><span class="p">[</span><span class="n">theta_i</span><span class="p">]</span> <span class="o">/</span> <span class="n">evidence_p</span>
                   <span class="k">for</span> <span class="n">theta_i</span> <span class="ow">in</span> <span class="n">theta_vals</span><span class="p">}</span>
    <span class="k">return</span> <span class="n">posterior_p</span>

</pre></div>
</div>
</div>
</div>
<p>Now we call the function with our data and plot the resulting posterior distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">theta_posterior</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">theta_vals</span><span class="p">)</span>
<span class="n">theta_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta_posterior</span><span class="p">[</span><span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">theta_vals</span><span class="p">]</span>
<span class="n">plot_theta_dist</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">theta_p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/03_01_Notes_01_Intro_to_Bayesian_Inference_16_0.png" src="../_images/03_01_Notes_01_Intro_to_Bayesian_Inference_16_0.png" />
</div>
</div>
</div>
<div class="section" id="model-application">
<h2>4. Model application<a class="headerlink" href="#model-application" title="Permalink to this headline">¶</a></h2>
<p>We can see that the resulting posterior distribution gives higher probability to values of the parameter around <span class="math notranslate nohighlight">\(0.7\)</span>, this is consistent with the fact that the data from the experiment has more heads than tails.</p>
<p>If we want to make a prediction of the outcome of a new coin toss, we can do it in different ways. We can use the mode of the posterior distribution, i.e.:</p>
<div class="math notranslate nohighlight">
\[
\theta_{\text{MAP}} = \arg \max_\theta P(\theta|D)
\]</div>
<p>This is called maximum a posteriori estimation (MAP). The following Python function calculates the MAP estimator for a given posterior distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">map_estimator</span><span class="p">(</span><span class="n">posterior_p</span><span class="p">):</span>
    <span class="n">items</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">posterior_p</span><span class="o">.</span><span class="n">items</span><span class="p">())))</span>
    <span class="k">return</span> <span class="n">items</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">items</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">]</span>

</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7000000000000001
</pre></div>
</div>
</div>
</div>
<p>In our example the MAP estimator will be:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">map_estimator</span><span class="p">(</span><span class="n">theta_posterior</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7000000000000001
</pre></div>
</div>
</div>
</div>
<p>Which means that the probability of getting heads in the new experiment will be:</p>
<div class="math notranslate nohighlight">
\[
P(Y=1|\theta = \theta_{\text{MAP}}) = \theta_{\text{MAP}} = 0.7
\]</div>
<p>Another possibility is to calculate the expected value of <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\theta_{\text{Bayes}} = E[\theta]
\]</div>
<p>This is called the Bayes estimator. The following Python function calculates the Bayes estimator:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bayes_estimator</span><span class="p">(</span><span class="n">posterior_p</span><span class="p">):</span>
    <span class="n">items</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">posterior_p</span><span class="o">.</span><span class="n">items</span><span class="p">())))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">items</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">items</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>

</pre></div>
</div>
</div>
</div>
<p>In our example the Bayes estimator will be:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">bayes_estimator</span><span class="p">(</span><span class="n">theta_posterior</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6961813468955773
</pre></div>
</div>
</div>
</div>
<p>The probability of getting heads in the new experiment according to this estimator will be:</p>
<div class="math notranslate nohighlight">
\[
P(Y=1|\theta = \theta_{\text{Bayes}}) = \theta_{\text{Bayes}} = 0.6961
\]</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Raúl Ramos / Universidad de Antioquia, Fabio González / Universidad Nacional de Colombia<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>