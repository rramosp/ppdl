{"cells": [{"cell_type": "code", "execution_count": null, "id": "2dadcab0", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# TF symbolic engine "]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/plain": ["'2.2.0'"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["import numpy as np\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "%matplotlib inline\n", "%load_ext tensorboard\n", "\n", "from local.lib import mlutils\n", "from IPython.display import Image\n", "tf.__version__"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## TF is a symbolic computing + optimization library for machine learning problems\n", "\n", "ML expressions involve:\n", "\n", "- variables representing data as n-dimensional objects\n", "- variables representing parameters as n-dimensional objects\n", "- mostly matrix operations (multiplications, convolutions, etc.)\n", "- some non linear operations (activation functions)\n", "\n", "\n", "**Recall** that in `sympy` we **FIRST** define expressions (a computational graph) and **THEN** we evaluate them feed concrete values.\n", "\n", "Tensorflow **INTEGRATES** both aspects so that building computational graphs **LOOKS LIKE** writing regular Pytohn code as must as possible.\n", "\n", "- a `tf.Variable` represents a **symbolic** variable, that **contains a value**\n", "\n", "\n", "See:\n", "\n", "- https://www.tensorflow.org/guide/keras/train_and_evaluate\n", "- https://www.tensorflow.org/guide/keras/custom_layers_and_models\n", "- https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": ["<tf.Tensor: shape=(1,), dtype=float32, numpy=array([778.], dtype=float32)>"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["x = tf.Variable(initial_value=[7], name=\"x\", dtype=tf.float32)\n", "y = tf.Variable(initial_value=[9], name=\"y\", dtype=tf.float32)\n", "f = x**2+y**3\n", "f"]}, {"cell_type": "markdown", "metadata": {}, "source": ["`f` is **SYMBOLIC EXPRESSION** (a `Tensor` in TF terms) that also contains a value attached to it.\n", "\n", "\n", "for which TF can obtain gradients automatically. This might seem a rather akward way of obtaining the gradient (with `GradientTape`). The goal is that you **write code as in Python** and TF takes care of building the computational graph with it. \n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["tf.Tensor([14.], shape=(1,), dtype=float32) tf.Tensor([243.], shape=(1,), dtype=float32)\n"]}], "source": ["with tf.GradientTape(persistent=True) as t:\n", "    f = x**2 + y**3\n", "    \n", "print (t.gradient(f, x), t.gradient(f, y))\n", "    "]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([14.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([243.], dtype=float32)>]\n"]}], "source": ["print (t.gradient(f, [x,y]))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["usually expressions are built within functions decorated with `@tf.function` for performance"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["@tf.function\n", "def myf(x,y):\n", "    return x**2 + y**3"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["tf.Tensor([14.], shape=(1,), dtype=float32) tf.Tensor([243.], shape=(1,), dtype=float32)\n"]}], "source": ["with tf.GradientTape(persistent=True) as t:\n", "    f = myf(x,y)\n", "    \n", "print (t.gradient(f, x), t.gradient(f, y))\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<tensorflow.python.eager.def_function.Function object at 0x7f82b0317850>\n"]}], "source": ["!rm -rf logs\n", "mlutils.make_graph(myf, x, y, logdir=\"logs\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["%tensorboard --logdir logs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Tensors\n", "\n", "in `Tensorflow` the notion of a Tensor is just a **symbolic multidimensional array**. Although, this is a recent simplified version of what always has been known as a tensor in differential geometry (see [https://bjlkeng.github.io/posts/tensors-tensors-tensors/](https://bjlkeng.github.io/posts/tensors-tensors-tensors/)).\n", "\n", "\n", "Observe how Tensorflow naturally deals with multidimensional symbolic variables (Tensors)\n", "\n", "$$\\frac{1}{m} \\sum (X\\theta - y)^2$$"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": ["<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n", "array([[-38.      ],\n", "       [-48.666668]], dtype=float32)>"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["n = 3\n", "X = tf.Variable(initial_value=[[2, 6], [3, 1], [4, 5]], name=\"X\", dtype=tf.float32)\n", "w = tf.Variable(initial_value=[[-2],[1]], name=\"w\", dtype=tf.float32)\n", "y = tf.Variable(initial_value=[[8],[2],[3]], name=\"y\", dtype=tf.float32)\n", "\n", "with tf.GradientTape(persistent=True) as t:\n", "    f = tf.reduce_mean((tf.matmul(X,w)-y)**2)\n", "    \n", "g = t.gradient(f, w)\n", "g"]}, {"cell_type": "markdown", "metadata": {}, "source": ["But a `tf.Tensor` is always a symbolic variable. In order to reconcile symbolic and execution worlds, `Tensorflow` **attaches** a value to each symbolic variable, and carries it forward when making derivations.\n", "\n", "- `X`, `y` and `w` are Tensors that we define with a specific value\n", "- `g` is a Tensor derived from `X`, `y` and `w` that have ALSO been evaluated with the corresponding values."]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": ["<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n", "array([[-38.      ],\n", "       [-48.666668]], dtype=float32)>"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["g"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[-38.      ],\n", "       [-48.666668]], dtype=float32)"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["g.numpy()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "p39", "language": "python", "name": "p39"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 2}