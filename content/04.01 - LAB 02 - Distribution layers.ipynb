{"cells": [{"cell_type": "code", "execution_count": null, "id": "29c09cf1", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "d70cb490", "metadata": {}, "source": ["# LAB 2. Distribution layers\n", "\n", "In this laboratory, We'll understand how the `tensorflow_probability.layers` module works."]}, {"cell_type": "code", "execution_count": null, "id": "e7e612b3", "metadata": {}, "outputs": [], "source": ["import inspect\n", "from rlxmoocapi import submit, session\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import tensorflow as tf\n", "import tensorflow_probability as tfp\n", "\n", "from scipy import stats\n", "from tensorflow.keras.layers import Dense, Input\n", "from tensorflow.keras.optimizers import SGD\n", "from tensorflow.keras.models import Model\n", "\n", "tfd = tfp.distributions\n", "tfpl = tfp.layers"]}, {"cell_type": "code", "execution_count": null, "id": "9ef3c13d", "metadata": {}, "outputs": [], "source": ["course_id = \"ppdl.v1\"\n", "endpoint = \"https://m5knaekxo6.execute-api.us-west-2.amazonaws.com/dev-v0001/rlxmooc\"\n", "lab = \"L02.03.02\""]}, {"cell_type": "code", "execution_count": null, "id": "e9cd3cb5", "metadata": {}, "outputs": [], "source": ["session.LoginSequence(\n", "    endpoint=endpoint,\n", "    course_id=course_id,\n", "    lab_id=lab,\n", "    varname=\"student\"\n", "    );"]}, {"cell_type": "markdown", "id": "e817ab76", "metadata": {}, "source": ["For this laboratory, We'll learn the distribution parameters for each point in the following dataset:"]}, {"cell_type": "code", "execution_count": null, "id": "49c3f512", "metadata": {}, "outputs": [], "source": ["x = np.linspace(0, 10, 100)\n", "y = (\n", "        .3 * x +\n", "        2 * np.sin(x / 1.5) +\n", "        np.random.normal(\n", "            loc=.2,\n", "            scale=.02 * ((x - 5) * 2) ** 2,\n", "            size=(100, )\n", "            )\n", "        )"]}, {"cell_type": "code", "execution_count": null, "id": "17a5b0a7", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n", "ax.scatter(x, y)\n", "ax.set_xlabel(\"$x$\")\n", "ax.set_ylabel(\"$y$\")"]}, {"cell_type": "markdown", "id": "e1429466", "metadata": {}, "source": ["## Task 1\n", "\n", "In this task you must a feedforward neural network using using `tensorflow.keras.models.Model`. This model must output an independent normal distribution.\n", "\n", "> **NOTE: the dense layer that represents the distribution parameters must have a linear activation.**"]}, {"cell_type": "code", "execution_count": null, "id": "4393266a", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def make_model(hidden_units, activation, input_shape):\n", "    # YOUR CODE HERE\n", "    return Model()"]}, {"cell_type": "markdown", "id": "9bfefa90", "metadata": {}, "source": ["The summary of your model must be equivalent to this one:\n", "\n", "![model_summary](local/imgs/prob_nn_summary.png)"]}, {"cell_type": "code", "execution_count": null, "id": "8aa84090", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["model = make_model(\n", "        hidden_units=[64, 32],\n", "        activation=\"relu\",\n", "        input_shape=1,\n", "        event_shape=1\n", "        )\n", "model.summary()"]}, {"cell_type": "markdown", "id": "1c83ae90", "metadata": {}, "source": ["Test your code:"]}, {"cell_type": "code", "execution_count": null, "id": "3c7b2a3b", "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T1\");"]}, {"cell_type": "code", "execution_count": null, "id": "d2735138", "metadata": {}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "markdown", "id": "64dd854c", "metadata": {}, "source": ["We can visualize the density of the model (untrained):"]}, {"cell_type": "code", "execution_count": null, "id": "6b084a4a", "metadata": {}, "outputs": [], "source": ["dist = model(x.reshape(-1, 1))\n", "means = dist.mean().numpy().flatten()\n", "stds = dist.stddev().numpy().flatten()"]}, {"cell_type": "code", "execution_count": null, "id": "e8b056a6", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(7, 7))\n", "ax.plot(x, means) \n", "ax.fill_between(x, means - 3 * stds, means + 3 * stds, alpha=0.5)"]}, {"cell_type": "markdown", "id": "f645c747", "metadata": {}, "source": ["## Task 2\n", "\n", "Implement the `train_model` function, which must compute the loss for a maximum likelihood estimation problem:\n", "\n", "$$\n", "\\mathcal{L} = -\\frac{1}{N}\\text{log}(P_x(y))\\\\\n", "P_x(y) = \\mathcal{N}(\\mu=f_1(x), \\sigma=f_2(x))\n", "$$\n", "\n", "Where $f_1(\\cdot)$ and $f_2(\\cdot)$ are functions approximated through the feedforward neural network."]}, {"cell_type": "code", "execution_count": null, "id": "5f178863", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def train_model(model, x, y, epochs, optimizer):\n", "    ## YOUR CODE HERE\n", "    for epoch in range(epochs):\n", "        with tf.GradientTape() as t:\n", "            loss = ...\n", "        grads = t.gradient(..., ...)\n", "        optimizer.apply_gradients(zip(..., ...))\n", "        print(f\"epoch: {epoch}, loss: {float(loss.numpy())}\")"]}, {"cell_type": "code", "execution_count": null, "id": "c1ede06b", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["train_model(model, x, y, 2000, SGD(learning_rate=0.01))"]}, {"cell_type": "code", "execution_count": null, "id": "12102d30", "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T2\");"]}, {"cell_type": "markdown", "id": "de95fa52", "metadata": {}, "source": ["Now, let's visualize the learned distributions, your predictions must look similar to the following result:\n", "\n", "![regression_dist](local/imgs/regression_dist.png)"]}, {"cell_type": "code", "execution_count": null, "id": "53a9636d", "metadata": {}, "outputs": [], "source": ["dist = model(x.reshape(-1, 1))\n", "means = dist.mean().numpy().flatten()\n", "stds = dist.stddev().numpy().flatten()\n", "\n", "fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n", "ax.plot(x, means, label=\"means\") \n", "ax.fill_between(x, means - 3 * stds, means + 3 * stds, alpha=0.5, label=\"stds\")\n", "ax.scatter(x, y, alpha=0.5, label=\"data\")\n", "ax.legend()\n", "ax.set_xlabel(\"$x$\")\n", "ax.set_ylabel(\"$y$\")"]}, {"cell_type": "markdown", "id": "24651c07", "metadata": {}, "source": ["## Task 3\n", "\n", "Implement the `log_prob` function **without** `tensorflow_probability` or `tensorflow`. You can use `numpy` and `scipy.stats`. This function receives as arguments the data (`x`, `y`), and the neural network parameters `weights`.\n", "\n", "> **NOTE 1: you must transform the `rstd` ($\\hat{\\sigma}$) into valid standard deviations, with a softplus transformation**:\n", "\n", "$$\n", "\\sigma = \\text{log}(\\text{exp}(\\hat{\\sigma}) + 1)\n", "$$\n", "\n", "> **NOTE 2: your function must support models with different layers sizes, you can test your function creating models with different number of layers and units.**"]}, {"cell_type": "code", "execution_count": null, "id": "fcc1c62e", "metadata": {}, "outputs": [], "source": ["weights = [layer.get_weights() for layer in model.layers[1:-1]]"]}, {"cell_type": "code", "execution_count": null, "id": "7873daad", "metadata": {}, "outputs": [], "source": ["print(len(weights))"]}, {"cell_type": "code", "execution_count": null, "id": "068ac50a", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["# parameters for layer 1\n", "w, b = weights[0]\n", "print(w)\n", "print(b)"]}, {"cell_type": "code", "execution_count": null, "id": "b16cd350", "metadata": {}, "outputs": [], "source": ["pdf1 = pdf(x, y, weights)\n", "pdf1[:10]"]}, {"cell_type": "code", "execution_count": null, "id": "a381c141", "metadata": {}, "outputs": [], "source": ["dist = model(x.reshape(-1, 1))\n", "pdf2 = np.exp(dist.log_prob(y.reshape(-1, 1)))\n", "pdf2[:10]"]}, {"cell_type": "markdown", "id": "13d35f69", "metadata": {}, "source": ["You can visualize that both log probs are the same:"]}, {"cell_type": "code", "execution_count": null, "id": "5184138d", "metadata": {}, "outputs": [], "source": ["data1 = np.array(sorted(zip(y, pdf1), key=lambda x: x[0])).T\n", "data2 = np.array(sorted(zip(y, pdf2), key=lambda x: x[0])).T"]}, {"cell_type": "code", "execution_count": null, "id": "c1945c14", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n", "ax[0].plot(data1[0], data1[1])\n", "ax[0].fill_between(\n", "        data1[0], np.zeros((100, )), data1[1], alpha=0.4\n", "        )\n", "ax[0].set_title(\"Your solution\")\n", "\n", "ax[1].plot(data2[0], data2[1])\n", "ax[1].fill_between(\n", "        data2[0], np.zeros((100, )), data2[1], alpha=0.4\n", "        )\n", "ax[1].set_title(\"Tensorflow probability\")"]}, {"cell_type": "code", "execution_count": null, "id": "7c109d75", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T3\");"]}, {"cell_type": "markdown", "id": "c9807505", "metadata": {}, "source": ["## Task 4\n", "\n", "Compute the gradients for the probabilistic layer in the model **without** `tensorflow`. You can use `numpy` and `scipy`.\n", "\n", "Consider the loss function:\n", "\n", "$$\n", "\\mathcal{L}(\\mu_i, \\sigma_i) = \\text{log}(\\mathcal{N}(\\mu_i, \\sigma_i))\\\\\n", "\\frac{\\partial \\mathcal{L}(\\mu_i, \\sigma_i)}{\\partial \\mu_i} = ?\\\\\n", "$$\n", "\n", "> **NOTE: you must transform the `rstd` ($\\hat{\\sigma}$) into valid standard deviations, with a softplus transformation**:\n", "\n", "$$\n", "\\sigma = \\text{log}(\\text{exp}(\\hat{\\sigma}) + 1)\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "a3ff676b", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def compute_gradients(mu, rstd, X):\n", "    # YOUR CODE HERE\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "id": "fc309b29", "metadata": {}, "outputs": [], "source": ["mu = np.random.normal(loc=1, scale=1, size=(10, 1))\n", "rstd = np.random.normal(loc=1, scale=1, size=(10, 1))\n", "parameters = np.concatenate([mu, rstd], axis=1).astype(\"float32\")\n", "X = np.random.normal(loc=0, scale=1, size=(10, 1)).astype(\"float32\")"]}, {"cell_type": "code", "execution_count": null, "id": "79417962", "metadata": {}, "outputs": [], "source": ["raw_grads = compute_gradients(mu, rstd, X)\n", "print(raw_grads)"]}, {"cell_type": "code", "execution_count": null, "id": "5f4978d0", "metadata": {}, "outputs": [], "source": ["# Let's see the `tensorflow` solution"]}, {"cell_type": "code", "execution_count": null, "id": "243364bf", "metadata": {}, "outputs": [], "source": ["input_layer = Input(shape=(2, ))\n", "output = tfpl.IndependentNormal(event_shape=1)(input_layer)\n", "model = Model(input_layer, output)\n", "model.summary()"]}, {"cell_type": "markdown", "id": "9487b2b4", "metadata": {}, "source": ["Your function must return the following result:"]}, {"cell_type": "code", "execution_count": null, "id": "6beea3da", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["params_tf = tf.constant(parameters)\n", "X_tf = tf.constant(X)\n", "with tf.GradientTape() as t:\n", "    t.watch(params_tf)\n", "    dist = model(params_tf)\n", "    output = - dist.log_prob(X_tf)\n", "\n", "grads = t.gradient(output, params_tf)[:, :1]\n", "print(grads.numpy())"]}, {"cell_type": "code", "execution_count": null, "id": "b84d96a0", "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T4\");"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}