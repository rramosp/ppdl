{"cells": [{"cell_type": "code", "execution_count": null, "id": "2ac7ef1d", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "b5b49fae", "metadata": {}, "source": ["# LAB 2. Distribution layers\n", "\n", "In this laboratory, We'll understand how the `tensorflow_probability.layers` module works."]}, {"cell_type": "code", "execution_count": null, "id": "679a5f2f", "metadata": {}, "outputs": [], "source": ["import inspect\n", "from rlxmoocapi import submit, session\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import tensorflow as tf\n", "import tensorflow_probability as tfp\n", "\n", "from scipy import stats\n", "from tensorflow.keras.layers import Dense, Input\n", "from tensorflow.keras.optimizers import SGD\n", "from tensorflow.keras.models import Model\n", "\n", "tfd = tfp.distributions\n", "tfpl = tfp.layers"]}, {"cell_type": "code", "execution_count": null, "id": "320b5563", "metadata": {}, "outputs": [], "source": ["course_id = \"ppdl.v1\"\n", "endpoint = \"https://m5knaekxo6.execute-api.us-west-2.amazonaws.com/dev-v0001/rlxmooc\"\n", "lab = \"L02.03.02\""]}, {"cell_type": "code", "execution_count": null, "id": "7d8c1f70", "metadata": {}, "outputs": [], "source": ["session.LoginSequence(\n", "    endpoint=endpoint,\n", "    course_id=course_id,\n", "    lab_id=lab,\n", "    varname=\"student\"\n", "    );"]}, {"cell_type": "markdown", "id": "90819e00", "metadata": {}, "source": ["For this laboratory, We'll learn the distribution parameters for each point in the following dataset:"]}, {"cell_type": "code", "execution_count": null, "id": "03c9aca9", "metadata": {}, "outputs": [], "source": ["x = np.linspace(0, 10, 100)\n", "y = (\n", "        .3 * x +\n", "        2 * np.sin(x / 1.5) +\n", "        np.random.normal(\n", "            loc=.2,\n", "            scale=.02 * ((x - 5) * 2) ** 2,\n", "            size=(100, )\n", "            )\n", "        )"]}, {"cell_type": "code", "execution_count": null, "id": "36303df9", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n", "ax.scatter(x, y)\n", "ax.set_xlabel(\"$x$\")\n", "ax.set_ylabel(\"$y$\")"]}, {"cell_type": "markdown", "id": "f7a3bb00", "metadata": {}, "source": ["## Task 1\n", "\n", "In this task you must a feedforward neural network using using `tensorflow.keras.models.Model`. This model must output an independent normal distribution.\n", "\n", "> **NOTE: the dense layer that represents the distribution parameters must have a linear activation.**"]}, {"cell_type": "code", "execution_count": null, "id": "f49e87c7", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def make_model(hidden_units, activation, input_shape):\n", "    # YOUR CODE HERE\n", "    return Model()"]}, {"cell_type": "markdown", "id": "f226c051", "metadata": {}, "source": ["The summary of your model must be equivalent to this one:\n", "\n", "![model_summary](local/imgs/prob_nn_summary.png)"]}, {"cell_type": "code", "execution_count": null, "id": "05a8b8a0", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["model = make_model(\n", "        hidden_units=[64, 32],\n", "        activation=\"relu\",\n", "        input_shape=1,\n", "        event_shape=1\n", "        )\n", "model.summary()"]}, {"cell_type": "markdown", "id": "f41915db", "metadata": {}, "source": ["Test your code:"]}, {"cell_type": "code", "execution_count": null, "id": "40555761", "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T1\");"]}, {"cell_type": "code", "execution_count": null, "id": "cc298693", "metadata": {}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "markdown", "id": "d8bf834e", "metadata": {}, "source": ["We can visualize the density of the model (untrained):"]}, {"cell_type": "code", "execution_count": null, "id": "a161f5b3", "metadata": {}, "outputs": [], "source": ["dist = model(x.reshape(-1, 1))\n", "means = dist.mean().numpy().flatten()\n", "stds = dist.stddev().numpy().flatten()"]}, {"cell_type": "code", "execution_count": null, "id": "566d5cb6", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(7, 7))\n", "ax.plot(x, means) \n", "ax.fill_between(x, means - 3 * stds, means + 3 * stds, alpha=0.5)"]}, {"cell_type": "markdown", "id": "896f2e40", "metadata": {}, "source": ["## Task 2\n", "\n", "Implement the `train_model` function, which must compute the loss for a maximum likelihood estimation problem:\n", "\n", "$$\n", "\\mathcal{L} = -\\frac{1}{N}\\text{log}(P_x(y))\\\\\n", "P_x(y) = \\mathcal{N}(\\mu=f_1(x), \\sigma=f_2(x))\n", "$$\n", "\n", "Where $f_1(\\cdot)$ and $f_2(\\cdot)$ are functions approximated through the feedforward neural network."]}, {"cell_type": "code", "execution_count": null, "id": "f0ec837e", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def train_model(model, x, y, epochs, optimizer):\n", "    ## YOUR CODE HERE\n", "    for epoch in range(epochs):\n", "        with tf.GradientTape() as t:\n", "            loss = ...\n", "        grads = t.gradient(..., ...)\n", "        optimizer.apply_gradients(zip(..., ...))\n", "        print(f\"epoch: {epoch}, loss: {float(loss.numpy())}\")"]}, {"cell_type": "code", "execution_count": null, "id": "657803fc", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["train_model(model, x, y, 2000, SGD(learning_rate=0.01))"]}, {"cell_type": "code", "execution_count": null, "id": "653aeccd", "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T2\");"]}, {"cell_type": "markdown", "id": "1db89571", "metadata": {}, "source": ["Now, let's visualize the learned distributions, your predictions must look similar to the following result:\n", "\n", "![regression_dist](local/imgs/regression_dist.png)"]}, {"cell_type": "code", "execution_count": null, "id": "54a31d6d", "metadata": {}, "outputs": [], "source": ["dist = model(x.reshape(-1, 1))\n", "means = dist.mean().numpy().flatten()\n", "stds = dist.stddev().numpy().flatten()\n", "\n", "fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n", "ax.plot(x, means, label=\"means\") \n", "ax.fill_between(x, means - 3 * stds, means + 3 * stds, alpha=0.5, label=\"stds\")\n", "ax.scatter(x, y, alpha=0.5, label=\"data\")\n", "ax.legend()\n", "ax.set_xlabel(\"$x$\")\n", "ax.set_ylabel(\"$y$\")"]}, {"cell_type": "markdown", "id": "76ec4ca0", "metadata": {}, "source": ["## Task 3\n", "\n", "Implement the `log_prob` function **without** `tensorflow_probability` or `tensorflow`. You can use `numpy` and `scipy.stats`. This function receives as arguments the data (`x`, `y`), and the neural network parameters `weights`.\n", "\n", "> **NOTE 1: you must transform the `rstd` ($\\hat{\\sigma}$) into valid standard deviations, with a softplus transformation**:\n", "\n", "$$\n", "\\sigma = \\text{log}(\\text{exp}(\\hat{\\sigma}) + 1)\n", "$$\n", "\n", "> **NOTE 2: your function must support models with different layers sizes, you can test your function creating models with different number of layers and units.**"]}, {"cell_type": "code", "execution_count": null, "id": "4f8a7ea5", "metadata": {}, "outputs": [], "source": ["weights = [layer.get_weights() for layer in model.layers[1:-1]]"]}, {"cell_type": "code", "execution_count": null, "id": "d14f36cb", "metadata": {}, "outputs": [], "source": ["print(len(weights))"]}, {"cell_type": "code", "execution_count": null, "id": "d8268ccd", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["# parameters for layer 1\n", "w, b = weights[0]\n", "print(w)\n", "print(b)"]}, {"cell_type": "code", "execution_count": null, "id": "c38513b8", "metadata": {}, "outputs": [], "source": ["pdf1 = pdf(x, y, weights)\n", "pdf1[:10]"]}, {"cell_type": "code", "execution_count": null, "id": "617d859d", "metadata": {}, "outputs": [], "source": ["dist = model(x.reshape(-1, 1))\n", "pdf2 = np.exp(dist.log_prob(y.reshape(-1, 1)))\n", "pdf2[:10]"]}, {"cell_type": "markdown", "id": "83e02c91", "metadata": {}, "source": ["You can visualize that both log probs are the same:"]}, {"cell_type": "code", "execution_count": null, "id": "1e260c99", "metadata": {}, "outputs": [], "source": ["data1 = np.array(sorted(zip(y, pdf1), key=lambda x: x[0])).T\n", "data2 = np.array(sorted(zip(y, pdf2), key=lambda x: x[0])).T"]}, {"cell_type": "code", "execution_count": null, "id": "766d1b2b", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n", "ax[0].plot(data1[0], data1[1])\n", "ax[0].fill_between(\n", "        data1[0], np.zeros((100, )), data1[1], alpha=0.4\n", "        )\n", "ax[0].set_title(\"Your solution\")\n", "\n", "ax[1].plot(data2[0], data2[1])\n", "ax[1].fill_between(\n", "        data2[0], np.zeros((100, )), data2[1], alpha=0.4\n", "        )\n", "ax[1].set_title(\"Tensorflow probability\")"]}, {"cell_type": "code", "execution_count": null, "id": "f1ca494f", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T3\");"]}, {"cell_type": "markdown", "id": "229efb9a", "metadata": {}, "source": ["## Task 4\n", "\n", "Compute the gradients for the probabilistic layer in the model **without** `tensorflow`. You can use `numpy` and `scipy`.\n", "\n", "Consider the loss function:\n", "\n", "$$\n", "\\mathcal{L}(\\mu_i, \\sigma_i) = \\text{log}(\\mathcal{N}(\\mu_i, \\sigma_i))\\\\\n", "\\frac{\\partial \\mathcal{L}(\\mu_i, \\sigma_i)}{\\partial \\mu_i} = ?\\\\\n", "$$\n", "\n", "> **NOTE: you must transform the `rstd` ($\\hat{\\sigma}$) into valid standard deviations, with a softplus transformation**:\n", "\n", "$$\n", "\\sigma = \\text{log}(\\text{exp}(\\hat{\\sigma}) + 1)\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "e501699d", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def compute_gradients(mu, rstd, X):\n", "    # YOUR CODE HERE\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "id": "d5c5440c", "metadata": {}, "outputs": [], "source": ["mu = np.random.normal(loc=1, scale=1, size=(10, 1))\n", "rstd = np.random.normal(loc=1, scale=1, size=(10, 1))\n", "parameters = np.concatenate([mu, rstd], axis=1).astype(\"float32\")\n", "X = np.random.normal(loc=0, scale=1, size=(10, 1)).astype(\"float32\")"]}, {"cell_type": "code", "execution_count": null, "id": "7435af26", "metadata": {}, "outputs": [], "source": ["raw_grads = compute_gradients(mu, rstd, X)\n", "print(raw_grads)"]}, {"cell_type": "code", "execution_count": null, "id": "4192529b", "metadata": {}, "outputs": [], "source": ["# Let's see the `tensorflow` solution"]}, {"cell_type": "code", "execution_count": null, "id": "ee0f975b", "metadata": {}, "outputs": [], "source": ["input_layer = Input(shape=(2, ))\n", "output = tfpl.IndependentNormal(event_shape=1)(input_layer)\n", "model = Model(input_layer, output)\n", "model.summary()"]}, {"cell_type": "markdown", "id": "a2494e5f", "metadata": {}, "source": ["Your function must return the following result:"]}, {"cell_type": "code", "execution_count": null, "id": "41851606", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["params_tf = tf.constant(parameters)\n", "X_tf = tf.constant(X)\n", "with tf.GradientTape() as t:\n", "    t.watch(params_tf)\n", "    dist = model(params_tf)\n", "    output = - dist.log_prob(X_tf)\n", "\n", "grads = t.gradient(output, params_tf)[:, :1]\n", "print(grads.numpy())"]}, {"cell_type": "code", "execution_count": null, "id": "5688fb2d", "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T4\");"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}