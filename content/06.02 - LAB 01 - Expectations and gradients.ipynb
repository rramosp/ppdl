{"cells": [{"cell_type": "code", "execution_count": null, "id": "2dba479c", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "fb7a93ac", "metadata": {}, "source": ["# Lab 06.01.01: Expectations, Gradients and Reparameterization Trick\n", "---\n", "\n", "In this laboratory you'll compute gradients using different approaches, including numerical methods, Monte Carlo sampling and the reparameterization trick."]}, {"cell_type": "code", "execution_count": null, "id": "1684aa71", "metadata": {}, "outputs": [], "source": ["## Ignore this cell\n", "!pip install ppdl==0.1.5 rlxmoocapi==0.1.0 --quiet"]}, {"cell_type": "code", "execution_count": null, "id": "bd02dff7", "metadata": {}, "outputs": [], "source": ["import inspect\n", "from rlxmoocapi import submit, session\n", "from tensorflow_probability.python.math import integration\n", "course_id = \"ppdl.v1\"\n", "endpoint = \"https://m5knaekxo6.execute-api.us-west-2.amazonaws.com/dev-v0001/rlxmooc\"\n", "lab = \"L06.01.01\""]}, {"cell_type": "markdown", "id": "4ff2f7cc", "metadata": {}, "source": ["Log-in with your username and password:"]}, {"cell_type": "code", "execution_count": null, "id": "92be6dd8", "metadata": {}, "outputs": [], "source": ["session.LoginSequence(\n", "    endpoint=endpoint,\n", "    course_id=course_id,\n", "    lab_id=lab,\n", "    varname=\"student\"\n", "    );"]}, {"cell_type": "markdown", "id": "300cd737", "metadata": {}, "source": ["First, let us import the required libraries:"]}, {"cell_type": "code", "execution_count": null, "id": "29a88039", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "import tensorflow_probability as tfp\n", "\n", "tfd = tfp.distributions\n", "tfb = tfp.bijectors\n", "\n", "plt.style.use(\"ggplot\")\n", "COLOR1 = \"#598EDC\"\n", "COLOR2 = \"#57956C\""]}, {"cell_type": "markdown", "id": "105f6d5e", "metadata": {}, "source": ["## Task 1\n", "---\n", "\n", "In this task you have to compute a numerical approximation of the expectation of any function $f(x)$ given any probability distribution $p(x)$, as follows:\n", "\n", "$$\n", "\\underset{p(x)}{\\mathbb{E}}[f(x)] = \\int p(x) f(x) dx\n", "$$\n", "\n", "You must implement the `expected_value_trapz` function, which has the following parameters:\n", "\n", "* `f`: a `tf.function` representing $f(x)$.\n", "* `dist`: a `tfd.Distribution` representing $p(x)$.\n", "* `integration_range`: a tuple with the range that must be used for the numerical approximation of the integral.\n", "* `n_points`: number of points for the numerical approximation.\n", "\n", "You must use the trapezoidal rule to approximate the integral, please use the `tensorflow_probability`'s implementation `tfp.math.trapz`:"]}, {"cell_type": "code", "execution_count": null, "id": "fed246de", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def expected_value_trapz(f, dist, integration_range, n_points=1000):\n", "    ..."]}, {"cell_type": "markdown", "id": "8e1068ce", "metadata": {}, "source": ["Lets test the function with the following values:"]}, {"cell_type": "code", "execution_count": null, "id": "876174a0", "metadata": {}, "outputs": [], "source": ["@tf.function\n", "def function(x):\n", "    return tf.exp(-0.01 * tf.abs(x)) * tf.cos(0.1 * x)\n", "\n", "integration_range = (-100, 100)\n", "\n", "dist = tfd.Uniform(\n", "        low=integration_range[0],\n", "        high=integration_range[1]\n", "        )\n", "\n", "n_points = 1000"]}, {"cell_type": "markdown", "id": "41502ef6", "metadata": {}, "source": ["The following figure shows how must be the expectation for the given function and probability distribution.\n", "\n", "You must obtain the same value that is shown in this image:\n", "\n", "![expectation_value](local/imgs/expectation_value.png)"]}, {"cell_type": "code", "execution_count": null, "id": "fefb5667", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax2 = ax.twinx()\n", "x_range = tf.cast(tf.linspace(*integration_range, 1000), \"float32\")\n", "ax.plot(x_range, function(x_range), color=COLOR1)\n", "ax.set_xlabel(\"$x$\")\n", "ax.set_ylabel(\"$f(x)$\", color=COLOR1)\n", "\n", "ax2.plot(x_range, dist.prob(x_range), color=COLOR2)\n", "ax2.set_ylabel(\"$P(x)$\", color=COLOR2)\n", "ax2.grid(False)\n", "e_val = expected_value_trapz(function, dist, integration_range, n_points)\n", "ax2.set_title(r\"$\\mathbb{E}[f(x)]=\" + f\"{e_val:.2f}$\")\n", "\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "7e92fc55", "metadata": {}, "source": ["Now, let's change the probability distribution to see differences in the expectation.\n", "\n", "You must obtain the following result if your implementation of `expected_value_trapz` is correct:\n", "\n", "![expectation_value_dists](local/imgs/expectation_value_dists.png)"]}, {"cell_type": "code", "execution_count": null, "id": "9d00edbd", "metadata": {}, "outputs": [], "source": ["dists = [\n", "        tfd.Normal(loc=mean, scale=5)\n", "        for mean in tf.cast(tf.linspace(-50, 50, 5), \"float32\")\n", "        ]\n", "\n", "fig, axes = plt.subplots(5, 1, figsize=(10, 15))\n", "for i in range(5):\n", "    ax = axes[i]\n", "    ax2 = ax.twinx()\n", "\n", "    dist = dists[i]\n", "\n", "    x_range = tf.cast(\n", "            tf.linspace(*integration_range, 100),\n", "            \"float32\"\n", "            )\n", "    ax.plot(x_range, function(x_range), color=COLOR1)\n", "    ax.set_xlabel(\"$x$\")\n", "    ax.set_ylabel(\"$f(x)$\", color=COLOR1)\n", "\n", "    ax2.plot(x_range, dist.prob(x_range), color=COLOR2)\n", "    ax2.set_ylabel(\"$P(x)$\", color=COLOR2)\n", "    ax2.grid(False)\n", "    e_val = expected_value_trapz(function, dist, integration_range, n_points)\n", "    ax2.set_title(\n", "            r\"$\\mathbb{E}[f(x)]=\" + f\"{e_val:.2f}, P(x)=N(\\\\mu={dist.mean():.2f}, \\\\sigma=5)$\"\n", "            )\n", "fig.tight_layout()"]}, {"cell_type": "code", "execution_count": null, "id": "e1bf247c", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T1\");"]}, {"cell_type": "markdown", "id": "68d14652", "metadata": {}, "source": ["## Task 2\n", "\n", "In this task, you must implement an expectation using the Monte Carlo approach, i.e.:\n", "\n", "$$\n", "\\underset{P(x)}{\\mathbb{E}}[f(x)] \\approx \\frac{1}{N}\\sum_{i=1} ^ N f(x_i)\\\\\n", "x_i \\sim P(x)\\\\\n", "N \\rightarrow \\infty\\\\\n", "$$\n", "\n", "You have to implement the `expected_value_mc` function that has the following parameters:\n", "\n", "* `f`: a `tf.function` representing $f(x)$.\n", "* `dist`: a `tfd.Distribution` representing $p(x)$.\n", "* `n_samples`: number of samples to use for the Monte Carlo approximation.\n", "\n", "> **Note**: the `expected_value_mc` function must be implemented using `tensorflow` operations only. You can't use `numpy` neither _Python_'s native functions like `sum` or `for` loops."]}, {"cell_type": "code", "execution_count": null, "id": "63f3f0c3", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def expected_value_mc(f, dist, n_samples):\n", "    ..."]}, {"cell_type": "markdown", "id": "3941fbd3", "metadata": {}, "source": ["Let's compute the expected value for the following values:"]}, {"cell_type": "code", "execution_count": null, "id": "b64411ba", "metadata": {}, "outputs": [], "source": ["@tf.function\n", "def function(x):\n", "    return tf.exp(-0.05 * tf.abs(x))\n", "    \n", "dist = tfd.Normal(loc=1, scale=1)\n", "n_points = 10_000\n", "integration_range = (-10, 10)"]}, {"cell_type": "markdown", "id": "46d0105c", "metadata": {}, "source": ["The following cell must return values similar to:\n", "\n", "```python\n", "\u2771 print(f\"Monte Carlo: {e_mc}\")\n", "Monte Carlo: 0.9449480175971985\n", "\u2771 print(f\"Trapz integration: {e_trapz}\")\n", "Trapz integration: 0.9440837502479553\n", "```"]}, {"cell_type": "code", "execution_count": null, "id": "cd93a042", "metadata": {}, "outputs": [], "source": ["e_mc = expected_value_mc(function, dist, n_points)\n", "e_trapz = expected_value_trapz(\n", "        function, dist, integration_range, n_points\n", "        )\n", "\n", "print(f\"Monte Carlo: {e_mc}\")\n", "print(f\"Trapz integration: {e_trapz}\")"]}, {"cell_type": "markdown", "id": "d1bbde03", "metadata": {}, "source": ["Now, we can see a comparison between `expected_value_mc` and `expected_value_trapz` functions using different distributions.\n", "\n", "If your implementation is correct, the following cell must output the following image:\n", "\n", "![companrison_expected_values](local/imgs/comparison_expected.png)"]}, {"cell_type": "code", "execution_count": null, "id": "253e3a97", "metadata": {}, "outputs": [], "source": ["dists = [\n", "        tfd.Normal(mean, 5)\n", "        for mean in tf.cast(tf.linspace(-50, 50, 5), \"float32\")\n", "        ]\n", "\n", "fig, axes = plt.subplots(5, 1, figsize=(10, 15))\n", "for i in range(5):\n", "    ax = axes[i]\n", "    ax2 = ax.twinx()\n", "\n", "    dist = dists[i]\n", "\n", "    x_range = tf.cast(tf.linspace(-100, 100, 1000), \"float32\")\n", "    ax.plot(x_range, function(x_range), color=COLOR1)\n", "    ax.set_xlabel(\"$x$\")\n", "    ax.set_ylabel(\"$f(x)$\", color=COLOR1)\n", "\n", "    ax2.plot(x_range, dist.prob(x_range), color=COLOR2)\n", "    ax2.set_ylabel(\"$P(x)$\", color=COLOR2)\n", "    ax2.grid(False)\n", "\n", "    e_mc = expected_value_mc(function, dist, 10000)\n", "    e_trapz = expected_value_trapz(function, dist, (-100, 100), 10000)\n", "\n", "    ax2.set_title(f\"MC={e_mc:.2f}, TRAPZ={e_trapz:.2f}\")\n", "fig.tight_layout()"]}, {"cell_type": "code", "execution_count": null, "id": "4a5ada72", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T2\");"]}, {"cell_type": "markdown", "id": "843a74eb", "metadata": {}, "source": ["## Task 3\n", "\n", "In this task you must compute the gradients with respect to a function's parameter $\\theta$ given it's expectation.\n", "\n", "Remember that the gradient of an expectation is equal to the expectation of the gradients:\n", "\n", "$$\n", "\\underset{\\theta}{\\nabla} \\underset{p(x)}{\\mathbb{E}}[f(x, \\theta)] = \\underset{\\theta}{\\nabla} \\int p(x) f(x, \\theta) dx\\\\\n", "\\underset{\\theta}{\\nabla} \\underset{p(x)}{\\mathbb{E}}[f(x, \\theta)] =  \\int p(x) \\underset{\\theta}{\\nabla} f(x, \\theta) dx\\\\\n", "\\underset{\\theta}{\\nabla} \\underset{p(x)}{\\mathbb{E}}[f(x, \\theta)] =   \\underset{p(x)}{\\mathbb{E}}[\\underset{\\theta}{\\nabla}f(x, \\theta)]\n", "$$\n", "\n", "To achieve this, compute $\\underset{p(x)}{\\mathbb{E}}[\\underset{\\theta}{\\nabla}f(x, \\theta)]$ using the Monte Carlo method. You must implement the `expectation_gradients` function which has the following parameters:\n", "\n", "* `f`: a `tf.function` representing $f(x, \\theta)$.\n", "* `theta`: `tf.Tensor` to compute the gradients.\n", "* `dist`: a `tfd.Distribution` representing $p(x)$.\n", "* `n_samples`: number of samples to use for the Monte Carlo approximation.\n", "\n", "> **Note**: you must compute the gradients using `tf.GradientTape`:"]}, {"cell_type": "code", "execution_count": null, "id": "c7e6e0e4", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def expectation_gradients(f, theta, dist, n_samples):\n", "    ..."]}, {"cell_type": "markdown", "id": "1c910b56", "metadata": {}, "source": ["Let's compute the gradients using the following values:"]}, {"cell_type": "code", "execution_count": null, "id": "e0639632", "metadata": {}, "outputs": [], "source": ["@tf.function\n", "def function(x, theta):\n", "    return tf.exp(theta * tf.abs(x))\n", "\n", "theta_0 = tf.Variable(-0.05)\n", "dist = tfd.Normal(loc=0, scale=1)\n", "n_samples = 1000"]}, {"cell_type": "markdown", "id": "3f091591", "metadata": {}, "source": ["The following cell must return values similar to:\n", "\n", "```python\n", "\u2771 print(expectation_gradients(function, theta_0, dist, n_samples))\n", "tf.Tensor(0.7384695, shape=(), dtype=float32)\n", "```"]}, {"cell_type": "code", "execution_count": null, "id": "a2a76232", "metadata": {}, "outputs": [], "source": ["print(expectation_gradients(function, theta_0, dist, n_samples))"]}, {"cell_type": "markdown", "id": "0ee58a94", "metadata": {}, "source": ["Now, let's see a comparison between the gradient's estimation using a large number of samples vs an expectation using batches.\n", "\n", "The following cell must output values similar to:\n", "\n", "```python\n", "\u2771 print(large_expectation)\n", "tf.Tensor(1.0713083, shape=(), dtype=float32)\n", "\u2771 print(batches_expectation)\n", "tf.Tensor(1.0762898, shape=(), dtype=float32)\n", "```"]}, {"cell_type": "code", "execution_count": null, "id": "bd2d50f7", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["# large sample\n", "dist = tfd.Normal(loc=1, scale=1)\n", "large_expectation = expectation_gradients(function, theta_0, dist, int(16e3))\n", "# batches\n", "batches_expectation = tf.reduce_mean(\n", "        [\n", "            expectation_gradients(function, theta_0, dist, 16)\n", "            for _ in range(1000)\n", "            ]\n", "        )\n", "print(large_expectation)\n", "print(batches_expectation)"]}, {"cell_type": "code", "execution_count": null, "id": "a5ce4cf2", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T3\");"]}, {"cell_type": "markdown", "id": "4bc8e5f5", "metadata": {}, "source": ["## Task 4\n", "\n", "In this task you must compute the gradients with respect of a function's parameter $\\theta$ given its expectation using a probability distribution which also has $\\theta$ as a parameter.\n", "\n", "Remember that this can't be directly computed using the Monte Carlo approach since the distribution's pdf also depends on $\\theta$:\n", "\n", "$$\n", "\\underset{\\theta}{\\nabla} \\underset{p(x|\\theta)}{\\mathbb{E}}[f(x, \\theta)] = \\underset{\\theta}{\\nabla} \\int p(x|\\theta) f(x, \\theta) dx\\\\\n", "\\underset{\\theta}{\\nabla} \\underset{p(x|\\theta)}{\\mathbb{E}}[f(x, \\theta)] = \\int \\underset{\\theta}{\\nabla}( p(x|\\theta) f(x, \\theta)) dx\\\\\n", "\\underset{\\theta}{\\nabla} \\underset{p(x|\\theta)}{\\mathbb{E}}[f(x, \\theta)] = \\int \\underset{\\theta}{\\nabla} p(x|\\theta) f(x, \\theta) + p(x|\\theta)\\underset{\\theta}{\\nabla} f(x, \\theta) dx\\\\\n", "$$\n", "\n", "From the last equation:\n", "\n", "$$\n", "\\underset{\\theta}{\\nabla} \\underset{p(x|\\theta)}{\\mathbb{E}}[f(x, \\theta)] = \\underbrace{\\int \\underset{\\theta}{\\nabla} p(x|\\theta) f(x, \\theta) dx}_\\text{Numerically compute this} + \\underbrace{\\underset{P(x|\\theta)}{\\mathbb{E}}[\\underset{\\theta}{\\nabla}f(x, \\theta)]}_\\text{Monte Carlo approach for this}\n", "$$\n", "\n", "You must implement the `gradients_dependence` function which has the following parameters:\n", "\n", "* `f`: a `tf.function` representing $f(x, \\theta)$.\n", "* `theta`: `tf.Tensor` to compute the gradients.\n", "* `dist_f`: a function that generates a `tfd.Distribution` given a `theta` value.\n", "* `n_samples`: number of samples to use for the Monte Carlo approximation and the trapezoidal rule.\n", "* `integration_range`: integration range to use in the trapezoidal rule.\n", "\n", "> **Note**: you must numerically approximate the left part of the equation using `tfp.math.trapz` and the right part using the Monte Carlo method. You must compute the gradients using `tf.GradientTape` using pure `tensorflow` code. Use `n_samples` for both the trapezoidal and Monte Carlo approaches."]}, {"cell_type": "code", "execution_count": null, "id": "932e2ff6", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def gradients_dependence(\n", "        f, theta, dist_f, n_samples,\n", "        integration_range\n", "        ):\n", "    ..."]}, {"cell_type": "markdown", "id": "6ea381c0", "metadata": {}, "source": ["Let's compute the gradients using the following values:"]}, {"cell_type": "code", "execution_count": null, "id": "b1e09b9b", "metadata": {}, "outputs": [], "source": ["@tf.function\n", "def function(x, theta):\n", "    return tf.exp(theta * tf.abs(x))\n", "\n", "def dist_function(theta):\n", "    return tfd.Normal(loc=theta, scale=1)\n", "\n", "theta_0 = tf.Variable(-1.0)\n", "integration_range = (-100, 100)\n", "n_samples = 10_000"]}, {"cell_type": "markdown", "id": "96339855", "metadata": {}, "source": ["The following cell must output values similar to:\n", "\n", "```python\n", "\u2771 print(grads_dep)\n", "tf.Tensor(0.4767751, shape=(), dtype=float32)\n", "```"]}, {"cell_type": "code", "execution_count": null, "id": "41e3afba", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["grads_dep = gradients_dependence(\n", "        function, theta_0, dist_function,\n", "        n_samples, integration_range\n", "        )\n", "print(grads_dep)"]}, {"cell_type": "code", "execution_count": null, "id": "69d7e0a1", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T4\");"]}, {"cell_type": "markdown", "id": "17256242", "metadata": {}, "source": ["## Task 5\n", "\n", "As you noted in the previous task, computing the gradients when there's a dependence between the distribution's pdf and the function's parameters is not an easy task, specially, it involves a numerical integration step that does not scale when We have parameters in higher dimensions.\n", "\n", "For this reason, We need the reparameterization trick, in this task you must compute the gradients for this distribution:\n", "\n", "$$\n", "x \\sim N(\\mu=\\theta, \\sigma=1)\n", "$$\n", "\n", "And this reparameterization:\n", "\n", "$$\n", "x' \\sim N(\\mu=0, \\sigma=0)\\\\\n", "g(x', \\theta) = x' + \\theta\n", "$$\n", "\n", "Remember that after reparameterization, the gradients are:\n", "\n", "$$\n", "\\underset{\\theta}{\\nabla} \\underset{P(x')}{\\mathbb{E}}[f(g(x', \\theta), \\theta)] = \\underset{\\theta}{\\nabla} \\int p(x') f(g(x', \\theta), \\theta) dx\\\\\n", "\\underset{\\theta}{\\nabla} \\underset{P(x')}{\\mathbb{E}}[f(g(x', \\theta), \\theta)] = \\int p(x') \\underset{\\theta}{\\nabla} f(g(x', \\theta), \\theta) dx\n", "$$\n", "\n", "This means, you can compute the gradients using the Monte Carlo method:\n", "\n", "$$\n", "\\underset{\\theta}{\\nabla} \\underset{P(x')}{\\mathbb{E}}[f(g(x', \\theta), \\theta)] = \\underset{P(x')}{\\mathbb{E}}\\left[\\underset{\\theta}{\\nabla} f(g(x', \\theta), \\theta)\\right]\n", "$$\n", "\n", "In this task, you must implement the `gradients_reparameterization` function which has the following parameters:\n", "\n", "* `f`: a `tf.function` representing $f(x, \\theta)$.\n", "* `theta`: `tf.Tensor` to compute the gradients.\n", "* `dist`: a `tfd.Distribution` representing $p(x')$.\n", "* `n_samples`: number of samples to use for the Monte Carlo approximation and the trapezoidal rule.\n", "* `rep_f`: a `tf.function` representing $g(x', \\theta)$.\n", "\n", "> **Note**: your implementation must use `tf.GradientTape` and you must use pure `tensorflow` code."]}, {"cell_type": "code", "execution_count": null, "id": "3953d509", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def gradients_reparameterization(\n", "        f, theta, dist, n_samples, rep_f\n", "        ):\n", "    ..."]}, {"cell_type": "markdown", "id": "ad934369", "metadata": {}, "source": ["Let's compute the gradients using the following values:"]}, {"cell_type": "code", "execution_count": null, "id": "20247869", "metadata": {}, "outputs": [], "source": ["@tf.function\n", "def function(x, theta):\n", "    return tf.exp(theta * tf.abs(x))\n", "\n", "@tf.function\n", "def rep_f(x, theta):\n", "    return x + theta\n", "\n", "dist = tfd.Normal(loc=0, scale=1)\n", "theta_0 = tf.Variable(-2.0)\n", "n_samples = 100_000\n", "integration_range = (-100, 100)"]}, {"cell_type": "markdown", "id": "1dead101", "metadata": {}, "source": ["The following cell must return values similar to:\n", "\n", "```python\n", "\u2771 print(gradients_rep)\n", "tf.Tensor(0.16392285, shape=(), dtype=float32)\n", "\u2771 print(gradients_dep)\n", "tf.Tensor(0.16380094, shape=(), dtype=float32)\n", "```"]}, {"cell_type": "code", "execution_count": null, "id": "16621a89", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["gradients_rep = gradients_reparameterization(\n", "        function, theta_0, dist, n_samples, rep_f\n", "        )\n", "gradients_dep = gradients_dependence(\n", "        function, theta_0, dist_function,\n", "        n_samples, integration_range\n", "        )\n", "print(gradients_rep)\n", "print(gradients_dep)"]}, {"cell_type": "markdown", "id": "d346edae", "metadata": {}, "source": ["As you can see, the reparameterization trick allows a simpler approximation of the gradients that only depends on the Monte Carlo method and doesn't require the trapezoidal rule. This allows probabilistic distributions inside of neural networks while allowing backpropagation."]}, {"cell_type": "code", "execution_count": null, "id": "310da1e8", "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T5\");"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}