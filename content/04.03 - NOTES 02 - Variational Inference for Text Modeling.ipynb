{"cells": [{"cell_type": "code", "execution_count": null, "id": "2a91621c", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.dev.git > /dev/null 2> /dev/null\n", "!mv -n ppdl.dev/content/init.py ppdl.dev/content/local . 2> /dev/null\n", "!pip install -r ppdl.dev/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "92839113", "metadata": {}, "source": ["# Variational Inference for Text Modeling\n", "\n", "In this notebook, we will implement the variational inference algorithm for topic modeling using a neural network and Latent Dirichlet Allocation (LDA). We'll review the bayesian concepts behind LDA, and its implementation using tensorflow probability.\n", "\n", "First, let us import the necessary libraries."]}, {"cell_type": "code", "execution_count": null, "id": "5735d9ad", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "import tensorflow_probability as tfp\n", "from collections import Counter\n", "\n", "tfd = tfp.distributions\n", "tfb = tfp.bijectors\n", "plt.style.use(\"ggplot\")"]}, {"cell_type": "markdown", "id": "11d8f3ee", "metadata": {}, "source": ["## Distributions in a Corpus of Documents\n", "\n", "Let us consider the following simple corpus of documents:"]}, {"cell_type": "code", "execution_count": null, "id": "a996b362", "metadata": {}, "outputs": [], "source": ["corpus = [\n", "    \"the cat sat on the mat the mat was on the cat the cat was on the mat\",\n", "    \"the dog sat on the mat the mat was on the dog the dog was on the mat\",\n", "    \"when the cat was on the roof the roof was on the cat the cat was on the roof\",\n", "    \"the bird sang a song the song sang the bird the bird sang a song\"\n", "    ]"]}, {"cell_type": "markdown", "id": "a3a21d95", "metadata": {}, "source": ["\n", "First, we'll compute the words distribution for a the first document in the corpus.\n", "\n", "Let us tokenize the document into words by splitting on the space characters:"]}, {"cell_type": "code", "execution_count": null, "id": "4b102838", "metadata": {}, "outputs": [], "source": ["words = corpus[0].split()\n", "print(words)"]}, {"cell_type": "markdown", "id": "a8fdb016", "metadata": {}, "source": ["Next, we'll compute words occurences in the document and the vocabulary:"]}, {"cell_type": "code", "execution_count": null, "id": "25382a1a", "metadata": {}, "outputs": [], "source": ["counts = Counter(words)\n", "vocab = np.array(list(counts.keys()))\n", "print(vocab)\n", "print(counts)"]}, {"cell_type": "markdown", "id": "047018f4", "metadata": {}, "source": ["Finally, we'll estimate the parameters of the multinomial distribution for this document:"]}, {"cell_type": "code", "execution_count": null, "id": "d2475d9c", "metadata": {}, "outputs": [], "source": ["dist_params = tf.Variable(list(counts.values()))\n", "dist_params = dist_params / tf.reduce_sum(dist_params)\n", "\n", "document_dist = tfd.Multinomial(\n", "        total_count=float(len(vocab)),\n", "        probs=dist_params,\n", "        )\n", "print(document_dist)"]}, {"cell_type": "markdown", "id": "b15b5f72", "metadata": {}, "source": ["Using this distribution, we can sample documents counts with the counts distribution of the first document:"]}, {"cell_type": "code", "execution_count": null, "id": "070679bb", "metadata": {}, "outputs": [], "source": ["sample = document_dist.sample(10)\n", "print(sample)"]}, {"cell_type": "markdown", "id": "b9684811", "metadata": {}, "source": ["We can convert the sample into a list of words, here, We repeat each word in the vocabulary the number of times it appears in the `sample` matrix:"]}, {"cell_type": "code", "execution_count": null, "id": "6453d5ea", "metadata": {}, "outputs": [], "source": ["sample_np = (\n", "        sample\n", "        .numpy()\n", "        .astype(\"int\")\n", "        )\n", "\n", "for sample_id, doc_counts in enumerate(sample_np):\n", "    print(f\"Sample {sample_id}: \", end=\"\")\n", "    for i, count in enumerate(doc_counts):\n", "        if count > 0:\n", "            print(\" \".join([vocab[i]] * count), end=\" \")\n", "    print()"]}, {"cell_type": "markdown", "id": "21353a14", "metadata": {}, "source": ["Now, We'll compute the parameters of the multinomial distribution for each document in the corpus. \n", "\n", "First, we'll compute the vocabulary for the corpus:"]}, {"cell_type": "code", "execution_count": null, "id": "8aab6135", "metadata": {}, "outputs": [], "source": ["vocab = np.unique(\n", "        np.concatenate([\n", "            np.array(doc.split())\n", "            for doc in corpus\n", "            ])\n", "        )\n", "print(vocab)"]}, {"cell_type": "markdown", "id": "e613a31d", "metadata": {}, "source": ["Also, we need a mapping from the vocabulary to the indices of the parameters."]}, {"cell_type": "code", "execution_count": null, "id": "4e35dccf", "metadata": {}, "outputs": [], "source": ["word2idx = {w: i for i, w in enumerate(vocab)}"]}, {"cell_type": "markdown", "id": "4b4068d5", "metadata": {}, "source": ["We can compute the counts for each document:"]}, {"cell_type": "code", "execution_count": null, "id": "ae2be26f", "metadata": {}, "outputs": [], "source": ["words = list(map(lambda doc: doc.split(), corpus))\n", "print(words)"]}, {"cell_type": "code", "execution_count": null, "id": "14d11a6c", "metadata": {}, "outputs": [], "source": ["tokens = [[word2idx[w] for w in doc] for doc in words]\n", "print(tokens)"]}, {"cell_type": "code", "execution_count": null, "id": "6151c707", "metadata": {}, "outputs": [], "source": ["counts = list(map(Counter, tokens))\n", "print(counts)"]}, {"cell_type": "markdown", "id": "89032856", "metadata": {}, "source": ["We can now compute the parameters of the multinomial distribution for each document:"]}, {"cell_type": "code", "execution_count": null, "id": "92f0e4a5", "metadata": {}, "outputs": [], "source": ["params = np.zeros((len(corpus), len(vocab)))\n", "for i, doc in enumerate(counts):\n", "    params[i, list(doc.keys())] = list(doc.values())\n", "params = params / params.sum(axis=1, keepdims=True)\n", "print(params)"]}, {"cell_type": "markdown", "id": "2d8d2547", "metadata": {}, "source": ["The distribution for each document in the corpus:"]}, {"cell_type": "code", "execution_count": null, "id": "127ec455", "metadata": {}, "outputs": [], "source": ["document_dist = tfd.Multinomial(\n", "        total_count=float(len(vocab)),\n", "        probs=params\n", "        )\n", "print(document_dist)"]}, {"cell_type": "markdown", "id": "8ab070eb", "metadata": {}, "source": ["Let us generate two sample corpuses from the distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "99112ba5", "metadata": {}, "outputs": [], "source": ["sample = document_dist.sample(2)\n", "sample_np = (\n", "        sample\n", "        .numpy()\n", "        .astype(\"int\")\n", "        )\n", "print(sample_np)"]}, {"cell_type": "code", "execution_count": null, "id": "09b68f7f", "metadata": {}, "outputs": [], "source": ["for sample_id, sample in enumerate(sample_np):\n", "    print(f\"Sample {sample_id}: \")\n", "    for document_id, doc_counts in enumerate(sample):\n", "        print(f\"\\tDocument {document_id}: \", end=\"\")\n", "        for i, count in enumerate(doc_counts):\n", "            if count > 0:\n", "                print(\" \".join([vocab[i]] * count), end=\" \")\n", "        print()"]}, {"cell_type": "markdown", "id": "a29b4f96", "metadata": {}, "source": ["This approach always generates a corpus with 4 documents that have the same distribution of the original documents. However, there are some questions that enmark some limitations of this approach:\n", "\n", "* What would happen if we had a large corpus?\n", "\n", "> Under this approach, we would have to sample the distribution for each document in the corpus. This would be a very expensive operation.\n", "\n", "* What happens to documents with similar distributions, is it necessary to save equivalent vectors multiple times?\n", "\n", "> We would have to save the same vector multiple times. Which is memory inefficient.\n", "\n", "* Is there any way to generate a corpus with a different number of documents?\n", "\n", "> There is no way to generate a corpus with a different number of documents, since we have positional distributions for a fixed-length corpus.\n", "\n", "To address these problems, we can use a distribution for the parameters of the multinomial distribution (Bayesian modeling), which allows us to: save the parameters for the parameters' distribution only; summarize multiple distributions in one; and to generate corpuses with different number of documents.\n", "\n", "However, what kind of distribution is the parameters' distribution?\n", "\n", "> Let us recap about Bayesian conjugates. First, consider the Bernoulli distribution (special case of the Multinomial distribution), its conjugate is the Beta distribution. We need a multivariate distribution that generalizes the beta distribution and that can be used for the Multinomial's parameters distribution (Dirichlet)."]}, {"cell_type": "markdown", "id": "bd0f0cf2", "metadata": {}, "source": ["## Bayesian Modeling for the Corpus Distribution\n", "\n", "Let us implement the following two-level hierarchical model for the corpus distribution:\n", "\n", "$$\n", "p \\sim Dirichlet(\\alpha)\\\\\n", "x \\sim Multinomial(p)\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "5cd74994", "metadata": {}, "outputs": [], "source": ["corpus_dist = tfd.JointDistributionNamed(\n", "        {\n", "            \"p\": tfd.Dirichlet(\n", "                concentration=tf.Variable(tf.ones(len(vocab)))\n", "                ),\n", "            \"x\": lambda p: tfd.Multinomial(\n", "                total_count=float(len(vocab)),\n", "                probs=p\n", "                )\n", "            }\n", "        )"]}, {"cell_type": "markdown", "id": "7f45be92", "metadata": {}, "source": ["We can generate some samples from this distribution, notice that the `JointDistributionNamed` object generates a sample for each of the levels:"]}, {"cell_type": "code", "execution_count": null, "id": "e87bd67c", "metadata": {}, "outputs": [], "source": ["sample = corpus_dist.sample(10)\n", "print(sample)"]}, {"cell_type": "markdown", "id": "0d4123cb", "metadata": {}, "source": ["Let's view the corpus for this case:"]}, {"cell_type": "code", "execution_count": null, "id": "77f5bb68", "metadata": {}, "outputs": [], "source": ["sample_np = corpus_dist.sample(10)[\"x\"].numpy().astype(\"int\")\n", "for document_id, doc_counts in enumerate(sample_np):\n", "    print(f\"\\tDocument {document_id}: \", end=\"\")\n", "    for i, count in enumerate(doc_counts):\n", "        if count > 0:\n", "            print(\" \".join([vocab[i]] * count), end=\" \")\n", "    print()"]}, {"cell_type": "markdown", "id": "7fc006f2", "metadata": {}, "source": ["We can solve this problem using variational inference, in this matter, We would need a surrogate posterior distribution, whose parameters can be easily estimated with tensorflow's automatic differentiation."]}, {"cell_type": "code", "execution_count": null, "id": "a919eb86", "metadata": {}, "outputs": [], "source": ["surrogate_posterior = tfd.JointDistributionNamedAutoBatched({\n", "    \"p\": tfb.Sigmoid()(\n", "        tfd.Normal(\n", "            loc=tf.Variable(tf.ones(len(vocab))),\n", "            scale=tf.ones(len(vocab))\n", "            )\n", "        )\n", "    })"]}, {"cell_type": "markdown", "id": "3a77c249", "metadata": {}, "source": ["We need the bag-of-words representation of the corpus:"]}, {"cell_type": "code", "execution_count": null, "id": "a459c1b8", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["counts_mat = np.zeros((len(corpus), len(vocab)))\n", "for i, doc in enumerate(counts):\n", "    counts_mat[i, list(doc.keys())] = list(doc.values())\n", "counts_mat = tf.constant(counts_mat, dtype=tf.float32)"]}, {"cell_type": "markdown", "id": "a25cd2f2", "metadata": {}, "source": ["We can now compute the log-likelihood of the corpus distribution:"]}, {"cell_type": "code", "execution_count": null, "id": "1a71b7ba", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def log_prob(p):\n", "    return corpus_dist.log_prob({\"p\": p, \"x\": counts_mat})"]}, {"cell_type": "markdown", "id": "404e3375", "metadata": {}, "source": ["Finally, we solve the problem using variational inference:"]}, {"cell_type": "code", "execution_count": null, "id": "136b3e82", "metadata": {}, "outputs": [], "source": ["optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n", "loss = tfp.vi.fit_surrogate_posterior(\n", "        target_log_prob_fn=log_prob,\n", "        surrogate_posterior=surrogate_posterior,\n", "        optimizer=optimizer,\n", "        num_steps=int(1e3),\n", "        )"]}, {"cell_type": "markdown", "id": "1742b15a", "metadata": {}, "source": ["Let's view a sample of documents:"]}, {"cell_type": "code", "execution_count": null, "id": "e5b0dd89", "metadata": {}, "outputs": [], "source": ["params = surrogate_posterior.sample(10)[\"p\"].numpy()\n", "params = params / params.sum(axis=1, keepdims=True)\n", "dist = tfd.Multinomial(\n", "        total_count=float(len(vocab)),\n", "        probs=params\n", "        )"]}, {"cell_type": "code", "execution_count": null, "id": "9d77a9c1", "metadata": {}, "outputs": [], "source": ["sample_np = dist.sample(4).numpy().astype(\"int\")\n", "for sample_id, sample in enumerate(sample_np):\n", "    print(f\"Sample {sample_id}: \")\n", "    for document_id, doc_counts in enumerate(sample):\n", "        print(f\"\\tDocument {document_id}: \", end=\"\")\n", "        for i, count in enumerate(doc_counts):\n", "            if count > 0:\n", "                print(\" \".join([vocab[i]] * count), end=\" \")\n", "        print()"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}