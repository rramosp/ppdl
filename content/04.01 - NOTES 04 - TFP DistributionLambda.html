
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Distribution Lambda &#8212; Probabilistic Programming for Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/xglobal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/spectre.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/spectre-exp.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/spectre-icons.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-MBFEZ3PF64"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-MBFEZ3PF64');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo-tf-udea-unal.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Probabilistic Programming for Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="outline.html">
   Course outline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="M1-videolist.html">
   1 Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="M2-videolist.html">
   2 TF for Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="M3-videolist.html">
   3 Intuitions on Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="M4-videolist.html">
   4 Tensorflow Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="M5-videolist.html">
   5 Bayesian Modelling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="M6-videolist.html">
   6 Variational Inference
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/04.01 - NOTES 04 - TFP DistributionLambda.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/rramosp/ppdl/blob/main/content/04.01 - NOTES 04 - TFP DistributionLambda.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-distribution-objects-in-keras-models">
   Using distribution objects in Keras models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tfp-layers-variablelayer">
   <code class="docutils literal notranslate">
    <span class="pre">
     tfp.layers.VariableLayer
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tfp-layers-distributionlambda">
   <code class="docutils literal notranslate">
    <span class="pre">
     tfp.layers.DistributionLambda
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-a-distribution-for-each-input-data-point">
   Learning a distribution for each input data point
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-distribution-parameters">
   Learning distribution parameters
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Distribution Lambda</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-distribution-objects-in-keras-models">
   Using distribution objects in Keras models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tfp-layers-variablelayer">
   <code class="docutils literal notranslate">
    <span class="pre">
     tfp.layers.VariableLayer
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tfp-layers-distributionlambda">
   <code class="docutils literal notranslate">
    <span class="pre">
     tfp.layers.DistributionLambda
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-a-distribution-for-each-input-data-point">
   Learning a distribution for each input data point
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-distribution-parameters">
   Learning distribution parameters
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># init repo notebook
!git clone https://github.com/rramosp/ppdl.git &gt; /dev/null 2&gt; /dev/null
!mv -n ppdl/content/init.py ppdl/content/local . 2&gt; /dev/null
!pip install -r ppdl/content/requirements.txt &gt; /dev/null
</pre></div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="distribution-lambda">
<h1>Distribution Lambda<a class="headerlink" href="#distribution-lambda" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.integrate import quad
from progressbar import progressbar as pbar
from rlxutils import subplots, copy_func

import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions
tfb = tfp.bijectors

%matplotlib inline
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-20 07:56:50.326703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-02-20 07:56:50.326739: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;-1&quot;
</pre></div>
</div>
</div>
</div>
<div class="section" id="using-distribution-objects-in-keras-models">
<h2>Using distribution objects in Keras models<a class="headerlink" href="#using-distribution-objects-in-keras-models" title="Permalink to this headline">¶</a></h2>
<p>TFP provides several layer objects so that we can include distributions in Keras models. This is useful because this would allow us to <em>learn</em> distribution parameters with the regular model machinery of Keras, such as using <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layers, transforming data, etc.</p>
</div>
<div class="section" id="tfp-layers-variablelayer">
<h2><code class="docutils literal notranslate"><span class="pre">tfp.layers.VariableLayer</span></code><a class="headerlink" href="#tfp-layers-variablelayer" title="Permalink to this headline">¶</a></h2>
<p>Simply produces a constant value that can be trainable. It is equivalent to a <code class="docutils literal notranslate"><span class="pre">Dense</span></code> Keras layer with the kernel set to zero and only the bias trainable. The output is the same (constant) for any input of any size. It is used to provide values to posterior layers which do not depend on input data (such as paratemeters of distributions).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vl = tfp.layers.VariableLayer(shape=(3,2))
vl
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow_probability.python.layers.variable_input.VariableLayer at 0x7fe472f814c0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = np.random.random(size=(10,4))
vl(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[0., 0.],
       [0., 0.],
       [0., 0.]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<p>the <code class="docutils literal notranslate"><span class="pre">initializer</span></code> defines the initial constant value. See <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers"><code class="docutils literal notranslate"><span class="pre">tf.keras.initializers</span></code></a> and <a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/BlockwiseInitializer"><code class="docutils literal notranslate"><span class="pre">tfp.layers.BlockwiseInitializer</span></code></a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vl = tfp.layers.VariableLayer(shape=(3,2), initializer=&quot;ones&quot;)
vl(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[1., 1.],
       [1., 1.],
       [1., 1.]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vl = tfp.layers.VariableLayer(
            shape=(3,2), 
            initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
     )

vl(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[-1.3975693,  1.2439306],
       [-1.4463489, -1.8343785],
       [ 1.1516911, -0.4883429]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<p>the <code class="docutils literal notranslate"><span class="pre">BlockwiseInitializer</span></code> concatenates other initializers and dimensions must be broadcastable</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vl = tfp.layers.VariableLayer(shape=(3,2), initializer=
    tfp.layers.BlockwiseInitializer([
          &#39;zeros&#39;,
           tf.keras.initializers.Constant(np.log(np.expm1(1.))),  # = 0.541325
        ], 
        sizes=[1, 1])
    )

vl(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[0.        , 0.54132485],
       [0.        , 0.54132485],
       [0.        , 0.54132485]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tfp-layers-distributionlambda">
<h2><code class="docutils literal notranslate"><span class="pre">tfp.layers.DistributionLambda</span></code><a class="headerlink" href="#tfp-layers-distributionlambda" title="Permalink to this headline">¶</a></h2>
<p>They provide a wrapper for <code class="docutils literal notranslate"><span class="pre">tfp</span></code> distribution objects so that they can be integrated in Keras models. Observe that <strong>their parameter is a function</strong>, so that it can be invoked by Keras within a <code class="docutils literal notranslate"><span class="pre">GradientTape</span></code>.</p>
<p>We can <strong>choose</strong> to use variables for distribution the distribution parameters, and use the input data <strong>ONLY</strong> to evaluate the loss.</p>
<p>Observe we use a <code class="docutils literal notranslate"><span class="pre">VariableLayer</span></code> to <strong>create</strong> a variable in the middle of the model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = 5*tfd.Beta(2.,5).sample(10000)+2
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>inp = tf.keras.layers.Input(shape=(1,))
var = tfp.layers.VariableLayer(shape=(2), initializer=&quot;ones&quot;)(inp)
out = tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[0], scale=tf.math.softplus(t[1])))(var)

m   = tf.keras.models.Model(inp, out)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-20 07:57:04.730721: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m.weights
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;tf.Variable &#39;constant:0&#39; shape=(2,) dtype=float32, numpy=array([1., 1.], dtype=float32)&gt;]
</pre></div>
</div>
</div>
</div>
<p>we get always the <strong>same distribution</strong> regardless the input, in this case with mean and <code class="docutils literal notranslate"><span class="pre">softplus</span></code> std as initialized in the <code class="docutils literal notranslate"><span class="pre">VariableLayer</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_x = m([np.random.random()]).sample(100000).numpy()
_x.mean(), _x.std(), tf.math.softplus(1.).numpy()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1.0002679, 1.3119836, 1.3132616)
</pre></div>
</div>
</div>
</div>
<p>the loss function takes the <strong>target</strong> and the model output. <strong>NOTICE</strong> that the model output is a <strong>distribution object</strong>, so we can use its methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>negloglik = lambda x, distribution: -distribution.log_prob(x)
m.compile(optimizer=&#39;adam&#39;, loss=negloglik)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>negloglik(x, tfd.Normal(0,1))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(10000,), dtype=float32, numpy=
array([4.533191 , 5.8602905, 8.682795 , ..., 5.0683117, 6.6048856,
       4.1267476], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<p>the model does not use the input, we only to provide the <strong>target</strong> to the <code class="docutils literal notranslate"><span class="pre">.fit</span></code> so that it can pass it on  to the loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dummy_input = np.random.random(len(x))
m.fit(dummy_input, x, epochs=40, verbose=0)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;keras.callbacks.History at 0x7fe472d58af0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># the weights are in the variables of the VariableLayer
m.weights
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;tf.Variable &#39;constant:0&#39; shape=(2,) dtype=float32, numpy=array([3.4179287 , 0.20450164], dtype=float32)&gt;]
</pre></div>
</div>
</div>
</div>
<p>This is a <strong>very particular</strong> setting, since <strong>the output DOES NOT depend on the input</strong>, it is just a distribution whose parameters have been previously learnt. The target is <strong>only used in the loss function</strong>. Once training is complete the loss function is not needed anymore.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>distribution = m([np.random.random()])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>xr = np.linspace(np.min(x), np.max(x), 100)
plt.hist(x.numpy(), bins=50, density=True, alpha=.5);
plt.plot(xr, np.exp(distribution.log_prob(xr).numpy()), color=&quot;black&quot;, label=&quot;fitted normal&quot;);
plt.axvline(np.mean(x), ls=&quot;--&quot;, color=&quot;black&quot;, alpha=.5, label=&quot;sample mean&quot;)
plt.grid(); plt.legend();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_26_0.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_26_0.png" />
</div>
</div>
</div>
<div class="section" id="learning-a-distribution-for-each-input-data-point">
<h2>Learning a distribution for each input data point<a class="headerlink" href="#learning-a-distribution-for-each-input-data-point" title="Permalink to this headline">¶</a></h2>
<p>The setting above seems a bit unnatural for Keras. In fact, the full power of TFP unveils when we <strong>learn</strong> a distribution for each input data point.</p>
<p>More in depth observe that:</p>
<ul class="simple">
<li><p>The output of the layer <strong>when using the implicit <code class="docutils literal notranslate"><span class="pre">call</span></code></strong> method of the layer is a distribution.</p></li>
<li><p>The output of the layer <strong>when using the <code class="docutils literal notranslate"><span class="pre">predict</span></code></strong> method of a model is a sample from the distribution.</p></li>
<li><p>It can use the output of previous layers as parameters for the distribution</p></li>
</ul>
<p>For comparison consider a model with a regular keras model, where <code class="docutils literal notranslate"><span class="pre">call</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> have the same behaviour.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>inp  = tf.keras.layers.Input(shape=(1,))
out  = tf.keras.layers.Dense(2)(inp)
m    = tf.keras.models.Model(inp, out)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = np.r_[1.,2.,3.].reshape(-1,1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># using the implicit call method of the model
m(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[-1.0067325,  1.0000728],
       [-2.013465 ,  2.0001457],
       [-3.0201974,  3.0002184]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># using the implicit call method of the layer
m.layers[1](x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[-1.0067325,  1.0000728],
       [-2.013465 ,  2.0001457],
       [-3.0201974,  3.0002184]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># using the predict method of the model
m.predict(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-1.0067325,  1.0000728],
       [-2.013465 ,  2.0001457],
       [-3.0201974,  3.0002184]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Now with a <code class="docutils literal notranslate"><span class="pre">DistributionLambda</span></code> layer. Observe that <strong>we choose</strong> to use the output of the previous layer to be  the mean (first column) and std (second column) of the distribution</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>inp = tf.keras.layers.Input(shape=(2,))
out = tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[:,0], scale=t[:,1]))(inp)

m   = tf.keras.models.Model(inp, out)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = np.r_[1.,2.,10.,4.,20.,6.].reshape(3,2)
x
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 1.,  2.],
       [10.,  4.],
       [20.,  6.]])
</pre></div>
</div>
</div>
</div>
<p>we implicitly use the <code class="docutils literal notranslate"><span class="pre">call</span></code> method of our model. Observe we get <strong>one distribution for each input data</strong>, which shows up in the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tfp.distributions._TensorCoercible &#39;tensor_coercible&#39; batch_shape=[3] event_shape=[] dtype=float32&gt;
</pre></div>
</div>
</div>
</div>
<p>which is the same as if we call directly the <code class="docutils literal notranslate"><span class="pre">DistributionLambda</span></code> layer object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m.layers[1](x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tfp.distributions._TensorCoercible &#39;tensor_coercible&#39; batch_shape=[3] event_shape=[] dtype=float32&gt;
</pre></div>
</div>
</div>
</div>
<p>we can sample this output distribution. Each sample will contain three elements, one per data point following a normal distribution parametrized by the two elements of the data point (mean and std).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_x = m(x).sample(100000).numpy()
_x.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(100000, 3)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_x.mean(axis=0), _x.std(axis=0)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([ 1.004689,  9.983776, 19.96568 ], dtype=float32),
 array([2.003365 , 3.9917862, 5.986713 ], dtype=float32))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for i in range(len(x)):
    plt.hist(_x[:,i], density=True, bins=100, alpha=.5)
plt.grid();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_44_0.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_44_0.png" />
</div>
</div>
<p>In this case, the <code class="docutils literal notranslate"><span class="pre">.predict</span></code> method <strong>behaves differently</strong> from the implicit <code class="docutils literal notranslate"><span class="pre">call</span></code>. It produces a sample following the input data.</p>
<p>This behaviour can be changed by the <code class="docutils literal notranslate"><span class="pre">convert_to_tensor_fn</span></code> argument. See the <a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda">DistributionLambda</a> docs. So that you could return the distribution mean, or any other representative value you want to have as a result of a predictions. Sampling from the distribution corresponding to each input data point is a generally convenient choice for generative models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># the predictions (samples) follow the distributions parametrized by x
x = np.concatenate([np.r_[[[0,1]]*40000], np.r_[[[10,3]]*50000]])
x.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(90000, 2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_x = m.predict(x)
_x.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(90000,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_x[:40000].mean(), _x[:40000].std(), &quot;::&quot;, _x[40000:].mean(), _x[40000:].std()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.0049270834, 1.0014801, &#39;::&#39;, 9.986944, 3.0177207)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.hist(_x[:40000], alpha=.5, density=True, bins=100)
plt.hist(_x[40000:], alpha=.5, density=True, bins=100);
plt.grid();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_49_0.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_49_0.png" />
</div>
</div>
</div>
<div class="section" id="learning-distribution-parameters">
<h2>Learning distribution parameters<a class="headerlink" href="#learning-distribution-parameters" title="Permalink to this headline">¶</a></h2>
<p>We have not fit any model yet, but we can use this machinery to <strong>learn</strong> the distribution parameters for each input data point with a small <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer.</p>
<p>We are trying to learn this model</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mu_x, \sigma_x &amp;= D_\theta(x) \\
y &amp;\sim \mathcal{N}(\mu_x, \sigma_x)
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(D_\theta\)</span> is a regular neural network with parameters <span class="math notranslate nohighlight">\(\theta\)</span> which, given an input <span class="math notranslate nohighlight">\(x\)</span>, returns two values <span class="math notranslate nohighlight">\(\mu_x\)</span> and <span class="math notranslate nohighlight">\(\sigma_x\)</span>. These two values will be used to parametrize a gaussian distribution for that specific <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>and we want <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes the log likelihood.</p>
<div class="math notranslate nohighlight">
\[\underset{\theta}{\text{arg min}}\;\; - \frac{1}{N} \sum_{x_i} \log P_{\mathcal{N}(\mu_{x_i}, \sigma_{x_i})}(x_i)\]</div>
<p>Observe that, this way, we have a <strong>predictive distribution</strong> for each input data point.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n_points = 300
k = 2
x = np.linspace(0, 10, n_points)
y = .3*x+2*np.sin(x/1.5) + (.2+.02* ((x-5)*k)**2) * np.random.randn(n_points)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.scatter(x, y, s=10)
plt.grid();
plt.xlabel(&quot;x&quot;); plt.ylabel(&quot;y&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;y&#39;)
</pre></div>
</div>
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_52_1.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_52_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>inp  = tf.keras.layers.Input(shape=(1,))
out  = tf.keras.layers.Dense(10, activation=&quot;tanh&quot;)(inp)
out  = tf.keras.layers.Dense(10, activation=&quot;tanh&quot;)(out)
out  = tf.keras.layers.Dense(2)(out)
out  = tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[:,0], scale=tf.math.softplus(t[:,1])))(out)

m    = tf.keras.models.Model(inp, out)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m.summary()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_4&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1)]               0         
                                                                 
 dense_2 (Dense)             (None, 10)                20        
                                                                 
 dense_3 (Dense)             (None, 10)                110       
                                                                 
 dense_4 (Dense)             (None, 2)                 22        
                                                                 
 distribution_lambda_2 (Dist  ((None,),                0         
 ributionLambda)              (None,))                           
                                                                 
=================================================================
Total params: 152
Trainable params: 152
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>negloglik = lambda x, distribution: -distribution.log_prob(x)
m.compile(optimizer=&#39;adam&#39;, loss=negloglik)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>history = m.fit(x,y, epochs=2000, verbose=0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.plot(history.epoch, history.history[&#39;loss&#39;])
plt.grid(); plt.xlabel(&quot;epoch&quot;); plt.ylabel(&quot;loss&quot;);
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_57_0.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_57_0.png" />
</div>
</div>
<p>get the predictive distributions for each input data point</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_y = m(x)
_y
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tfp.distributions._TensorCoercible &#39;tensor_coercible&#39; batch_shape=[300] event_shape=[] dtype=float32&gt;
</pre></div>
</div>
</div>
</div>
<p>observe the parameters learn for each datapoint</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_x = np.r_[2]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.scatter(x, y, s=10, color=&quot;steelblue&quot;)
plt.plot(x, _y.parameters[&#39;loc&#39;], color=&quot;black&quot;, lw=2, label=&quot;prediction mean&quot;)
plt.fill_between(x, 
                 _y.parameters[&#39;loc&#39;] + 2*_y.parameters[&#39;scale&#39;], 
                 _y.parameters[&#39;loc&#39;] - 2*_y.parameters[&#39;scale&#39;], alpha=.3, 
                 color=&quot;steelblue&quot;,
                 label=&quot;prediction std (x2)&quot;)
plt.axvline(_x[0], color=&quot;black&quot;, ls=&quot;--&quot;, alpha=.5, label=&quot;a selected data point&quot;)
plt.grid(); plt.legend();
plt.xlabel(&quot;x&quot;); plt.ylabel(&quot;y&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;y&#39;)
</pre></div>
</div>
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_62_1.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_62_1.png" />
</div>
</div>
<p>we can also see how it generlizes outside the input variable bounds</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>xr = np.linspace(np.min(x)-10, np.max(x)+10, 100)
yr = m(xr)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.figure(figsize=(10,4))
plt.scatter(x, y, s=10, color=&quot;steelblue&quot;)
plt.plot(xr, yr.parameters[&#39;loc&#39;], color=&quot;black&quot;, lw=2, label=&quot;prediction mean&quot;)
plt.fill_between(xr, 
                 yr.parameters[&#39;loc&#39;] + 2*yr.parameters[&#39;scale&#39;], 
                 yr.parameters[&#39;loc&#39;] - 2*yr.parameters[&#39;scale&#39;], alpha=.3, 
                 color=&quot;steelblue&quot;,
                 label=&quot;prediction std (x2)&quot;)
plt.axvline(_x[0], color=&quot;black&quot;, ls=&quot;--&quot;, alpha=.5, label=&quot;a selected data point&quot;)
plt.grid(); plt.legend();
plt.xlabel(&quot;x&quot;); plt.ylabel(&quot;y&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;y&#39;)
</pre></div>
</div>
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_65_1.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_65_1.png" />
</div>
</div>
<p>we can get the <strong>predictive distribution</strong> of any input data point (such as the selected one above)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_xd = m(_x)
_xd
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tfp.distributions._TensorCoercible &#39;tensor_coercible&#39; batch_shape=[1] event_shape=[] dtype=float32&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_ys = _xd.sample(10000)[:,0].numpy()
_yr = np.linspace(np.min(_ys), np.max(_ys), 100)
plt.plot(_yr, np.exp(_xd.log_prob(_yr)), color=&quot;black&quot;, alpha=1)
plt.hist(_ys, bins=100, density=True,  alpha=.5);
plt.axvline(_ys.mean(), color=&quot;black&quot;, lw=3)
plt.grid(); plt.xlabel(f&quot;y distribution for x={_x[0]}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;y distribution for x=2&#39;)
</pre></div>
</div>
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_68_1.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_68_1.png" />
</div>
</div>
<p>and, since we have distributions we also have a generative model using the <code class="docutils literal notranslate"><span class="pre">.predict</span></code> method for sampling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_x = np.random.random(5000)*14-2
_y = m.predict(_x)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.scatter(_x, _y, s=10, alpha=.1, color=&quot;red&quot;,  label=&quot;generated data&quot;);
plt.scatter(x, y, s=10, alpha=1, color=&quot;black&quot;, label=&quot;original data&quot;)
plt.grid(); plt.legend();
plt.xlabel(&quot;x&quot;); plt.ylabel(&quot;y&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;y&#39;)
</pre></div>
</div>
<img alt="../_images/04.01 - NOTES 04 - TFP DistributionLambda_71_1.png" src="../_images/04.01 - NOTES 04 - TFP DistributionLambda_71_1.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "p39"
        },
        kernelOptions: {
            kernelName: "p39",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'p39'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Raúl Ramos / Universidad de Antioquia, Fabio González / Universidad Nacional de Colombia<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>