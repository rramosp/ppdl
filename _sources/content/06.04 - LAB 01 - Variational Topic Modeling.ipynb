{"cells": [{"cell_type": "code", "execution_count": null, "id": "2e46977b", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "45217fb9", "metadata": {}, "source": ["# Lab 04.03.2: Variational Neural Topic Modeling "]}, {"cell_type": "code", "execution_count": null, "id": "8a02faba", "metadata": {}, "outputs": [], "source": ["## Ignore this cell\n", "!pip install ppdl==0.1.5 rlxmoocapi==0.1.0 --quiet"]}, {"cell_type": "code", "execution_count": null, "id": "0848550b", "metadata": {}, "outputs": [], "source": ["import inspect\n", "import nltk, re\n", "import tensorflow as tf\n", "import tensorflow_probability as tfp\n", "from rlxmoocapi import submit, session\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras.layers import Dense, Input, Layer\n", "from tensorflow.keras.initializers import GlorotNormal\n", "from sklearn.datasets import fetch_20newsgroups\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from nltk.corpus import stopwords\n", "from tqdm import tqdm\n", "\n", "tfd = tfp.distributions\n", "tfb = tfp.bijectors\n", "tfpl = tfp.layers\n", "nltk.download(\"popular\")\n", "\n", "course_id = \"ppdl.v1\"\n", "endpoint = \"https://m5knaekxo6.execute-api.us-west-2.amazonaws.com/dev-v0001/rlxmooc\"\n", "lab = \"L04.03.01\""]}, {"cell_type": "markdown", "id": "028c04b7", "metadata": {}, "source": ["Log-in with your username and password:"]}, {"cell_type": "code", "execution_count": null, "id": "ad9c56f4", "metadata": {}, "outputs": [], "source": ["session.LoginSequence(\n", "    endpoint=endpoint,\n", "    course_id=course_id,\n", "    lab_id=lab,\n", "    varname=\"student\"\n", "    );"]}, {"cell_type": "markdown", "id": "f57494e7", "metadata": {}, "source": ["## Topic Models\n", "In this lab, we will use a neural network for variational topic modeling. First, let us introduce a general topic model:\n", "\n", "![Topic model](https://raw.githubusercontent.com/rramosp/ppdl/main/content/local/imgs/topic_model.png)\n", "\n", "Where:\n", "\n", "* $N$ is the number of documents in the corpus.\n", "* $V$ is the vocabulary size.\n", "* $K$ is the number of topics.\n", "* $P(V=v_j|D=d_i)$ is the probability of word $v_j$ in document $d_i$ (a Bag-of-Words representation).\n", "* $P(K=k|D=d_i)$ is the probability of topic $k$ in document $d_i$.\n", "* $P(V=v_j|K=k)$ is the probability of document $d_i$ belonging to topic $k$.\n", "\n", "In this case, we will use a probabilistic encoder-decoder neural network to approximate $P(K=k|D=d_i)$ and $P(V=v_j|K=k)$.\n", "\n", "First, let us load the 20 newsgroups dataset:"]}, {"cell_type": "code", "execution_count": null, "id": "d7d717a0", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["newsgroups = fetch_20newsgroups(subset=\"test\")\n", "corpus, labels = newsgroups.data, newsgroups.target"]}, {"cell_type": "markdown", "id": "f5277b00", "metadata": {}, "source": ["Let us preprocess the data:"]}, {"cell_type": "code", "execution_count": null, "id": "23943372", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def preprocess_doc(doc):\n", "    \"\"\"\n", "    preprocess a document.\n", "    \"\"\"\n", "    lower_doc = doc.lower()\n", "    clean_doc = re.sub(r\"[^a-z]\", \" \", lower_doc)\n", "    clean_doc = re.sub(r\"\\s+\", \" \", clean_doc)\n", "    tokens = clean_doc.split(\" \")\n", "    sw = stopwords.words(\"english\")\n", "    filtered_tokens = filter(\n", "            lambda token: token not in sw,\n", "            tokens\n", "            )\n", "\n", "    return \" \".join(filtered_tokens)"]}, {"cell_type": "code", "execution_count": null, "id": "c84ac3ed", "metadata": {}, "outputs": [], "source": ["preprocessed_corpus = list(map(preprocess_doc, tqdm(corpus)))\n", "print(preprocessed_corpus[:5])"]}, {"cell_type": "markdown", "id": "cac08f9e", "metadata": {}, "source": ["The BoW representation of the documents is a matrix of size $N \\times V$:"]}, {"cell_type": "code", "execution_count": null, "id": "d473583d", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["bow = (\n", "        CountVectorizer(min_df=50)\n", "        .fit(preprocessed_corpus)\n", "        )\n", "X = (\n", "        bow\n", "        .transform(preprocessed_corpus)\n", "        .toarray()\n", "        )\n", "vocab = bow.get_feature_names_out()\n", "print(X.shape)"]}, {"cell_type": "markdown", "id": "30b79f57", "metadata": {}, "source": ["## Task 1:\n", "\n", "Implement the `Encoder` class that takes the BoW of a document ($P(V=v_j|D=d_i)$) as input and outputs a probability distribution over topics ($P(K=k|D=d_i)$), you must:\n", "\n", "* Implement the constructor, adding the `Dense` layers that you will need.\n", "* Implement the `call` method to connect the input and the layers, and return the output of the last layer.\n", "* The last layer must be a `Dense` layer with a `clipped_softplus` activation with `n_topics` units.\n", "* Use the `GlorotNormal` initializer for the weights."]}, {"cell_type": "code", "execution_count": null, "id": "08841d9d", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def build_encoder():\n", "    def clipped_softplus(x):\n", "        return tf.clip_by_value(tf.nn.softplus(x), .1, 1e3)\n", "\n", "    class Encoder(Model):\n", "        def __init__(\n", "                self,\n", "                n_topics,\n", "                hidden_layers,\n", "                activation,\n", "                *args, **kwargs\n", "                ):\n", "            super(Encoder, self).__init__(*args, **kwargs)\n", "            # YOUR CODE HERE\n", "            self.hidden_layers = ...\n", "            self.latent_layer = ...\n", "\n", "        @tf.function\n", "        def call(self, inputs):\n", "            # YOUR CODE HERE\n", "            ...\n", "    return Encoder"]}, {"cell_type": "markdown", "id": "b8b05569", "metadata": {}, "source": ["## Task 2\n", "\n", "Implement the `DecodingLayer`, it will receive the latent representation and return the reconstructed input, the layer should implement the following opperation:\n", "\n", "$$\n", "f(\\mathbf{L}, \\mathbf{W}) = \\mathbf{L} \\cdot \\text{softmax}(\\mathbf{W})\n", "$$\n", "\n", "Where $\\mathbf{L}$ is the latent representation (output of the encoder) and $\\mathbf{W}$ are the parameters of the layer."]}, {"cell_type": "code", "execution_count": null, "id": "45e1c2fb", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def build_decoder():\n", "    class DecodingLayer(Layer):\n", "        def __init__(self, n_topics, vocab_size, *args, **kwargs):\n", "            super(DecodingLayer, self).__init__(*args, **kwargs)\n", "            # YOUR CODE HERE\n", "            self.params = ...\n", "\n", "        @tf.function\n", "        def call(self, topics):\n", "            # YOUR CODE HERE\n", "            ...\n", "\n", "        def get_topic_words(self):\n", "            # This function must implement the following operation:\n", "            # softmax(W)\n", "            ...\n", "    return DecodingLayer"]}, {"cell_type": "code", "execution_count": null, "id": "ef292fb2", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["source_functions = [\"build_decoder\"]\n", "source_variables = []\n", "res = teacher.run_grader_locally(\n", "        \"grader2\", source_functions,\n", "        source_variables, locals()\n", "        )\n", "print(res.data)"]}, {"cell_type": "markdown", "id": "ecb1db34", "metadata": {}, "source": ["The `prior` function creates the prior distribution for the topics, We'll use this for variational inference."]}, {"cell_type": "code", "execution_count": null, "id": "6d4d43cf", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def prior(n_topics, init_val):\n", "    concentration = tf.fill([1, n_topics], init_val)\n", "    concentration = tfb.Softplus()(concentration)\n", "    return tfd.Dirichlet(concentration=concentration)"]}, {"cell_type": "code", "execution_count": null, "id": "76b18f19", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["prior_dist = prior(n_topics=5, init_val=1.0)\n", "print(prior_dist)"]}, {"cell_type": "markdown", "id": "42e7805c", "metadata": {}, "source": ["## Task 3\n", "\n", "The following class implements the variational neural topic model, you must implement the `NeuralTopicModel`, such that:\n", "\n", "* Initialize the `Encoder` and `DecodingLayer` with the correct hyperparameters, using the `build_encoder` and `build_decoder` functions.\n", "* Initialize a `DistributionLambda` layer for the topics using the dirichlet distribution, and using sampling as the `convert_to_tensor_fn` function.\n", "\n", "The model must implement the following operation:\n", "\n", "```\n", "output = decoder(dirichlet(encoder(input)))\n", "```"]}, {"cell_type": "code", "execution_count": null, "id": "2eef5fa9", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def build_full_model():\n", "    class NeuralTopicModel(Model):\n", "        def __init__(\n", "                self,\n", "                prior_dist,\n", "                neg_elbo,\n", "                n_topics=20,\n", "                hidden_layers=(256, 256),\n", "                activation=\"relu\",\n", "                vocab_size=10000,\n", "                *args, **kwargs\n", "                ):\n", "            super(NeuralTopicModel, self).__init__(*args, **kwargs)\n", "            self.prior_dist = prior_dist\n", "            self.neg_elbo = neg_elbo\n", "            # YOUR CODE HERE\n", "\n", "        def call(self, inputs):\n", "            # YOUR CODE HERE\n", "            encoded = ...\n", "            decoded = ...\n", "            self.add_loss(self.neg_elbo(inputs, encoded, decoded, self.prior_dist))\n", "            return decoded\n", "    return NeuralTopicModel"]}, {"cell_type": "markdown", "id": "2e5f51ee", "metadata": {}, "source": ["## Task 4\n", "\n", "Implement the following loss function:\n", "\n", "```\n", "neg_elbo(X) = mean(log_prob(X) - kl(prior || topics_posterior(X)))\n", "```\n", "\n", "Where:\n", "\n", "* `inputs`: is the input BoW.\n", "* `encoded`: output of the encoder model, represents the parameters of a Dirichlet distribution for the topics.\n", "* `decoded` output of the decoder model, represents the parameters of a OneHotCategorical distribution for the reconstruction\n", "* `prior` is the prior distribution for the topics."]}, {"cell_type": "code", "execution_count": null, "id": "24a6985d", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def neg_elbo(inputs, encoded, decoded, prior_dist):\n", "    # YOUR CODE HERE\n", "    return ..."]}, {"cell_type": "code", "execution_count": null, "id": "fa61bc6f", "metadata": {}, "outputs": [], "source": ["source_functions = [\"neg_elbo\"]\n", "source_variables = []\n", "res = teacher.run_grader_locally(\n", "        \"grader4\", source_functions,\n", "        source_variables, locals()\n", "        )\n", "print(res.data)"]}, {"cell_type": "markdown", "id": "033c1881", "metadata": {}, "source": ["Let us train the model"]}, {"cell_type": "code", "execution_count": null, "id": "1f7e7008", "metadata": {}, "outputs": [], "source": ["# hyperparameters\n", "N_TOPICS = 20\n", "HIDDEN_LAYERS = (256, 256)\n", "ACTIVATION = \"relu\"\n", "prior_dist = prior(n_topics=N_TOPICS, init_val=2.0)"]}, {"cell_type": "code", "execution_count": null, "id": "be1db0b5", "metadata": {}, "outputs": [], "source": ["input = tf.keras.layers.Input(shape=(len(vocab),))\n", "neural_topic = build_full_model()(\n", "        neg_elbo=neg_elbo,\n", "        prior_dist=prior_dist,\n", "        n_topics=N_TOPICS,\n", "        hidden_layers=HIDDEN_LAYERS,\n", "        activation=ACTIVATION,\n", "        vocab_size=len(vocab)\n", "        )(input)\n", "model = Model(inputs=input, outputs=neural_topic)\n", "model.compile(optimizer=\"adam\")"]}, {"cell_type": "code", "execution_count": null, "id": "1a297b0a", "metadata": {}, "outputs": [], "source": ["model.fit(X, epochs=15, batch_size=64)"]}, {"cell_type": "markdown", "id": "69b98ffb", "metadata": {}, "source": ["Finally, let us review the learned distributions.\n", "\n", "* The posterior distribution of the topics:"]}, {"cell_type": "code", "execution_count": null, "id": "c11fea1a", "metadata": {}, "outputs": [], "source": ["topics_posterior = tfd.Dirichlet(model.layers[1].encoder(X))\n", "print(topics_posterior.mean())\n", "print(topics_posterior.stddev())"]}, {"cell_type": "markdown", "id": "b873ad6a", "metadata": {}, "source": ["* The probabilities of the words in the topics:"]}, {"cell_type": "code", "execution_count": null, "id": "606ae80e", "metadata": {}, "outputs": [], "source": ["topics_words = model.layers[1].decoder.get_topic_words()\n", "print(topics_words)"]}, {"cell_type": "markdown", "id": "3bd38d28", "metadata": {}, "source": ["* Finally, We can view the 15 most relevant terms for each topic:"]}, {"cell_type": "code", "execution_count": null, "id": "c0f5c012", "metadata": {}, "outputs": [], "source": ["for i, comp in enumerate(topics_words.numpy()):\n", "    terms_comp = zip(vocab, comp)\n", "    sorted_terms = sorted(\n", "            terms_comp, key= lambda x:x[1],\n", "            reverse=True\n", "            )[:15]\n", "    print(\"Topic {}: {}\".format(\n", "        i, \" \".join(\n", "            map(\n", "                lambda x:x[0], sorted_terms\n", "                )\n", "             )\n", "         ))"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}