{"cells": [{"cell_type": "code", "execution_count": null, "id": "2da00c7d", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "markdown", "id": "b1d6113f", "metadata": {}, "source": ["# LAB 03.02.02 - Similarity and Entropy\n", "\n", "In this laboratory we'll review some commonly used entropy functions and how to interpret them."]}, {"cell_type": "code", "execution_count": null, "id": "0d38f69c", "metadata": {}, "outputs": [], "source": ["import inspect\n", "from rlxmoocapi import submit, session\n", "course_id = \"ppdl.v1\"\n", "endpoint = \"https://m5knaekxo6.execute-api.us-west-2.amazonaws.com/dev-v0001/rlxmooc\"\n", "lab = \"L03.02.02\""]}, {"cell_type": "markdown", "id": "b7ca5094", "metadata": {}, "source": ["Log-in with your username and password:"]}, {"cell_type": "code", "execution_count": null, "id": "83a45a52", "metadata": {}, "outputs": [], "source": ["session.LoginSequence(\n", "    endpoint=endpoint,\n", "    course_id=course_id,\n", "    lab_id=lab,\n", "    varname=\"student\"\n", "    );"]}, {"cell_type": "code", "execution_count": null, "id": "9681d78a", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["from scipy.stats import rv_continuous\n", "from scipy import stats\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use(\"ggplot\")"]}, {"cell_type": "markdown", "id": "99684139", "metadata": {}, "source": ["## Task 1\n", "\n", "Compute the entropy of a continuous distribution by solving the following integral:\n", "\n", "$$\n", "H(P) = \\mathbb{E}_{P}[-\\text{log}(P(x))] = -\\int_x P(x)\\text{log}(P(x)) dx\n", "$$\n", "\n", "To this end, you must do the following numerical approximation to compute the integral:\n", "\n", "\\begin{align}\n", "H(P) &\\approx \\sum_{x} P(x)\\text{log}(P(x)) \\Delta x\\\\\n", "     &\\approx \\sum_i P(x)\\text{log}(P(x)) (x_{i + 1} - x_i)\n", "\\end{align}\n", "\n", "Your function will receive the following arguments:\n", "\n", "* `dist`: a `scipy` distribution.\n", "* `min_x`: minimum range value to compute the integral.\n", "* `max_x`: maximum range value to compute the integral.\n", "* `n_points`: number of points in the grid to compute the integral."]}, {"cell_type": "code", "execution_count": null, "id": "a454e0d3", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def continous_entropy(\n", "        dist: rv_continuous,\n", "        min_x: float,\n", "        max_x: float,\n", "        n_points: int\n", "        ) -> float:\n", "    ..."]}, {"cell_type": "markdown", "id": "6c7af305", "metadata": {}, "source": ["For a normal distribution, We have an analytical solution for the entropy:\n", "\n", "$$\n", "H(\\mathcal{N}) = 0.5 (\\text{log}(2 \\pi \\sigma ^ 2) + 1)\n", "$$\n", "\n", "Therefore, your function must return a similar value."]}, {"cell_type": "code", "execution_count": null, "id": "39fda448", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def continous_entropy_normal(std: float) -> float:\n", "    return 0.5 * (np.log(2 * np.pi * std ** 2) + 1)"]}, {"cell_type": "markdown", "id": "79fe8e90", "metadata": {}, "source": ["You may change the `std` value to verify that both results are equivalent:"]}, {"cell_type": "code", "execution_count": null, "id": "1947acaa", "metadata": {}, "outputs": [], "source": ["std = 2\n", "dist = stats.norm(loc=0, scale=std)\n", "numerical = continous_entropy(dist, -100, 100, 1000) # type: ignore\n", "analytical = continous_entropy_normal(std)\n", "print(f\"Numerical approximation: {numerical}\")\n", "print(f\"Analytical solution: {analytical}\")"]}, {"cell_type": "markdown", "id": "1c5644d8", "metadata": {}, "source": ["Now, we will use another distribution to evaluate the entropy at different levels.\n", "\n", "Your result must be the same to the following figure:\n", "\n", "![laplace entropy](local/imgs/laplacian_entropy.png)\n", "\n", "As you can see, the entropy measures the level of information of the distribution. If the distribution is too informative (converges to a point) there's less entropy, and if the distribution is less informative (several points are more likely) then there's higher entropy."]}, {"cell_type": "code", "execution_count": null, "id": "28288e6c", "metadata": {}, "outputs": [], "source": ["scale = np.linspace(1, 10, 10)\n", "loc = 0\n", "min_x, max_x = -100, 100\n", "n_points = 1000\n", "\n", "*entropies, = map(\n", "        lambda scale: continous_entropy(\n", "            stats.laplace(loc=loc, scale=scale),\n", "            min_x=min_x,\n", "            max_x=max_x,\n", "            n_points=n_points\n", "            ),\n", "        scale\n", "        )\n", "\n", "fig, ax = plt.subplots(2, 5, figsize=(15, 10))\n", "cont = 0\n", "x_range = np.linspace(min_x, max_x, n_points)\n", "\n", "for i in range(2):\n", "    for j in range(5):\n", "        axi = ax[i, j]\n", "        axi.plot(\n", "                x_range,\n", "                stats.laplace(loc=loc, scale=scale[cont]).pdf(x_range)\n", "                )\n", "        axi.set_title(f\"$H(P)={entropies[cont]:.2f}, \\\\sigma={scale[cont]:.2f}$\")\n", "        cont += 1\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "adb30c13", "metadata": {}, "source": ["Use the following cell to evaluate your code:"]}, {"cell_type": "code", "execution_count": null, "id": "717fe223", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T1\");"]}, {"cell_type": "markdown", "id": "4dee74c6", "metadata": {}, "source": ["## Task 2\n", "\n", "In this task you must compute the continuous cross-entropy of two continuous distributions $p$ and $q$:\n", "\n", "\\begin{align}\n", "H_P(Q) = \\underset{P}{\\mathbb{E}}[-\\text{log}Q(x)]\\\\\n", "H_P(Q) = - \\int_x P(x) \\text{log}(Q(x)) dx\n", "\\end{align}\n", "\n", "However, you must compute this using a monte carlo approximation:\n", "\n", "$$\n", "H_P(Q) \\approx - \\frac{1}{N_{samples}} \\sum_{i=1}^{N_{samples}} \\text{log}(Q(x_{sample}))\\\\\n", "x_{sample} \\sim P\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "c4645901", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def cross_entropy(\n", "        p: rv_continuous,\n", "        q: rv_continuous,\n", "        n_samples: int\n", "        ) -> float:\n", "    ..."]}, {"cell_type": "markdown", "id": "93d8ae23", "metadata": {}, "source": ["In the following figure you can see a comparison between a standard normal distribution and different laplacian distributions.\n", "\n", "The result must be as follows:\n", "\n", "![cross entropy laplacian](local/imgs/cross_entropy_laplacian.png)"]}, {"cell_type": "code", "execution_count": null, "id": "1fdbbe03", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(5, 5, figsize=(20, 20))\n", "locs = np.linspace(-5, 5, 5)\n", "scales = np.linspace(1, 5, 5)\n", "\n", "p = stats.norm(loc=0, scale=1)\n", "x_range = np.linspace(-10, 10)\n", "\n", "for i in range(5):\n", "    for j in range(5):\n", "        axi = ax[i, j]\n", "        q = stats.laplace(loc=locs[i], scale=scales[j])\n", "        ce = cross_entropy(p, q, 10_000)\n", "\n", "        axi.plot(x_range, p.pdf(x_range), label=\"$P=N(\\\\mu=0, \\\\sigma=1)$\")\n", "        axi.plot(x_range, q.pdf(x_range), label=f\"$Q=L(\\\\mu={locs[i]}, \\\\sigma={scales[j]})$\")\n", "        axi.set_title(f\"$H_P(Q) = {ce:.2f}$\")\n", "        axi.legend()\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "27b2bf1d", "metadata": {}, "source": ["Use the following cell to evaluate your code:"]}, {"cell_type": "code", "execution_count": null, "id": "8c246259", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T2\");"]}, {"cell_type": "markdown", "id": "51866c56", "metadata": {}, "source": ["## Task 3\n", "\n", "In this task, you must compute the Kullback-Leibler (KL) divergence between two distributions $P$ and $Q$, as follows:\n", "\n", "\\begin{align}\n", "KL(P||Q) &= \\underset{P}{\\mathbb{E}}[\\text{log}(P) - \\text{log}(Q)]\\\\\n", "         &= \\int_x P(x) \\text{log}\\left(\\frac{P(x)}{Q(x)}\\right) dx\n", "\\end{align}\n", "\n", "You must also, perform a numerical approximation using monte carlo:\n", "\n", "$$\n", "KL(P||Q) \\approx \\frac{1}{N_{samples}} \\sum_{i=1} ^ {N_{samples}} \\text{log}\\left(\\frac{P(x_{sample})}{Q(x_{sample})}\\right)\\\\\n", "x_{sample} \\sim P\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "6fc11781", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def kl_divergence(\n", "        p: rv_continuous,\n", "        q: rv_continuous,\n", "        n_samples: int\n", "        ) -> float:\n", "    ..."]}, {"cell_type": "markdown", "id": "988cc9b3", "metadata": {}, "source": ["Now, we'll compute the KL between a standard normal distribution and different Laplace distributions.\n", "\n", "If you run the following cell you must obtain an image similar to the next one:\n", "\n", "![kl laplace](local/imgs/kl_laplace.png)"]}, {"cell_type": "code", "execution_count": null, "id": "caafb732", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(5, 5, figsize=(20, 20))\n", "\n", "locs = np.linspace(-5, 5, 5)\n", "scales = np.linspace(1, 5, 5)\n", "n_samples = 10000\n", "x_range = np.linspace(-10, 10, 100)\n", "\n", "p = stats.norm(loc=0, scale=1)\n", "\n", "for i in range(5):\n", "    for j in range(5):\n", "        axi = ax[i, j]\n", "        q = stats.laplace(loc=locs[i], scale=scales[j])\n", "        kl = kl_divergence(p, q, n_samples)\n", "        axi.plot(x_range, p.pdf(x_range), label=\"$P=N(\\\\mu=0, \\\\sigma=1)$\")\n", "        axi.plot(x_range, q.pdf(x_range), label=f\"$Q=L(\\\\mu={locs[i]}, \\\\sigma={scales[j]})$\")\n", "        axi.set_title(f\"$KL(P||Q) = {kl:.2f}$\")\n", "        axi.legend()\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "4f0004d0", "metadata": {}, "source": ["Use the following cell to evaluate your code:"]}, {"cell_type": "code", "execution_count": null, "id": "7a922ad2", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T3\");"]}, {"cell_type": "markdown", "id": "a0a40e86", "metadata": {}, "source": ["## Task 4\n", "\n", "In this final task, you'll fit a distribution using the KL-divergence. In this problem you must fit the parameters of a beta distribution so that it's similar to any given distribution.\n", "\n", "The following function takes as arguments the target distribution `p`, the initial beta parameters `params0`, and the number of samples `n_samples` used in the montecarlo approximation for the KL. You must return a `scipy` beta distribution with the best parameters.\n", "\n", "You must implement a hill-climbing approach, as follows:\n", "\n", "* You receive an initial set of parameters `params0`.\n", "* For a given number of iterations `n_iters` you must do:\n", "  1. Create a new set of parameters `params1` using the following rule (adding a number generated using a normal distribution to the previous parameters):\n", "\n", "      `params1 = params0 + N(mu=0, sigma=step_size)`\n", "  1. Compute the KL divergence between `p` and a beta distribution with the proposed parameters\n", "  2. If the KL is lower with the new parameters, you must replace `params0` with `params1`"]}, {"cell_type": "code", "execution_count": null, "id": "c23608bf", "metadata": {"lines_to_next_cell": 1}, "outputs": [], "source": ["def fit_beta(\n", "        p: rv_continuous,\n", "        params0: np.ndarray,\n", "        step_size: float,\n", "        n_iters: int,\n", "        n_samples: int\n", "        ) -> rv_continuous:\n", "    for i in range(n_iters):\n", "        params1 = ...\n", "        q_old = ...\n", "        q_new = ...\n", "        if (\n", "                kl_divergence(p, q_new, n_samples) <\n", "                kl_divergence(p, q_old, n_samples)\n", "                ):\n", "            params0 = params1\n", "    return stats.beta(a=params0[0], b=params0[1])"]}, {"cell_type": "code", "execution_count": null, "id": "66546263", "metadata": {}, "outputs": [], "source": ["p = stats.norm(loc=0.5, scale=0.05)\n", "dist = fit_beta(\n", "        p=p, params0=np.array([0.5, 0.5]),\n", "        step_size=0.05, n_iters=10_000,\n", "        n_samples=10_000\n", "        )"]}, {"cell_type": "markdown", "id": "055fed16", "metadata": {}, "source": ["If you run the next cell, you will see a comparison of the $P$ and $Q$ distributions before and after fitting with the KL divergence. Your result must look as follows:\n", "\n", "![fitting beta kl](local/imgs/beta_kl_fitted.png)"]}, {"cell_type": "code", "execution_count": null, "id": "0ee78813", "metadata": {}, "outputs": [], "source": ["x_range = np.linspace(0, 1, 100)\n", "dist0 = stats.beta(0.5, 0.5)\n", "\n", "fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n", "ax[0].plot(x_range, p.pdf(x_range), label=\"$P(x)$\")\n", "ax[0].plot(x_range, dist0.pdf(x_range), label=\"$Q(x)$\")\n", "ax[0].set_title(\"Before fitting\")\n", "\n", "ax[1].plot(x_range, p.pdf(x_range), label=\"$P(x)$\")\n", "ax[1].plot(x_range, dist.pdf(x_range), label=\"$Q(x)$\")\n", "ax[1].set_title(\"After fitting\")\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "cc3646a8", "metadata": {}, "source": ["Use the following cell to evaluate your code:"]}, {"cell_type": "code", "execution_count": null, "id": "09c36d49", "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), task_id=\"T4\");"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 5}