{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "collapsed_sections": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "code", "execution_count": null, "id": "2c0d0eb0", "metadata": {}, "outputs": [], "source": ["# init repo notebook\n", "!git clone https://github.com/rramosp/ppdl.git > /dev/null 2> /dev/null\n", "!mv -n ppdl/content/init.py ppdl/content/local . 2> /dev/null\n", "!pip install -r ppdl/content/requirements.txt > /dev/null"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "SZAdBAkBDjc3"}, "outputs": [], "source": ["import sympy as sy\n", "import numpy as np\n", "from scipy import stats\n", "from scipy.integrate import quad\n", "import matplotlib.pyplot as plt\n", "import tensorflow_probability as tfp\n", "import tensorflow as tf\n", "tfd =  tfp.distributions\n"]}, {"cell_type": "markdown", "source": ["## An expectation\n", "\n", "we have the following random variable, whose distribution depends on a parameter $\\theta$:\n", "\n", "$$z \\sim \\mathcal{N}_z(\\mu=3, \\sigma=\\theta)\\;\\;\\;\\text{with }p_\\theta(z)\\text{ the pdf}$$\n", "\n", "and we have the following function on $z$, which depends on also on parameter $\\theta$\n", "\n", "$$f_\\theta(z) = (z+\\theta)^2$$ \n", "\n", "we want to compute the following expectation\n", "\n", "$$\\begin{align}\n", "\\mathbb{E}_{z\\sim N_\\theta} [f_\\theta(z)] &= \\int_{-\\infty}^{+\\infty} p_\\theta(z)f_\\theta(z) dx & \\text{definite integral} \\\\\n", "                             &\\approx \\frac{1}{N}\\sum_{z_i\\sim N_\\theta}f_\\theta(z_i) &\\text{montecarlo approximation}\n", "\\end{align}                             \n", "$$\n", "\n", "for a given value of $\\theta$, we can compute this in three ways:\n", "\n", "- using symbolic integration with `sympy`\n", "- using numeric integration with `scipy.integrate`\n", "- using the montecarlo approximation\n", "\n", "Observe that **this is very powerful** as we are estimating an integral without actually having to do the integral. Recall that **integration is hard** in general, while derivation is mechanic.\n", "\n", "**[ref]** https://gregorygundersen.com/blog/2018/04/29/reparameterization/"], "metadata": {"id": "7lIj_jf3Dudo"}}, {"cell_type": "code", "source": ["tval = 1.5"], "metadata": {"id": "mWssGlTSIerb"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# using sympy\n", "\n", "t,z = sy.symbols(r'\\theta z')\n", "muz = sy.N(3)\n", "sigmaz = t\n", "\n", "ptz = 1/(sigmaz*sy.sqrt(2*sy.pi))*sy.exp(-((z-muz)/sigmaz)**2/2) # the distribution of z\n", "ftz = (t+z)**2\n", "\n", "sy.integrate((ptz*ftz).subs({t:tval}), (z,-sy.oo,+sy.oo)).n()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 37}, "id": "GqMifB9IDpiC", "outputId": "77a29b1a-94ef-46a3-9f5f-6990491875b6"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["22.5000000000000"], "text/latex": "$\\displaystyle 22.5$"}, "metadata": {}, "execution_count": 3}]}, {"cell_type": "code", "source": ["# using numerical integration\n", "dz = stats.norm(loc=3, scale=tval)\n", "ftzn = lambda z: (z+tval)**2\n", "quad(lambda z: dz.pdf(z)*ftzn(z), -20,20)[0]\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "hQIneUqzKiPg", "outputId": "0199020e-342f-4cc9-d404-55236e6f6b7f"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["22.5"]}, "metadata": {}, "execution_count": 4}]}, {"cell_type": "code", "source": ["# using montecarlo\n", "np.mean(ftzn(dz.rvs(100000)))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "_k668h34K-Xl", "outputId": "e5fc5aac-d236-4108-89cd-ebf51473dee0"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["22.52906253501012"]}, "metadata": {}, "execution_count": 5}]}, {"cell_type": "code", "source": ["# using montecarlo with TF\n", "\n", "tf_tval = tf.Variable(tval)\n", "\n", "tf_dz = tfd.Normal(loc=3, scale=tf_tval)\n", "tf.reduce_mean((tf_dz.sample(10000)+tf_tval)**2)\n", "       "], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Q7CBl5ehMkVM", "outputId": "93a50a0c-6aeb-4cf8-a0cd-996bdf6734aa"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["<tf.Tensor: shape=(), dtype=float32, numpy=22.334044>"]}, "metadata": {}, "execution_count": 6}]}, {"cell_type": "markdown", "source": ["# Expectations through batches\n", "\n", "In ML we regularly optimize by computing a loss function $\\mathcal{L}_\\theta$ over a set of batches and then we average. And $\\theta$ refers to the parameters of our model that we are optimizing.\n", "\n", "When we do this, **we are computing a Montecarlo approximation of an expectation** of the loss with respect to whatever distribution $D$ that produces our data.\n", "\n", "$$\\underbrace{\\mathbb{E}_{x\\sim D_x} \\mathcal{L}_\\theta(x) = \\int p(x)\\mathcal{L}_\\theta(x)dx}_\\text{an expectation} \\approx \\underbrace{\\frac{1}{N}\\sum_{x_i\\sim D_x} \\mathcal{L}_\\theta(x_i)}_\\text{what we do in ML}$$\n", "\n", "Where $p(x)$ is the probability (**pdf**) of $x$ as coming from distribution $D_x$\n", "\n", "Or in supervised learning\n", "\n", "$$\\underbrace{\\mathbb{E}_{x,y\\sim D_{xy}} \\mathcal{L}_\\theta(x,y) = \\int p(x,y)\\mathcal{L}_\\theta(x,y)dx dy}_\\text{an expectation} \\approx \\underbrace{\\frac{1}{N}\\sum_{x_i y_i\\sim D_{xy}} \\mathcal{L}_\\theta(x_i, y_i)}_\\text{what we do in ML}$$\n", "\n", "\n", "Regularly, in ML, we are not very much aware of $D_x$ or $D_{xy}$, because (1) we simply have a set of data that we are given and this data is actually a sample of $D_x$ or $D_{xy}$; and (2) when we make a Montecarlo approximation of an expectation **we do not need to use the pdf** $p(x)$, and that is the beauty of it.\n", "\n", "When we do variational inference, we will want to maximize or minimize some expectation, so this is why ML optimiztion fits very nicely in this schema."], "metadata": {"id": "I9fiOLmxdtfX"}}, {"cell_type": "code", "source": ["# using montecarlo with batches of 100 samples each\n", "np.mean([np.mean(ftzn(dz.rvs(100))) for _ in range(1000)])\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "xYau0l1YdsEl", "outputId": "cf6d982a-282f-4f63-cf86-492648898e76"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["22.505884963908255"]}, "metadata": {}, "execution_count": 7}]}, {"cell_type": "markdown", "source": ["# Optimizing through gradients of expectations\n", "\n", "In fact, when we implement an optimization loop in ML with Tensorflow what we do is:\n", "\n", "1. Get a batch of data\n", "2. Compute the loss, or more exactly, a Montecarlo approximation of the expectation of the loss.\n", "3. Compute the gradient of he above\n", "4. Apply the gradients (use the optimizer)\n", "\n", "So usually the gradients we compute step 4 are\n", "\n", "$$\\begin{align}\n", "\\nabla_\\theta\\mathbb{E}_{x\\sim D_x}\\mathcal{L}_\\theta(x) &= \\nabla_\\theta\\int p(x)\\mathcal{L}_\\theta(x)dx \\\\\n", "&=\\int \\nabla_\\theta \\big[p(x)\\mathcal{L}_\\theta(x)\\big]dx\\\\\n", "&=\\int p(x) \\nabla_\\theta \\big[\\mathcal{L}_\\theta(x)\\big]dx&\\;\\;\\;(*)\\\\\n", "&\\approx \\frac{1}{N}\\sum_{x_i}\\nabla_\\theta\\mathcal{L}_\\theta(x)\\\\\n", "&= \\nabla_\\theta\\frac{1}{N}\\sum_{x_i}\\mathcal{L}_\\theta(x)&\\;\\;\\;\\text{(what we do in TF)}\\\\\n", "\\end{align}\n", "$$\n", "\n", "Recall that in TF we usually do a `tf.reduce_mean` on the loss computed over a batch of data, and **then** we take the gradients. This means that in an optimization loop in ML we are **always computing the gradient of an expectation**\n", "\n", "Note that we can do this because in $(*)$ the $p(x)$ does not depend on $\\theta$, this is, on the parameters with respect to which we are taking the gradient $\\nabla_\\theta$. This is a **key observation** in what follows.\n"], "metadata": {"id": "_Swp8HK7wtEx"}}, {"cell_type": "markdown", "source": ["# The gradient of an expectation wrt distribution parameters\n", "\n", "now we are in an unsupervised learning scenario and our data is denoted by $z$, instead of $x$. Again, we want to compute the gradient of the expectation above with respect to $z$, and evaluate it at the same specific $z$ value. But now **our pdf also depends on $\\theta$**, and we denote it by $p_\\theta$\n", "\n", "\n", "$$\\begin{align}\n", "\\nabla_z \\mathbb{E}_{z\\sim N_\\theta} [f_\\theta(z)] &= \\nabla_\\theta \\int p_\\theta(z)f_\\theta(z) dz \\\\\n", "&= \\int \\nabla_\\theta \\Big[ p_\\theta(z)f_\\theta(z)\\Big] dz\\\\\n", "&\\text{but since }p_\\theta(z)\\text{ depends on }\\theta\\\\\n", "&\\not \\approx \\frac{1}{N}\\sum_{z_i\\sim N_\\theta}\\nabla_\\theta \\Big[f_\\theta(z_i) \\Big] \n", "\\end{align}\n", "$$\n"], "metadata": {"id": "LId2CDptNgBf"}}, {"cell_type": "code", "source": ["# with sympy\n", "sy.integrate((ptz*ftz).diff(t).subs({t: tval}), (z,-sy.oo, +sy.oo)).n()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 37}, "id": "QCMIknjtPwhU", "outputId": "8713f690-4480-463b-fd39-51329a2edb7d"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["12.0000000000000"], "text/latex": "$\\displaystyle 12.0$"}, "metadata": {}, "execution_count": 8}]}, {"cell_type": "code", "source": ["# montecarlo estimate with lambdified sympy function (wrong!!!) \n", "diff_ftz = sy.lambdify(z, ftz.diff(t).subs({t: tval}), \"numpy\") # we are lazy and use sympy to get diff fz\n", "np.mean(diff_ftz(dz.rvs(100000)))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "_e1vkonJSIZO", "outputId": "37b28266-c18f-4fbe-de0f-ccef9b74d2ce"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["8.99532267198478"]}, "metadata": {}, "execution_count": 9}]}, {"cell_type": "code", "source": ["# montecarlo estimate with TF (wrong!!!) \n", "# ----\n", "# NOTE: if you sample **within** tf.GradientTape, you get a different gradient\n", "#       as it seems that sampling modifies the computational graph that the \n", "#       gradient table is monitoring\n", "\n", "func             = lambda z,t: (z+t)**2\n", "zsample = tfd.Normal(loc=3, scale=tval).sample(10000)\n", "\n", "with tf.GradientTape() as tape:\n", "    loss = tf.reduce_mean(func(zsample, tf_tval))\n", "        \n", "grad = tape.gradient(loss, tf_tval)\n", "loss, grad"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Ne0me5OPOtxd", "outputId": "4646c7c2-f488-4c55-8d7e-7b7df642d03a"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["(<tf.Tensor: shape=(), dtype=float32, numpy=22.460865>,\n", " <tf.Tensor: shape=(), dtype=float32, numpy=8.999266>)"]}, "metadata": {}, "execution_count": 10}]}, {"cell_type": "markdown", "source": ["what happens is that\n", "$$\n", "\\begin{align}\n", "\\int \\nabla_\\theta \\Big[ p_\\theta(z)f_\\theta(z)\\Big] dz &= \\int \\nabla_\\theta \\Big[ p_\\theta(z)\\Big]f_\\theta(z) + p_\\theta(x)\\nabla_\\theta\\Big[ f_\\theta(z) \\Big] dz\\\\\n", "&= \\underbrace{\\int \\nabla_\\theta \\Big[ p_\\theta(z)\\Big]f_\\theta(z)}_\\text{not an expectation} +     \\underbrace{\\int p_\\theta(z)\\nabla_\\theta\\Big[ f_\\theta(z) \\Big] dz}_\\text{an expectation}\n", "\\end{align}$$\n", "\n", "in fact, the montecarlo estimate we used is actually computing only the second part of the expression above\n", "\n", "$$ \\frac{1}{N}\\sum_{z_i\\sim N_\\theta}\\nabla_\\theta f_\\theta(z_i) \\approx \\mathbb{E}_{z\\sim \\mathcal{N}_\\theta}\\Big[ \\nabla_\\theta f_z(z) \\Big] $$\n", "\n", "and this is unfortunate because this means that we cannot use Tensorflow differentiation engine and batch averaging to compute this. \n", "\n", "Tensorflow does not do integration. Integration is hard!!!\n"], "metadata": {"id": "UFr-qKCuYG1V"}}, {"cell_type": "code", "source": ["# the second part of the expression above is what the montecarlo estimate we just used approximates\n", "diff_ft = sy.lambdify(z, (ptz*ftz.diff(t)).subs({t: tval}), \"numpy\")\n", "quad(diff_ft, -20,20)[0]"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "6xr61V64SIiB", "outputId": "d4d0d631-55a2-47d7-97cc-13211bfdde90"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["9.000000000000009"]}, "metadata": {}, "execution_count": 11}]}, {"cell_type": "code", "source": [], "metadata": {"id": "mo3zf3o4Oow_"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["# Fixing Montecarlo for expectation gradients with the reparametrization trick\n", "\n", "observe that if $p_\\theta(z)$ did not depend on $\\theta$, denoted simply by $p(z)$, this would not happen as we would have $\\nabla_\\theta p(z)=0$\n", "\n", "so we would like to have an expectation whose probability does not depend on $\\theta$.\n", "\n", "we can do that by (1) sampling from a **different variable** $x$ whose probability does not depend on $\\theta$; and (2) define a transformation $g_\\theta$ so that we recover $z$:\n", "\n", "$$\\begin{align}\n", "g_\\theta(x)&=z\\\\\n", "x&\\sim\\text{ some distribution not dependant on }\\theta\n", "\\end{align}\n", "$$\n", "this way\n", "\n", "$$\n", "\\begin{align}\n", "\\mathbb{E}_{z\\sim N_\\theta} [f_\\theta(z)] &= \\mathbb{E}_{x} [f_\\theta(g_\\theta(x)]\\\\\n", "    &= \\nabla_\\theta \\int_{-\\infty}^{+\\infty} p(x)f_\\theta(g_\\theta(x)) dx\\\\\n", "    &= \\int_{-\\infty}^{+\\infty} p(x) \\nabla_\\theta  \\Big[ f_\\theta(g_\\theta(x)) \\Big] dx\\\\\n", "    &\\approx \\frac{1}{N}\\sum_{x_i} \\nabla_\\theta f_\\theta(g_\\theta(x_i))\n", "\\end{align}\n", "$$\n", "\n", "since $p(x)$ does not depend on $\\theta$, and $x_i$ is sampled from the distribution of $x$.\n", "\n", "observe that it is ok if $g_\\theta$ also depends on $\\theta$ (thus the subscript $\\_ _\\theta$).\n", "\n", "\n", "so, for this example, we can do as follows\n", "\n", "$$\n", "\\begin{align}\n", "x &\\sim \\mathcal{N}(0,1)\\\\\n", "z &= g_\\theta(x) = \\theta x \\\\\n", "&\\text{preserving the original distribution of }z\\\\\n", "z &\\sim \\mathcal{N}(0, \\theta)\n", "\\end{align}\n", "$$"], "metadata": {"id": "esA_JgGTpbRI"}}, {"cell_type": "code", "source": ["# sanity check: expectation using sympy and the reparametrization trick\n", "\n", "g = lambda x,t: 3+t*x  # the transformation of the reparametrization trick\n", "\n", "x = sy.symbols(r'x')\n", "mux = sy.N(0)\n", "sigmax = sy.N(1)\n", "\n", "ptx = 1/sy.sqrt(2*sy.pi)*sy.exp(-x**2/2) # standard gaussian distribution for x\n", "ftx = (t+g(x,t))**2\n", "\n", "sy.integrate((ptx*ftx).subs({t:tval}), (x,-sy.oo,+sy.oo)).n()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 37}, "id": "7sxktl0aeLDO", "outputId": "2cd7626a-06dc-4e57-909f-f71ef7e07b44"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["22.5000000000000"], "text/latex": "$\\displaystyle 22.5$"}, "metadata": {}, "execution_count": 12}]}, {"cell_type": "code", "source": ["# sanity check: expectation using montecarlo and the reparametrization trick\n", "dx = stats.norm(loc=0, scale=1)\n", "ftxn = lambda x: (g(x,tval)+tval)**2\n", "np.mean(ftxn(dx.rvs(100000)))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "80F-QE6AwP-N", "outputId": "e91b73d6-9f7b-4587-a17f-5c5af267140d"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["22.423727915764786"]}, "metadata": {}, "execution_count": 13}]}, {"cell_type": "code", "source": ["# reparametrization trick: the gradient of the expectation using sympy \n", "sy.integrate((ptx*ftx).diff(t).subs({t: tval}), (x,-sy.oo, +sy.oo)).n()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 37}, "id": "k5iJClLEyYSq", "outputId": "d5f70ab2-6099-482d-c461-2d5d7449c1a2"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["12.0000000000000"], "text/latex": "$\\displaystyle 12.0$"}, "metadata": {}, "execution_count": 14}]}, {"cell_type": "code", "source": ["# reparametrization trick: the gradient of the expectation using montecarlo. It works!!!!!\n", "diff_ftx = sy.lambdify(x, ftx.diff(t).subs({t: tval}), \"numpy\")\n", "np.mean(diff_ftx(dx.rvs(100000)))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "53m4yFBJzLwj", "outputId": "2c2135a1-c805-4ea7-c2e2-71a414f490be"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["11.992036881620106"]}, "metadata": {}, "execution_count": 15}]}, {"cell_type": "code", "source": ["# reparametrization trick with TF\n", "tf_tval = tf.Variable(tval)\n", "\n", "func    = lambda z,t: (z+t)**2\n", "xsample = tfd.Normal(loc=0, scale=1).sample(100000)\n", "\n", "with tf.GradientTape() as tape:\n", "    loss = tf.reduce_mean(func(g(xsample, tf_tval), tf_tval))\n", "        \n", "grad = tape.gradient(loss, tf_tval)\n", "loss, grad"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "jGjWoEQ-S6EP", "outputId": "723b57df-4625-44c3-b813-70f53ba07698"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["(<tf.Tensor: shape=(), dtype=float32, numpy=22.431013>,\n", " <tf.Tensor: shape=(), dtype=float32, numpy=11.936471>)"]}, "metadata": {}, "execution_count": 16}]}, {"cell_type": "markdown", "source": ["This way of doing the reparametrization trick requires us to find both a distribution for $x$ and $g_\\theta$ so that $z=g_\\theta(x)$ and $z$ has the distribution that we want.\n"], "metadata": {"id": "2TF2gk1e8y30"}}, {"cell_type": "code", "source": [], "metadata": {"id": "gdcBTwFUFElz"}, "execution_count": null, "outputs": []}]}